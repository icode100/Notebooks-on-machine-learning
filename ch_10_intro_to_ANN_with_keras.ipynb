{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body { font-family: Akaar; font-size: 16px; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML # type:ignore\n",
    "style = '<style> body { font-family: Akaar; font-size: 16px; } </style>'\n",
    "display(HTML(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Introduction to Atrificial Neural Network</div>\n",
    "The first artificial neuron had one or more binary inputs and one output. The AN activates its output when more than a certain number of inputs are active.\n",
    "**Logical Computations.**\\\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1003.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "### <div style=\"font-family:fantasy\">Perceptron</div>\n",
    "One of the simples ANN architecture based the threshold logic unit (TLU) or some times linear threshold logic unit. Inputs and ouputs are numbers instead of boolean values. Each input connection is associated with a weight. TLU computes the weighted sum of the inputs.\\\n",
    "\n",
    ">$$ \n",
    ">z=w_1x_1+w_2x_2...w_nx_n = \\mathbf{X}^T\\cdot \\mathbf{W}\n",
    ">$$.\n",
    "Then it applies a step function to the sum and outputs the result \n",
    ">$$ \n",
    ">h_w(\\mathbf{X}) = step(z)\n",
    ">$$.\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1004.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "more common step functions used are heaviside and signum.\n",
    "\n",
    ">$$\n",
    ">\\begin{equation}\n",
    ">heaviside(z) = \n",
    ">\\begin{cases}\n",
    ">0, & \\text{if} z < 0 \\\\\n",
    ">1, & \\text{if} z > 0 \\\\\n",
    ">\\end{cases}\n",
    ">\\end{equation}\n",
    ">$$\n",
    "\n",
    ">$$\n",
    ">\\begin{equation}\n",
    ">signum(z) = \n",
    ">\\begin{cases}\n",
    ">-1, & \\text{if} z < 0 \\\\\n",
    ">0, & \\text{if} z = 0 \\\\\n",
    ">1, & \\text{if} z > 0 \\\\\n",
    ">\\end{cases}\n",
    ">\\end{equation}\n",
    ">$$\n",
    "\n",
    "A single TLU computes linear combination of inputs and if result exceeds a threshold it outputs the positive class otherwise negative class. Training the TLU means finding the right value of $\\mathbf{W}$. A perceptron is connected woth a layers of TLU with each TLU connected to all the inputs. When all the neurons in layer are connected to every neuron in the previous layer it is called fully connected layer ot dense layer. The inputs are fed through input neurons they output whatever input is fed. It forms input layer. An axtra bias feature is added in the input layer ($x_0=1$). It is typically represented by the bias neuron.\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1005.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "To compute the outputs of a fully connected layer.\n",
    "\n",
    ">$$\n",
    ">h_{\\mathbf{W},\\mathbf{b}}(\\mathbf{X}) = \\phi(\\mathbf{X}\\mathbf{W}+\\mathbf{b})\n",
    ">$$\n",
    "\n",
    "**X** is the input feature\\\n",
    "**W** is the weight matrix (one row per input, one column per AN layer ) except the bias\\\n",
    "**b** bias vector weight from the bias neuron to the neurons of the next AN layer.\n",
    "$\\phi$ activation function (when neurons are TLU it is step function)\n",
    "<u>Hebb's rule</u>: connection between the two neurons tend to increase when they fire simultaneously.\\ \n",
    "Perceptron training use this rule that takes into account the error made by the network when it makes a preiction, the perceptron learning rules reinforeces the connection weights that help reduce the errors.\n",
    "\n",
    ">$$\n",
    ">w_{i,j}^{\\text{next step}} = w_{i,j}+\\eta(y_j-\\hat{y}_j)x_i\n",
    ">$$\n",
    "* $w_i$, j is the connection weight between the ith input neuron and the jth output neuron.\n",
    "\n",
    "* $x_i$ is the ith input value of the current training instance.\n",
    "\n",
    "* $\\hat{y}_j$ is the output of the jth output neuron for the current training instance.\n",
    " \n",
    "* $y_j$ is the target output of the jth output neuron for the current training instance.\n",
    " \n",
    "* $\\eta$ is the learning rate.\n",
    "**perceptron convergence theorem**: Decision boundary of each output neuron is linear thus they are not capable of complex learning patterns. However if the training instance are linearly separable this would converge to a solution. (Now here comes the concept of MLPs i.e. multi layered perceptron as they have several layers s they no longer just support linear boundaries. They can perform more complex classifications and regressions). A single level perceptron resembles SDG with ```loss = 'perceptron'```. It cannot do simple XOR operation( while MLPs can)\n",
    "\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1006.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "In the above figure the weights have been changed for the bias( not all the weights are assigned to 1)\n",
    "Now it can specifically solve XOR\n",
    "ex = input is X=(1,1)\n",
    "for the first neuron (non-input) (from left),\\\n",
    "(1,1,1).(1.5,1,1) = 0.5 =>h(0.5) = 1\\\n",
    "for the second neuron (non-input) (from left),\\\n",
    "(1,1,1).(-0.5,1,1) = 1.5 =>h(1.5) = 1\\\n",
    "for the otuput neuron on the top,\\\n",
    "(1,1,1).(-0.5,-1,1) = -1.5 =>h(-1.5) = 0\\\n",
    "Thus it solves 1 $\\oplus$ 1 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perceptron training demo\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X = iris.data[:80,(2,3)] #type: ignore\n",
    "X_val = iris.data[80:,(2,3)] #type: ignore\n",
    "y = (iris.target[:80]==0).astype(np.int32) #iris setosa #type: ignore\n",
    "y_val = (iris.target[80:]==0).astype(np.int32) #type: ignore\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "y_pred = per_clf.predict(X_val)\n",
    "accuracy_score(y_val,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this can also be done using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sdg_reg = SGDClassifier(loss='perceptron',eta0=0.1,learning_rate='constant')\n",
    "sdg_reg.fit(X,y)\n",
    "y_pred = sdg_reg.predict(X_val)\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Multilayered Perceptron</div>\n",
    "<div style=\"font-family:fantasy\">construction</div>\n",
    "\n",
    "It does have one input layer, one or more layers of TLUs called hidden layers and one final output layer.\n",
    "\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1007.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called a deep neural network (DNN). Signals in only one direction from input to output is called *feed forward* NN\n",
    "\n",
    "<div style=\"font-family:fantasy\">working Algorithm (Backpropagation)</div>\n",
    "\n",
    "* Handles one minibatch at a time and it goes through the full training set many times. Each pass is called epoch. \n",
    "* Each minibatch set is passed to input layer. Then outputs for each layer are computed based on outputs for each layer are computed based on the outputs of the previous layer till the final output layer. This is forward pass. Here all the intermediate results are preserved since they are needed for the backward pass.\n",
    "* It uses loss function to compute network output error.\n",
    "* computes how much each output contributed to the error. This is done by applying chain rule(of calculus differentiation). This contribution to error calculation progresses in backward direction until it reaches input layer. It measures gradient accross all the connection weights in the network by propagation of error gradients in the bacward direction.\n",
    "* Algorithm performs gradient descent to tweak all the connection weights in the network using error gradients it just computed\n",
    "It is important to initialize the connection weights of all the hidden layers to random numbers to maintain the diversity btwn each neuron. In order for the backpropagation to work properly $\\phi$ is replaced with logistic sunction or sigmoid.\n",
    "\n",
    ">$$\n",
    ">\\sigma(z) = \\frac{1}{1+\\exp{(z)}}\n",
    ">$$\n",
    "This is beacuse the step function contain flat segments only so there is no gradient to work with, while $\\sigma(z)$ has non-zero derivatives everywhere.\n",
    "Some other popular choices for $\\phi$ are \n",
    "* $\\tanh{(z)} = 2\\sigma(2z)-1$ the range of this function is (-1,1) hence more centered wrt 0 thus helps to speed up the convergence at the begining \n",
    "* rectified linear unit function ReLU(z) = max(0,z) is continuous every where but not differentiable at z<0. It is fast to compute hence has become the default\n",
    "\n",
    "<div style=\"font-family:fantasy\">Mathematical background</div>\n",
    "\n",
    "1. Perceptron\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mdZWOnL-XcHF70UOTd3jvLxWJumqPGwEg7fhfd7T-wndl00XEQxosyGtp94e4pluAb3XhiSMRnkpdeQ0p9lJMXZn-LnE1qDs_g8FTH9x2ujdrPaeK0Zjv_emsJD2kyX4SRVPpUTxxRQPsSaFTnqA7ULa01jSIs0J4cu9RI_KkzjrBZTdrtkqOYga7pLfAFfXz?width=2160&height=1129&cropmode=none\" width=\"500\" height=\"300\">\n",
    "\n",
    ">$z=w_1x_1+w_2x_2+b$\n",
    ">$\\hat{y} = step(z)$\n",
    "\n",
    "2. Logistic regression\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mjebF_OaA1DZOa1sABn9Wpen2E9clAr-yCrpe-fBj2HkRUNh2GT6tJL23B3pgpUJ3CubA_SkDcVE3dJibRgbqxGDvkzYy2RD88jZ8DJV9mdCm1njZWuRb7COPkhmRmiQN5On9NVE0xRkHvSozYyMMU2hQfKDxm-NhuHPgc8tMbCDGSlOl85b3ULNYdvBFgxku?width=2160&height=1181&cropmode=none\" width=\"500\" height=\"200\" />\n",
    "\n",
    ">$z=w_1x_1+w_2x_2+b$\n",
    ">$\\hat{p} = \\sigma(z) = \\frac{1}{1+exp(-z)}=$ predicted probability\n",
    "\n",
    "3. General Neural network\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mCVj6gYWAm2HrCN7i42vcuzwiPy4naLKbbBFnektAMMy0VUW6RTYeITKENCjrWh27aPT01FW3Il1OLe0PCe5jMBPJtKvWXZgBwiKSK8yD0EOjOi4yH9aove6IpPneXodR4wsf1OCHHi46DbBHaHKvj2lvfB-QujFVRNBBOnFaxHdQKYryoAgnMwvx9dOogpSR?width=2160&height=1193&cropmode=none\" width=\"600\" height=\"300\" />\n",
    "\n",
    "<u>step 1</u>:\n",
    ">$$\n",
    ">\\begin{bmatrix}\n",
    ">z_1^{(1)} = w_{11}^{(1)}x_1 + w_{21}^{(1)}x_2 +b_1^{(1)}\\\\\n",
    ">\\\\\n",
    ">z_2^{(1)} = w_{12}^{(1)}x_1 + w_{22}^{(1)}x_2 +b_2^{(1)}\\\\\n",
    ">\\end{bmatrix}\n",
    ">$$\n",
    "\n",
    "<u>step 2</u>:\n",
    ">$$\n",
    ">\\begin{bmatrix}\n",
    ">h_1 = \\sigma(z_1^{(1)})\\\\\n",
    ">\\\\\n",
    ">h_2 = \\sigma(z_2^{(1)})\\\\\n",
    ">\\end{bmatrix}\n",
    ">$$\n",
    "\n",
    "<u>step 3</u>:\n",
    ">$$\n",
    ">\\begin{bmatrix}\n",
    ">z^{(2)} = w_1^{(2)}h_1+w_2^{(2)}h_2+b^{(2)}\n",
    ">\\end{bmatrix}\n",
    ">$$\n",
    "\n",
    "<u>step 4</u>:\n",
    ">$\\hat{p} = \\sigma(z^{(2)})$\n",
    "\n",
    "<u>Matrix form </u>:\n",
    ">$$\n",
    "\\begin{matrix}\n",
    ">\\mathbf{Z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{X} + \\mathbf{b}^{(1)}\\\\\n",
    ">\\\\\n",
    ">\\mathbf{h} = \\sigma(\\mathbf{Z}^{(1)})\\\\\n",
    ">\\\\\n",
    ">\\mathbf{Z}^{(2)} = \\mathbf{W}^{(2)}\\mathbf{h} + \\mathbf{b}^{(1)}\\\\\n",
    ">\\\\\n",
    ">\\hat{p} = \\sigma(\\mathbf{Z}^{(2)})\n",
    "\\end{matrix}\n",
    ">$$\n",
    "\n",
    "##### _**Backpropagation**_\n",
    "*A prerequite*:\\\n",
    "$p = \\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "=>$\\frac{d p}{d x} = \\frac{e^{-x}}{(1+e^{-x})^2} = p(1-p)$\n",
    "\n",
    "*error function*:\\\n",
    "$L_i=\\frac{1}{2}||\\hat{p}-y||^2 = \\sum_{i=1}^{N}\\frac{1}{2}[\\hat{p}_i-y_i]^2$\\\n",
    "where N is the number of training instances.\n",
    "\n",
    "*computations*:\\\n",
    "Now first compute,$\\frac{\\partial L}{\\partial w_1^{(2)}}$\n",
    "\n",
    "$\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial L}{\\partial w_1^{(2)}} = \\sum_{i=1}^{N}\\frac{\\partial L_i}{\\partial w_1^{(2)}} \\text{ i.e path } h_1 \\ to \\ \\hat{p}_i\\\\\n",
    "\\\\\n",
    "=>\\frac{\\partial L_i}{\\partial w_1^{(2)}}= \\frac{\\partial z^{(2)}}{\\partial w_1^{(2)}} \\times \\frac{\\partial \\hat{p}_i}{\\partial z^{(2)}} \\times\\frac{\\partial L_i}{\\partial \\hat{p}_i} \\text{ (using chain rule)}\\\\\n",
    "\\\\\n",
    "=[h_1] \\times [\\hat{p}_i(1-\\hat{p}_i)] \\times [\\hat{p}_i - y_i]\n",
    "=h_1\\delta_{\\hat{p}_i}\\\\\n",
    "\\end{matrix}\n",
    "$\\\n",
    "\n",
    "Now again compute, $\\frac{\\partial L_i}{\\partial w_{21}^{(1)}}$ that is path from $x_2$ to $h_1$\\\n",
    "$\n",
    "\\begin{matrix}\n",
    "\\\\\n",
    "\\frac{\\partial L_i}{\\partial w_{21}^{(1)}} = \\frac{\\partial z_1^{(2)}}{\\partial w_{21}^{(2)}} \\times \\frac{\\partial h_1}{\\partial z_1^{(1)}} \\times \\frac{\\partial z^{(2)}}{\\partial h_1} \\times \\frac{\\partial \\hat{p}_i}{\\partial z^{(2)}} \\times \\frac{\\partial L_i}{\\partial \\hat{p}_i} \\text{ (using chain rule)}\\\\\n",
    "\\\\\n",
    "=[x_2]\\times[h_1(1-h_1)]\\times[w_1^{(2)}]\\times[\\hat{p}_i(1-\\hat{p}_i)]\\times[\\hat{p}_i-y_i]\\\\\n",
    "\\\\\n",
    "=x_2h_1(1-h_1)w_1^{(2)}\\delta_{\\hat{p}_i} = x_2\\times h_1(1-h_1)\\times\\delta_{h_1} = (from)\\times(to)\\times\\delta_{to}\\\\\n",
    "\\\\\n",
    "\\end{matrix}\n",
    "$\\\n",
    "Thus for $\\frac{\\partial L_i}{\\partial w_{12}^{(1)}}$ i.e from $x_1$ to $h_2$ = $x_1\\times h_2(1-h_2)\\times\\delta_{h_2}$.\\\n",
    "Thus here we see with every layer of AN the error is propagating and equating this deriavtive of loss funtion to 0 gives us the gradient at each connection using which we perform the gradient descent and determine the optimal values of $\\mathbf{W}$ and $\\mathbf{b}$\n",
    "\n",
    "# <div style=\"font-family:fantasy\">MLP for regression</div>\n",
    "Here we do not use any activation function for the output neurons so they are free to output any range of values. Thus we can use the ReLU activation function for output layers else softplus activation function. $softplus(z)=\\log(1+exp(z))$. Finally if we want to gurantee prediction that predictions will fall within a given range of values thenwe can use logistic regression function or hyperbolic tangent and scale the labels to appropriate range we can use hubbers loss function(a combination of $l1$ and $l2$ loss functions)\n",
    "$$\n",
    "HL =\n",
    "\\begin{cases}\n",
    "l2,&\\text{ if } error<\\delta\\\\\n",
    "l1,&\\text{ if } error>\\delta\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "linear part makes it less sensitive to outliers and quadratic part allows it to converge faster and be more precise than $l1$ error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Implementing MLPs with Keras</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.12.0', '2.12.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__,keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Building an image classifier using sequential API</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full),(X_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5c8a7e0890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train_full[0],cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a validation set from the training set. Also as we will train our neural network using Gradient Descent then so we should scale our input features. Thus we bring the pixels in down to in the range 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid,X_train,X_test = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0,X_test/255.0\n",
    "y_valid,y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5c884a8090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0],cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to explicitly define the class names the following class names have been taken from the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\",\"Trousers\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneakers\",\"Bag\",\"Ankle Boot\"]\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a neural network with two hidden layers using sequential api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest keras model composed of single stack of layers connected sequentially\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#first,layer of the model, role is to convert each input into an 1D array thus we have used flatten.\n",
    "# if it recieves X, it computes X.reshape(-1,1). We need to specify the input shape for the first layer.\n",
    "#alternatively we can use keras.layers.InputLayer(input_shape=[28,28])\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "\n",
    "#we add a dense layer(first hidden layer) with 300 neurons that will use ReLU activation function. \n",
    "# Each of such dense layer manages its own weight matrix and a vector of bias terms\n",
    "model.add(keras.layers.Dense(300,activation='relu',name='hidden1'))\n",
    "\n",
    "#second dense layer (hidden layer)\n",
    "model.add(keras.layers.Dense(100,activation='relu'))\n",
    "\n",
    "#output layer with 10 neurons (i.e. 1 per class) this uses softmax activation function\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden1 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see there are a lot of parameters. This can make the model overfit only when very less number of data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x7f5c8a7c9d10>,\n",
       " <keras.layers.core.dense.Dense at 0x7f5c88504890>,\n",
       " <keras.layers.core.dense.Dense at 0x7f5c884fa310>,\n",
       " <keras.layers.core.dense.Dense at 0x7f5c884aa6d0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hidden1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('hidden1') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameteers of the layer can be accessed using get weight and set weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights,biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03006566,  0.01157558,  0.02074   , ...,  0.05172713,\n",
       "        -0.03684393,  0.03913989],\n",
       "       [-0.03140466,  0.02670597,  0.03846909, ..., -0.06674509,\n",
       "        -0.0472322 ,  0.03509906],\n",
       "       [ 0.01874004, -0.00348052, -0.00643624, ..., -0.04788692,\n",
       "        -0.02688803,  0.00842904],\n",
       "       ...,\n",
       "       [-0.04530234, -0.03219036,  0.04395688, ..., -0.04851828,\n",
       "        -0.00687166, -0.0596063 ],\n",
       "       [-0.00040616,  0.05332552, -0.05434116, ...,  0.0221475 ,\n",
       "         0.00931282,  0.07362515],\n",
       "       [-0.06339576, -0.0474619 , -0.04106263, ...,  0.02282326,\n",
       "         0.04291612,  0.04839803]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is so because there are 28 $\\times$ 28 = 784 input neurons connected to 300 other neurons of the first hidden layer and also note that these connections are generated randomly. Though we can set ```kernel_initializer``` and `bias_initializer` for using different initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ofcourse because there are 300 neurons in the first hidden layer thus 300 bias terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you do not provide the input size in in first layer the model will automatically assign some values to the number of inputs when u call the build method or while proviting training data to the model but until then you cannot call the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"font-family:fantasy\">compiling the model</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we finally compile the model by providing the loss function optimizer and metrics for measuring accuracy measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we use `sparse_categorical_crossentropy` because we have sparse labels that is we have 10 classes to choose upon each directly assigned values from 0-9. Instead if we had some one-hot vetors we would have used `categorical _crossentropy`. If we are doing binary classification then we can use `sigmoid` in the output layer instead of `softmax` and would use `binary_crossentropy` loss. Also we have used `sdg` as the optimizer that is we will train the model using backpropagation algorithm as described earlier. We can also set the learning rate by doing `optimizer=keras.optimizer.SGD(lr=0.01)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"font-family:fantasy\">Training and evaluating model</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 14:00:13.498575: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 172480000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1715/1719 [============================>.] - ETA: 0s - loss: 0.7222 - accuracy: 0.7624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 14:00:26.594280: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15680000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.7218 - accuracy: 0.7625 - val_loss: 0.5122 - val_accuracy: 0.8244\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4864 - accuracy: 0.8312 - val_loss: 0.4406 - val_accuracy: 0.8514\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4411 - accuracy: 0.8453 - val_loss: 0.4647 - val_accuracy: 0.8346\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4124 - accuracy: 0.8548 - val_loss: 0.3903 - val_accuracy: 0.8670\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3928 - accuracy: 0.8607 - val_loss: 0.4011 - val_accuracy: 0.8598\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3781 - accuracy: 0.8659 - val_loss: 0.3725 - val_accuracy: 0.8734\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3645 - accuracy: 0.8705 - val_loss: 0.3613 - val_accuracy: 0.8746\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3528 - accuracy: 0.8752 - val_loss: 0.3538 - val_accuracy: 0.8746\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3424 - accuracy: 0.8783 - val_loss: 0.3454 - val_accuracy: 0.8782\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3331 - accuracy: 0.8815 - val_loss: 0.3675 - val_accuracy: 0.8676\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3252 - accuracy: 0.8828 - val_loss: 0.3350 - val_accuracy: 0.8820\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3182 - accuracy: 0.8862 - val_loss: 0.3318 - val_accuracy: 0.8818\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3100 - accuracy: 0.8883 - val_loss: 0.3304 - val_accuracy: 0.8844\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3038 - accuracy: 0.8910 - val_loss: 0.3203 - val_accuracy: 0.8878\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2968 - accuracy: 0.8927 - val_loss: 0.3240 - val_accuracy: 0.8844\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2916 - accuracy: 0.8943 - val_loss: 0.3398 - val_accuracy: 0.8748\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2859 - accuracy: 0.8960 - val_loss: 0.3154 - val_accuracy: 0.8862\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2800 - accuracy: 0.8988 - val_loss: 0.3116 - val_accuracy: 0.8858\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2744 - accuracy: 0.9013 - val_loss: 0.3657 - val_accuracy: 0.8688\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2694 - accuracy: 0.9029 - val_loss: 0.3198 - val_accuracy: 0.8830\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2648 - accuracy: 0.9036 - val_loss: 0.3044 - val_accuracy: 0.8900\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2594 - accuracy: 0.9066 - val_loss: 0.3064 - val_accuracy: 0.8902\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2548 - accuracy: 0.9081 - val_loss: 0.2991 - val_accuracy: 0.8932\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2501 - accuracy: 0.9099 - val_loss: 0.3088 - val_accuracy: 0.8874\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2459 - accuracy: 0.9109 - val_loss: 0.3167 - val_accuracy: 0.8842\n",
      "Epoch 26/30\n",
      "1715/1719 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9130"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=30,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`epoch` suggest the number of times the algorithm is run on the training instance. It also calculates the error after each epoch on the training and validation data. If there is a big difference then the model is propably overfitting the data(or a bug, such as data mismathch between the training and validation set). As we can see the training accuracy and validation accuracy do not differ by huge amount. That is a good sign. Thus the model here is not overfitting. we can set the class weight high in case some class are underrepresented and vice versa for overpresented.This can b done by setting `class_weight`. We can also set the weight for each instance by setting `sample_weight`. fit returns a history which can be used to derive info about the model as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 30, 'steps': 1719}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params #the training paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch #the list of epochs it went through\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7245148420333862,\n",
       "  0.49180474877357483,\n",
       "  0.44598621129989624,\n",
       "  0.4181451201438904,\n",
       "  0.3959883451461792,\n",
       "  0.38015806674957275,\n",
       "  0.36524587869644165,\n",
       "  0.3533961772918701,\n",
       "  0.3436450958251953,\n",
       "  0.33432504534721375,\n",
       "  0.32558977603912354,\n",
       "  0.3172856271266937,\n",
       "  0.31039801239967346,\n",
       "  0.3036155104637146,\n",
       "  0.2973192036151886,\n",
       "  0.29107266664505005,\n",
       "  0.28479886054992676,\n",
       "  0.2784164845943451,\n",
       "  0.2741815745830536,\n",
       "  0.26898398995399475,\n",
       "  0.2645437717437744,\n",
       "  0.2584874927997589,\n",
       "  0.25493910908699036,\n",
       "  0.25088104605674744,\n",
       "  0.24546651542186737,\n",
       "  0.24196358025074005,\n",
       "  0.23736225068569183,\n",
       "  0.23301082849502563,\n",
       "  0.22988677024841309,\n",
       "  0.2265242487192154],\n",
       " 'accuracy': [0.7644000053405762,\n",
       "  0.8291636109352112,\n",
       "  0.8444363474845886,\n",
       "  0.8538545370101929,\n",
       "  0.8599818348884583,\n",
       "  0.8660545349121094,\n",
       "  0.8715090751647949,\n",
       "  0.8743818402290344,\n",
       "  0.8767091035842896,\n",
       "  0.8807636499404907,\n",
       "  0.8835636377334595,\n",
       "  0.8863454461097717,\n",
       "  0.889054536819458,\n",
       "  0.890999972820282,\n",
       "  0.8926363587379456,\n",
       "  0.8949818015098572,\n",
       "  0.8964909315109253,\n",
       "  0.8988363742828369,\n",
       "  0.9007272720336914,\n",
       "  0.9025454521179199,\n",
       "  0.9043999910354614,\n",
       "  0.9073272943496704,\n",
       "  0.9083091020584106,\n",
       "  0.9092181921005249,\n",
       "  0.9109818339347839,\n",
       "  0.9123818278312683,\n",
       "  0.9144726991653442,\n",
       "  0.9160181879997253,\n",
       "  0.9175817966461182,\n",
       "  0.918109118938446],\n",
       " 'val_loss': [0.5118072032928467,\n",
       "  0.4451672434806824,\n",
       "  0.4245346784591675,\n",
       "  0.40200453996658325,\n",
       "  0.3771321773529053,\n",
       "  0.37521496415138245,\n",
       "  0.36261600255966187,\n",
       "  0.35322305560112,\n",
       "  0.35798102617263794,\n",
       "  0.3503147065639496,\n",
       "  0.34498727321624756,\n",
       "  0.3418782949447632,\n",
       "  0.3236616551876068,\n",
       "  0.33433622121810913,\n",
       "  0.32029756903648376,\n",
       "  0.31907376646995544,\n",
       "  0.3170084059238434,\n",
       "  0.3112024962902069,\n",
       "  0.32528287172317505,\n",
       "  0.2982259690761566,\n",
       "  0.30766749382019043,\n",
       "  0.3218900263309479,\n",
       "  0.3200182020664215,\n",
       "  0.31354591250419617,\n",
       "  0.2955973744392395,\n",
       "  0.2986030876636505,\n",
       "  0.2927270531654358,\n",
       "  0.29782459139823914,\n",
       "  0.28552448749542236,\n",
       "  0.2922282814979553],\n",
       " 'val_accuracy': [0.8216000199317932,\n",
       "  0.8525999784469604,\n",
       "  0.8546000123023987,\n",
       "  0.8640000224113464,\n",
       "  0.8733999729156494,\n",
       "  0.8691999912261963,\n",
       "  0.8733999729156494,\n",
       "  0.8751999735832214,\n",
       "  0.8736000061035156,\n",
       "  0.8722000122070312,\n",
       "  0.8758000135421753,\n",
       "  0.8799999952316284,\n",
       "  0.8862000107765198,\n",
       "  0.8784000277519226,\n",
       "  0.8876000046730042,\n",
       "  0.8855999708175659,\n",
       "  0.8894000053405762,\n",
       "  0.8867999911308289,\n",
       "  0.8826000094413757,\n",
       "  0.8931999802589417,\n",
       "  0.88919997215271,\n",
       "  0.8827999830245972,\n",
       "  0.8830000162124634,\n",
       "  0.8859999775886536,\n",
       "  0.8902000188827515,\n",
       "  0.8903999924659729,\n",
       "  0.8984000086784363,\n",
       "  0.8966000080108643,\n",
       "  0.8971999883651733,\n",
       "  0.8989999890327454]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history #loss, accuracy, val_loss and val_accuracy at each epoch in the form of a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724515</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.8216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491805</td>\n",
       "      <td>0.829164</td>\n",
       "      <td>0.445167</td>\n",
       "      <td>0.8526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.445986</td>\n",
       "      <td>0.844436</td>\n",
       "      <td>0.424535</td>\n",
       "      <td>0.8546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418145</td>\n",
       "      <td>0.853855</td>\n",
       "      <td>0.402005</td>\n",
       "      <td>0.8640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.395988</td>\n",
       "      <td>0.859982</td>\n",
       "      <td>0.377132</td>\n",
       "      <td>0.8734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.380158</td>\n",
       "      <td>0.866055</td>\n",
       "      <td>0.375215</td>\n",
       "      <td>0.8692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.871509</td>\n",
       "      <td>0.362616</td>\n",
       "      <td>0.8734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.353396</td>\n",
       "      <td>0.874382</td>\n",
       "      <td>0.353223</td>\n",
       "      <td>0.8752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.343645</td>\n",
       "      <td>0.876709</td>\n",
       "      <td>0.357981</td>\n",
       "      <td>0.8736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.334325</td>\n",
       "      <td>0.880764</td>\n",
       "      <td>0.350315</td>\n",
       "      <td>0.8722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.325590</td>\n",
       "      <td>0.883564</td>\n",
       "      <td>0.344987</td>\n",
       "      <td>0.8758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.317286</td>\n",
       "      <td>0.886345</td>\n",
       "      <td>0.341878</td>\n",
       "      <td>0.8800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.310398</td>\n",
       "      <td>0.889055</td>\n",
       "      <td>0.323662</td>\n",
       "      <td>0.8862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.303616</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.334336</td>\n",
       "      <td>0.8784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.297319</td>\n",
       "      <td>0.892636</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.8876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.291073</td>\n",
       "      <td>0.894982</td>\n",
       "      <td>0.319074</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.284799</td>\n",
       "      <td>0.896491</td>\n",
       "      <td>0.317008</td>\n",
       "      <td>0.8894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.278416</td>\n",
       "      <td>0.898836</td>\n",
       "      <td>0.311202</td>\n",
       "      <td>0.8868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.274182</td>\n",
       "      <td>0.900727</td>\n",
       "      <td>0.325283</td>\n",
       "      <td>0.8826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.268984</td>\n",
       "      <td>0.902545</td>\n",
       "      <td>0.298226</td>\n",
       "      <td>0.8932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.264544</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>0.307667</td>\n",
       "      <td>0.8892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.258487</td>\n",
       "      <td>0.907327</td>\n",
       "      <td>0.321890</td>\n",
       "      <td>0.8828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.254939</td>\n",
       "      <td>0.908309</td>\n",
       "      <td>0.320018</td>\n",
       "      <td>0.8830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.250881</td>\n",
       "      <td>0.909218</td>\n",
       "      <td>0.313546</td>\n",
       "      <td>0.8860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.245467</td>\n",
       "      <td>0.910982</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>0.8902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.241964</td>\n",
       "      <td>0.912382</td>\n",
       "      <td>0.298603</td>\n",
       "      <td>0.8904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.237362</td>\n",
       "      <td>0.914473</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.8984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.233011</td>\n",
       "      <td>0.916018</td>\n",
       "      <td>0.297825</td>\n",
       "      <td>0.8966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.229887</td>\n",
       "      <td>0.917582</td>\n",
       "      <td>0.285524</td>\n",
       "      <td>0.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.226524</td>\n",
       "      <td>0.918109</td>\n",
       "      <td>0.292228</td>\n",
       "      <td>0.8990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.724515  0.764400  0.511807        0.8216\n",
       "1   0.491805  0.829164  0.445167        0.8526\n",
       "2   0.445986  0.844436  0.424535        0.8546\n",
       "3   0.418145  0.853855  0.402005        0.8640\n",
       "4   0.395988  0.859982  0.377132        0.8734\n",
       "5   0.380158  0.866055  0.375215        0.8692\n",
       "6   0.365246  0.871509  0.362616        0.8734\n",
       "7   0.353396  0.874382  0.353223        0.8752\n",
       "8   0.343645  0.876709  0.357981        0.8736\n",
       "9   0.334325  0.880764  0.350315        0.8722\n",
       "10  0.325590  0.883564  0.344987        0.8758\n",
       "11  0.317286  0.886345  0.341878        0.8800\n",
       "12  0.310398  0.889055  0.323662        0.8862\n",
       "13  0.303616  0.891000  0.334336        0.8784\n",
       "14  0.297319  0.892636  0.320298        0.8876\n",
       "15  0.291073  0.894982  0.319074        0.8856\n",
       "16  0.284799  0.896491  0.317008        0.8894\n",
       "17  0.278416  0.898836  0.311202        0.8868\n",
       "18  0.274182  0.900727  0.325283        0.8826\n",
       "19  0.268984  0.902545  0.298226        0.8932\n",
       "20  0.264544  0.904400  0.307667        0.8892\n",
       "21  0.258487  0.907327  0.321890        0.8828\n",
       "22  0.254939  0.908309  0.320018        0.8830\n",
       "23  0.250881  0.909218  0.313546        0.8860\n",
       "24  0.245467  0.910982  0.295597        0.8902\n",
       "25  0.241964  0.912382  0.298603        0.8904\n",
       "26  0.237362  0.914473  0.292727        0.8984\n",
       "27  0.233011  0.916018  0.297825        0.8966\n",
       "28  0.229887  0.917582  0.285524        0.8972\n",
       "29  0.226524  0.918109  0.292228        0.8990"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(history.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the model looks like it\n",
    "performed better on the validation set than on the training set at the beginning of\n",
    "training. But that’s not the case: indeed, the validation error is computed at the end of\n",
    "each epoch, while the training error is computed using a running mean during each\n",
    "epoch. So the training curve should be shifted by half an epoch to the left. The training set performance ends up beating the validation performance, as is gen‐\n",
    "erally the case when you train for long enough. You can tell that the model has not\n",
    "quite converged yet, as the validation loss is still going down, so you should probably\n",
    "continue training. It’s as simple as calling the `fit()` method again, since Keras just\n",
    "continues training where it left off.\n",
    "\n",
    "Once you are satisfied with your model’s validation\n",
    "accuracy, you should evaluate it on the test set to estimate the generalization error\n",
    "before you deploy the model to production. You can easily do this using the `evaluate()` method it also supports several other arguments, such as batch_size and\n",
    "`sample_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/313 [..............................] - ETA: 2s - loss: 0.2949 - accuracy: 0.8945 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 11:24:08.361889: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3297 - accuracy: 0.8820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32974734902381897, 0.8820000290870667]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"font-family:fantasy\">using model to make predictions</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 183ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the model predicts one probability per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[9 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also get the classes \n",
    "y_pred = np.argmax(model.predict(X_new),axis=1)\n",
    "print(y_pred)\n",
    "# accuracy_score calculation\n",
    "accuracy_score(y_test[:3],y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy\">Building Regression MLP using sequential API</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target)#type:ignore\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude   \n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88  \\\n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(housing.data,columns=housing.feature_names)#type:ignore\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between the classification model and this are\n",
    "the fact that the output layer has a single neuron (since we only want to predict a sin‐\n",
    "gle value) and uses no activation function, and the loss function is the mean squared\n",
    "error. Since the dataset is quite noisy, we just use a single hidden layer with fewer\n",
    "neurons than before, to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(30,activation=keras.activations.relu,input_shape=X_train.shape[1:])) #because we want the number of features\n",
    "model.add(keras.layers.Dense(1)) #because we will predict a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling\n",
    "model.compile(loss=keras.losses.mean_squared_error,optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 2.2846 - val_loss: 1.0305\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8261 - val_loss: 0.7366\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7109 - val_loss: 0.6725\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6654 - val_loss: 0.6350\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6308 - val_loss: 0.6104\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6028 - val_loss: 0.5801\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5799 - val_loss: 0.5637\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5606 - val_loss: 0.5491\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5447 - val_loss: 0.5302\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5304 - val_loss: 0.5159\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5197 - val_loss: 0.5061\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5097 - val_loss: 0.4979\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5011 - val_loss: 0.4920\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4939 - val_loss: 0.4880\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4878 - val_loss: 0.4785\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4822 - val_loss: 0.4741\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4777 - val_loss: 0.4734\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4733 - val_loss: 0.4691\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4697 - val_loss: 0.4691\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4663 - val_loss: 0.4608\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4633 - val_loss: 0.4583\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4605 - val_loss: 0.4580\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4577 - val_loss: 0.4580\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4554 - val_loss: 0.4525\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.4472\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4509 - val_loss: 0.4475\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4488 - val_loss: 0.4447\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4467 - val_loss: 0.4433\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4449 - val_loss: 0.4494\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4433 - val_loss: 0.4392\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,y_train,epochs=30,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4599434733390808"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test,y_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[:3] # type: ignore\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Building Complex Models Using Functional API</div>\n",
    "*Wide & Deep* It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mZffW45GvCtsAxAbJPRmUpfIjI7UuhaLCoVzFxSBMLPYfCUK1bfFhgAjGiOZWjLrWCamhRXOPTjxp9FPWdgcxaOxUY24RejEuci5LdzlJUNO8jXCQjtIicKcI95WnLSxMORrI9h18abBnSWZNXu-2U9XfXZ5tY9DgCcKPN6PGXLZ0esMwu7P_O5VCLeJBB6Ef?width=624&height=394&cropmode=none\" width=\"600\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first layer is of the input with `dtype` and `shape`.\n",
    "* then we create a `Dense` layer with 30 neurons using ReLU activation function. Then we call it as a function and pass the input layer to it. This API is thus functional as here we define how the layers are going to be connected.\n",
    "* we then create a second layer and again we use it as a function. Note that we pass it the output of the first hidden layer.\n",
    "* Now we create a concatenation layer for concatnating the input and the outout of the `hidden2` layer. \n",
    "* Then we create the output layer with a single neuron and no activation function and we call it like a function passing the result of concatenation layer. \n",
    "* then we model the keras by specifying the input and the ouput\n",
    "after doing all this the next step is simply compiling and fitting.\n",
    "\n",
    "---\n",
    "\n",
    "Next comes the case where we want to pass one batch on the input through the wide path and another batch through deep path. Here's where we need two inputs.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mvh4jrE7BisZtEcWj6o2HYSvTEjXGmj2xuZ-f5BYfU1jJFUToTYIgy_R2Bo60AJjaC2IABGlM-Dvv6il26LY0sAB0Ej9cQmG1mnTbwLM_EUTMSp2XX6q8UxFTHvHABdkFAb1Olp3Ka__ANXNJ-qswdYiHkIdbWlhiZvbdEay4StKGQBZr6gR-2quPnLv_dBnD?width=578&height=444&cropmode=none\" width=\"500\" height=\"200\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the only thing is while fitting this model we need to pass two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.9904 - val_loss: 1.1570\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7586 - val_loss: 0.6795\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6356 - val_loss: 0.5912\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5949 - val_loss: 0.5637\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5715 - val_loss: 0.5448\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5548 - val_loss: 0.5309\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5422 - val_loss: 0.5220\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5316 - val_loss: 0.5120\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.5068\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5143 - val_loss: 0.4987\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5067 - val_loss: 0.4924\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5001 - val_loss: 0.4872\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4940 - val_loss: 0.4808\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4889 - val_loss: 0.4788\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4841 - val_loss: 0.4723\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4797 - val_loss: 0.4693\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4759 - val_loss: 0.4661\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4723 - val_loss: 0.4644\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4689 - val_loss: 0.4595\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4654 - val_loss: 0.4574\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4791\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test)\n",
    "y_pred = model.predict((X_new_A,X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4791209101676941"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cases of multiple ouputs:\n",
    "* you may want to locate and classify the\n",
    "main object in a picture. This is both a regression task (finding the coordinates of\n",
    "the object’s center, as well as its width and height) and a classification task\n",
    "* you may have multiple independent tasks based on the same data. Sure,\n",
    "you could train one neural network per task, but in many cases you will get better\n",
    "results on all tasks by training a single neural network with one output per task.\n",
    "This is because the neural network can learn features in the data that are useful\n",
    "across tasks. For example, you could perform multitask classification on pictures\n",
    "of faces, using one output to classify the person’s facial expression (smiling, sur‐\n",
    "prised, etc.) and another output to identify whether they are wearing glasses or\n",
    "not.\n",
    "* Another use case is as a regularization technique (i.e., a training constraint whose\n",
    "objective is to reduce overfitting and thus improve the model’s ability to general‐\n",
    "ize). For example, you may want to add some auxiliary outputs in a neural net‐\n",
    "work architecture (see Figure 10-16) to ensure that the underlying part of the\n",
    "network learns something useful on its own, without relying on the rest of the\n",
    "network.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4maZZr2wG1TMeDcRJOLAVvt4ds0juu3yUM53ytpv9voi7UPa-lcxLiYa6QELDmnKx7uLdulFQ-qSWYyDbCw2qo2bvUWReuHVwa_szRiTLf8u6uDtkPk9yQjhayv7n86t7bEPBHn1nkw70qSnIX5PcexfgzwZ6SgzlF4-qspKnsqiZVNWz8y2nq5VUKx4QCJIBq?width=464&height=296&cropmode=none\" width=\"464\" height=\"296\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each output will need its own loss function thus we pass a list of losses(if we pass a single loss, Keras will assume that the\n",
    "same loss must be used for all outputs). Keras will compute all these losses\n",
    "and simply add them up to get the final loss used for training. Thus, we can also give loss weights for weighted sum of the losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.9840 - output_loss: 0.8558 - aux_output_loss: 2.1377 - val_loss: 0.6149 - val_output_loss: 0.5512 - val_aux_output_loss: 1.1878\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5554 - output_loss: 0.4993 - aux_output_loss: 1.0600 - val_loss: 0.6723 - val_output_loss: 0.6311 - val_aux_output_loss: 1.0435\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5146 - output_loss: 0.4721 - aux_output_loss: 0.8974 - val_loss: 0.4735 - val_output_loss: 0.4356 - val_aux_output_loss: 0.8148\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4748 - output_loss: 0.4410 - aux_output_loss: 0.7788 - val_loss: 0.4618 - val_output_loss: 0.4329 - val_aux_output_loss: 0.7226\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4583 - output_loss: 0.4303 - aux_output_loss: 0.7103 - val_loss: 0.4491 - val_output_loss: 0.4241 - val_aux_output_loss: 0.6749\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4453 - output_loss: 0.4204 - aux_output_loss: 0.6696 - val_loss: 0.4374 - val_output_loss: 0.4152 - val_aux_output_loss: 0.6380\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4360 - output_loss: 0.4134 - aux_output_loss: 0.6391 - val_loss: 0.4286 - val_output_loss: 0.4081 - val_aux_output_loss: 0.6132\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4280 - output_loss: 0.4071 - aux_output_loss: 0.6164 - val_loss: 0.4311 - val_output_loss: 0.4125 - val_aux_output_loss: 0.5986\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4210 - output_loss: 0.4012 - aux_output_loss: 0.5994 - val_loss: 0.4360 - val_output_loss: 0.4190 - val_aux_output_loss: 0.5891\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4146 - output_loss: 0.3956 - aux_output_loss: 0.5855 - val_loss: 0.4089 - val_output_loss: 0.3908 - val_aux_output_loss: 0.5725\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4074 - output_loss: 0.3891 - aux_output_loss: 0.5727 - val_loss: 0.4023 - val_output_loss: 0.3849 - val_aux_output_loss: 0.5588\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4015 - output_loss: 0.3840 - aux_output_loss: 0.5595 - val_loss: 0.3984 - val_output_loss: 0.3811 - val_aux_output_loss: 0.5540\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3980 - output_loss: 0.3810 - aux_output_loss: 0.5509 - val_loss: 0.4048 - val_output_loss: 0.3894 - val_aux_output_loss: 0.5436\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3922 - output_loss: 0.3757 - aux_output_loss: 0.5406 - val_loss: 0.3885 - val_output_loss: 0.3723 - val_aux_output_loss: 0.5341\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3897 - output_loss: 0.3740 - aux_output_loss: 0.5317 - val_loss: 0.3963 - val_output_loss: 0.3806 - val_aux_output_loss: 0.5373\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3862 - output_loss: 0.3710 - aux_output_loss: 0.5228 - val_loss: 0.3823 - val_output_loss: 0.3674 - val_aux_output_loss: 0.5164\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3833 - output_loss: 0.3689 - aux_output_loss: 0.5131 - val_loss: 0.3788 - val_output_loss: 0.3638 - val_aux_output_loss: 0.5146\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3771 - output_loss: 0.3624 - aux_output_loss: 0.5087 - val_loss: 0.3771 - val_output_loss: 0.3623 - val_aux_output_loss: 0.5102\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3737 - output_loss: 0.3595 - aux_output_loss: 0.5011 - val_loss: 0.3718 - val_output_loss: 0.3577 - val_aux_output_loss: 0.4993\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3709 - output_loss: 0.3571 - aux_output_loss: 0.4946 - val_loss: 0.3725 - val_output_loss: 0.3588 - val_aux_output_loss: 0.4954\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will be retured 3 losses on evaluation. total, main & aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 1s 4ms/step - loss: 0.3885 - output_loss: 0.3730 - aux_output_loss: 0.5279\n",
      "0.38849976658821106 0.3730071187019348 0.5279331207275391\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
    "print(total_loss,main_loss,aux_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now here we can predict the two outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy\">Using the Subclassing API to Build Dynamic Models</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models involve loops, varying shapes, conditional branching,\n",
    "and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\n",
    "tive programming style, the Subclassing API is for you.\n",
    "\n",
    "Simply subclass the Model class, create the layers you need in the constructor, and use\n",
    "them to perform the computations you want in the `call()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self,units=30,activation='relu',**kwargs):\n",
    "        super().__init__(**kwargs) # **kwargs handles standard arguments.\n",
    "        self.hidden1 = keras.layers.Dense(units,activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units,activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self,inputs):\n",
    "        input_A,input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output,aux_output\n",
    "model = WideAndDeepModel()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 3s 4ms/step - loss: 2.2681 - output_1_loss: 1.8334 - output_2_loss: 6.1810 - val_loss: 1.3862 - val_output_1_loss: 1.0366 - val_output_2_loss: 4.5322\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.1094 - output_1_loss: 0.8482 - output_2_loss: 3.4602 - val_loss: 0.9480 - val_output_1_loss: 0.7646 - val_output_2_loss: 2.5983\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8859 - output_1_loss: 0.7353 - output_2_loss: 2.2414 - val_loss: 0.8065 - val_output_1_loss: 0.6874 - val_output_2_loss: 1.8785\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7837 - output_1_loss: 0.6754 - output_2_loss: 1.7582 - val_loss: 0.7376 - val_output_1_loss: 0.6419 - val_output_2_loss: 1.5982\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7265 - output_1_loss: 0.6342 - output_2_loss: 1.5570 - val_loss: 0.6880 - val_output_1_loss: 0.6013 - val_output_2_loss: 1.4692\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6875 - output_1_loss: 0.6020 - output_2_loss: 1.4571 - val_loss: 0.6555 - val_output_1_loss: 0.5736 - val_output_2_loss: 1.3927\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6583 - output_1_loss: 0.5763 - output_2_loss: 1.3956 - val_loss: 0.6308 - val_output_1_loss: 0.5522 - val_output_2_loss: 1.3382\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6354 - output_1_loss: 0.5560 - output_2_loss: 1.3494 - val_loss: 0.6104 - val_output_1_loss: 0.5343 - val_output_2_loss: 1.2958\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6166 - output_1_loss: 0.5392 - output_2_loss: 1.3130 - val_loss: 0.5924 - val_output_1_loss: 0.5181 - val_output_2_loss: 1.2616\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6016 - output_1_loss: 0.5260 - output_2_loss: 1.2821 - val_loss: 0.5789 - val_output_1_loss: 0.5063 - val_output_2_loss: 1.2318\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.6037 - output_1_loss: 0.5258 - output_2_loss: 1.3046\n",
      "1/1 [==============================] - 0s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra flexibility does come at a cost: your model’s architecture is hidden within\n",
    "the `call()` method, so Keras cannot easily inspect it; it cannot save or clone it; and\n",
    "when you call the `summary()` method, you only get a list of layers, without any infor‐\n",
    "mation on how they are connected to each other. Moreover, Keras cannot check types\n",
    "and shapes ahead of time, and it is easier to make mistakes\n",
    "### <div style=\"font-family:fantasy\">Saving and Restoring a Model</div>\n",
    "when using the Sequential or Functional API we can save it by running the command:\n",
    "\n",
    "      model.save('dlmodel.h5')\n",
    "`dlmodel` can be repalaced with any name of choice.\n",
    "Again to load the model we can call:\n",
    "\n",
    "      model = keras.models.load_model('dlmodel.h5')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.5192 - val_loss: 0.8354\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7202 - val_loss: 0.6655\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6542 - val_loss: 0.6201\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6191 - val_loss: 0.6012\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5945 - val_loss: 0.5732\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5718 - val_loss: 0.5497\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5537 - val_loss: 0.5418\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5384 - val_loss: 0.5239\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5227 - val_loss: 0.5161\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5126 - val_loss: 0.5017\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.5297\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/housing_price.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./models/housing_price.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5988476],\n",
       "       [2.5174873],\n",
       "       [3.0775604]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new) #type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <div style=\"font-family:fantasy\">Using Callbacks</div>\n",
    "If the training lasts for several hours then we can also save checkpoints at regular intervals during the training, to avoid losing everything. This can be done by using callbacks.\n",
    "\n",
    "The fit() method accepts a callbacks argument that lets you specify a list of objects\n",
    "that Keras will call at the start and end of training, at the start and end of each epoch,\n",
    "and even before and after processing each batch. For example, the ModelCheckpoint\n",
    "callback saves checkpoints of your model at regular intervals during training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7836\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.7641\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6434\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6008\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5690\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5438\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5238\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5079\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4947\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4839\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./models/housing_price_cb.h5')\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we use validation set we can use `save_best_only=True` when creating `ModelCheckpoint`. In this case it will only save the model when it's performance is best so far. Thus we don't need to worry for training too long and overfitting of dataset: simply restore the last model saved after training(when calling the `keras.model.load_model()`), and this will be the best model\n",
    "on the validation set.\n",
    "\n",
    "```\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,validation_data=(X_valid, y_valid),callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model\n",
    "```\n",
    "\n",
    "Another way to implement early stopping is to simply use the EarlyStopping call‐\n",
    "back. It will interrupt training when it measures no progress on the validation set for\n",
    "a number of epochs (defined by the patience argument), and it will optionally roll\n",
    "back to the best model. You can combine both callbacks to save checkpoints of your\n",
    "model (in case your computer crashes) and interrupt training early when there is no\n",
    "more progress (to avoid wasting time and resources)\n",
    "```\n",
    "early_stopping_cb=keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "```\n",
    "\n",
    "The number of epochs can be set to a large value since training will stop automati‐\n",
    "cally when there is no more progress. In this case, there is no need to restore the best\n",
    "model saved because the EarlyStopping callback will keep track of the best weights\n",
    "and restore them for you at the end of training.\n",
    "\n",
    "**custom calback**\n",
    " an\n",
    "example of how to do that, the following custom callback will display the ratio\n",
    "between the validation loss and the training loss during training (e.g., to detect over‐\n",
    "fitting):\n",
    "\n",
    "```\n",
    "class PrintTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(f\"\\nvaltrain:{logs['val_loss']/logs['loss']:.2f}\")\n",
    "```\n",
    "you can implement `on_train_begin()`, `on_train_end()`,\n",
    "`on_epoch_begin()`, `on_epoch_end()`, `on_batch_begin()`, and `on_batch_end()`.\n",
    "Call‐backs can also be used during evaluation and predictions, should you ever need them\n",
    "(e.g., for debugging). For evaluation, you should implement `on_test_begin()`,\n",
    "`on_test_end()`, `on_test_batch_begin()`, or `on_test_batch_end()` ,(called by `evaluate()`), and for prediction you should implement `on_predict_begin()`, `on_predict_end()`, `on_predict_batch_begin()`, or `on_predict_batch_end()` (called by\n",
    "`predict()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Vizualization using TensorBoard</div>\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visu‐\n",
    "alize to special binary log files called *event files*. Each binary data record is called a\n",
    "*summary*. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allows you to visualize\n",
    "live data (with a short delay), such as the learning curves during training. In general,\n",
    "you want to point the TensorBoard server to a root log directory and configure your\n",
    "program so that it writes to a different subdirectory every time it runs. This way, the\n",
    "same TensorBoard server instance will allow you to visualize and compare data from\n",
    "multiple runs of your program, without getting everything mixed up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2023_06_04-20:17:19'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H:%M:%S\")\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we use the TensorBoard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building and compiling \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu, input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=keras.losses.mse, optimizer=keras.optimizers.SGD(learning_rate=5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 5s 10ms/step - loss: 0.9295 - val_loss: 0.6435\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5077 - val_loss: 0.4683\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.4544 - val_loss: 0.4497\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.4253 - val_loss: 0.4227\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 4s 10ms/step - loss: 0.4065 - val_loss: 0.4149\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3958 - val_loss: 0.3998\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3867 - val_loss: 0.4021\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3804 - val_loss: 0.3854\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3841 - val_loss: 0.5023\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3952 - val_loss: 0.4077\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3709 - val_loss: 0.3884\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3648 - val_loss: 0.3632\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3644 - val_loss: 0.3633\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3590 - val_loss: 0.3718\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3567 - val_loss: 0.3546\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3543 - val_loss: 0.3629\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3540 - val_loss: 0.3680\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3503 - val_loss: 0.3529\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3472 - val_loss: 0.3674\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3467 - val_loss: 0.3501\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3430 - val_loss: 0.3580\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3422 - val_loss: 0.3503\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3435 - val_loss: 0.3568\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3432 - val_loss: 0.3669\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3402 - val_loss: 0.3485\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3357 - val_loss: 0.3492\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3350 - val_loss: 0.3431\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3345 - val_loss: 0.3496\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3315 - val_loss: 0.3394\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3290 - val_loss: 0.3323\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,y_train,epochs=30,validation_data=(X_valid,y_valid),callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we start the TensorBoard server by the following command.\n",
    "\n",
    "    $ tensorboard --logdir=./my_logs --port=6006\n",
    "in the terminal\n",
    "\n",
    "and again to kill we can use \n",
    "\n",
    "    $ sudo kill -9 $(sudo lsof -ti:PORT_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a summary writer using `create_file_writer()` for some user defined logs and graphs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy\">Fine Tuning Hyperparameters</div>\n",
    "To tune the model for the best value of the hyperparameters the most naive apprach can be using `GridSearchCV` or `RandomizedSearchCV`. For this we wrapup our keras model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1,n_neurons=30,learning_rate=3e-3,input_shape=[8]):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons,activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss = 'mse',optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `KerasRegressor` based on this `build_model()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156002/1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KerasRegressor` is a thin wrapper around the keras model built using the `build_model()`. Now we can use this object as a regular scikit learn regressor(note that we have not provided any params in `buil_model` so defaults will be used). We can train it using fit method and evaluate the scores as well using `score()` and make the predictions using `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.2909 - val_loss: 0.6709\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6321 - val_loss: 0.5975\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5703 - val_loss: 0.5309\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5275 - val_loss: 0.5020\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4987 - val_loss: 0.4765\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4782 - val_loss: 0.4748\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4643 - val_loss: 0.4496\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4525 - val_loss: 0.4459\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4445 - val_loss: 0.4376\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4369 - val_loss: 0.4243\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4309 - val_loss: 0.4279\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4264 - val_loss: 0.4148\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4225 - val_loss: 0.4102\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4079\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4145 - val_loss: 0.4129\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4025\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4114\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4084\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4045 - val_loss: 0.4044\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.3928\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.3915\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3973 - val_loss: 0.3988\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3964 - val_loss: 0.3976\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3936 - val_loss: 0.3866\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.3845\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3899 - val_loss: 0.3925\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3894 - val_loss: 0.3964\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3879 - val_loss: 0.3824\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3872 - val_loss: 0.3806\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3863 - val_loss: 0.3788\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3846 - val_loss: 0.3791\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3843 - val_loss: 0.3783\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3832 - val_loss: 0.3772\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3821 - val_loss: 0.3747\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3811 - val_loss: 0.3743\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3790 - val_loss: 0.3866\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3799 - val_loss: 0.3793\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3788 - val_loss: 0.3855\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3774 - val_loss: 0.3729\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3780 - val_loss: 0.3712\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3768 - val_loss: 0.3719\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3768 - val_loss: 0.3708\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3757 - val_loss: 0.3689\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3744 - val_loss: 0.3792\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3743 - val_loss: 0.3786\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3742 - val_loss: 0.3775\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3729 - val_loss: 0.3676\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3731 - val_loss: 0.3685\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3731 - val_loss: 0.3661\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3726 - val_loss: 0.3661\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3710 - val_loss: 0.3657\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3709 - val_loss: 0.3674\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3694 - val_loss: 0.3745\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3707 - val_loss: 0.3750\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3705 - val_loss: 0.3737\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3687 - val_loss: 0.3630\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3689 - val_loss: 0.3642\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3684 - val_loss: 0.3632\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3684 - val_loss: 0.3630\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3677 - val_loss: 0.3638\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3679 - val_loss: 0.3620\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3655 - val_loss: 0.3738\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3659 - val_loss: 0.3624\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3651 - val_loss: 0.3726\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3651 - val_loss: 0.3617\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3659 - val_loss: 0.3601\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3641 - val_loss: 0.3719\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3655 - val_loss: 0.3699\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3640 - val_loss: 0.3619\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3633 - val_loss: 0.3741\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3652 - val_loss: 0.3692\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3629 - val_loss: 0.3675\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3622 - val_loss: 0.3731\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3643 - val_loss: 0.3695\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3624 - val_loss: 0.3604\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3631 - val_loss: 0.3591\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3612 - val_loss: 0.3671\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3625 - val_loss: 0.3664\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3616 - val_loss: 0.3572\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3608 - val_loss: 0.3678\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3607 - val_loss: 0.3572\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3608 - val_loss: 0.3619\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3595 - val_loss: 0.3683\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3613 - val_loss: 0.3647\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3608 - val_loss: 0.3670\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3594 - val_loss: 0.3589\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3590 - val_loss: 0.3673\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3589 - val_loss: 0.3589\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3602 - val_loss: 0.3583\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3578 - val_loss: 0.3652\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3592 - val_loss: 0.3675\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3811\n",
      "WARNING:tensorflow:5 out of the last 1300 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fae5b88cae0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1300 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fae5b88cae0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,validation_data=(X_valid, y_valid),callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here the score is calculated not the loss thus it is opposite. Higher is better. Also as here several hyperparams are there thus we use Randomized search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 4ms/step - loss: 4.9485 - val_loss: 3.7687\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.6504 - val_loss: 2.0731\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.6439 - val_loss: 1.3438\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.1671 - val_loss: 1.0016\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9300 - val_loss: 0.8348\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8078 - val_loss: 0.7490\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7423 - val_loss: 0.7035\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7056 - val_loss: 0.6777\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6834 - val_loss: 0.6614\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6689 - val_loss: 0.6503\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6585 - val_loss: 0.6419\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6504 - val_loss: 0.6350\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6436 - val_loss: 0.6290\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6376 - val_loss: 0.6237\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6323 - val_loss: 0.6187\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6272 - val_loss: 0.6140\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6225 - val_loss: 0.6095\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6181 - val_loss: 0.6052\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6139 - val_loss: 0.6011\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6099 - val_loss: 0.5973\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6061 - val_loss: 0.5937\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6026 - val_loss: 0.5903\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5991 - val_loss: 0.5870\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5958 - val_loss: 0.5838\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5928 - val_loss: 0.5808\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5899 - val_loss: 0.5780\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5871 - val_loss: 0.5753\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5844 - val_loss: 0.5728\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5819 - val_loss: 0.5704\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5794 - val_loss: 0.5681\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5772 - val_loss: 0.5658\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5749 - val_loss: 0.5639\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5728 - val_loss: 0.5617\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5708 - val_loss: 0.5599\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5689 - val_loss: 0.5579\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5671 - val_loss: 0.5563\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5653 - val_loss: 0.5547\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5636 - val_loss: 0.5531\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5620 - val_loss: 0.5515\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5605 - val_loss: 0.5499\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5590 - val_loss: 0.5485\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5576 - val_loss: 0.5472\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5563 - val_loss: 0.5461\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5550 - val_loss: 0.5449\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.5436\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5525 - val_loss: 0.5423\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5514 - val_loss: 0.5413\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5503 - val_loss: 0.5406\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5492 - val_loss: 0.5393\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5482 - val_loss: 0.5382\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5473 - val_loss: 0.5374\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5464 - val_loss: 0.5365\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5455 - val_loss: 0.5357\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5446 - val_loss: 0.5350\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.5344\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5335\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5422 - val_loss: 0.5328\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 0.5323\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5408 - val_loss: 0.5315\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5401 - val_loss: 0.5309\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5395 - val_loss: 0.5303\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5388 - val_loss: 0.5298\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5292\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.5288\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 0.5281\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5275\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 0.5270\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 0.5266\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5263\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5346 - val_loss: 0.5257\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5253\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 0.5250\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.5244\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5241\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 0.5238\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5322 - val_loss: 0.5235\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5232\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5230\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5230\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5308 - val_loss: 0.5226\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5304 - val_loss: 0.5221\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5302 - val_loss: 0.5220\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5299 - val_loss: 0.5217\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5296 - val_loss: 0.5215\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5293 - val_loss: 0.5211\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5209\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5288 - val_loss: 0.5205\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5286 - val_loss: 0.5206\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5283 - val_loss: 0.5203\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5281 - val_loss: 0.5199\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5198\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5196\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5193\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.5192\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5193\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5269 - val_loss: 0.5192\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5267 - val_loss: 0.5192\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5266 - val_loss: 0.5188\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5264 - val_loss: 0.5185\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5261 - val_loss: 0.5182\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5234\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 5.4198 - val_loss: 7.7575\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.1140 - val_loss: 6.1046\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9410 - val_loss: 5.2718\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3265 - val_loss: 4.8424\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9957 - val_loss: 4.6206\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8140 - val_loss: 4.5100\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7118 - val_loss: 4.4603\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6526 - val_loss: 4.4446\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6172 - val_loss: 4.4487\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5951 - val_loss: 4.4654\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5806 - val_loss: 4.4894\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5706 - val_loss: 4.5185\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5632 - val_loss: 4.5506\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5576 - val_loss: 4.5853\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5530 - val_loss: 4.6216\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5491 - val_loss: 4.6588\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5457 - val_loss: 4.6967\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5427 - val_loss: 4.7354\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 2.2674\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 4.2277 - val_loss: 3.1203\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 2.4646 - val_loss: 1.9022\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.5805 - val_loss: 1.2871\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.1289 - val_loss: 0.9708\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8949 - val_loss: 0.8061\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7713 - val_loss: 0.7181\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7042 - val_loss: 0.6694\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6662 - val_loss: 0.6415\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6436 - val_loss: 0.6242\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6290 - val_loss: 0.6125\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6187 - val_loss: 0.6040\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6108 - val_loss: 0.5972\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6043 - val_loss: 0.5915\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5987 - val_loss: 0.5864\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5936 - val_loss: 0.5819\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5891 - val_loss: 0.5777\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5849 - val_loss: 0.5738\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5809 - val_loss: 0.5704\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5773 - val_loss: 0.5671\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5640\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5706 - val_loss: 0.5611\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 0.5582\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5648 - val_loss: 0.5558\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5621 - val_loss: 0.5534\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 0.5511\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5573 - val_loss: 0.5489\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5551 - val_loss: 0.5470\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5530 - val_loss: 0.5451\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5512 - val_loss: 0.5435\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5492 - val_loss: 0.5418\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5475 - val_loss: 0.5403\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5459 - val_loss: 0.5389\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5444 - val_loss: 0.5376\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5429 - val_loss: 0.5363\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5416 - val_loss: 0.5351\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5403 - val_loss: 0.5341\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5391 - val_loss: 0.5330\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5380 - val_loss: 0.5321\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5369 - val_loss: 0.5313\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5359 - val_loss: 0.5303\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5350 - val_loss: 0.5295\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5341 - val_loss: 0.5287\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5333 - val_loss: 0.5281\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5325 - val_loss: 0.5274\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5317 - val_loss: 0.5268\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5311 - val_loss: 0.5263\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5258\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5297 - val_loss: 0.5252\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5292 - val_loss: 0.5248\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5286 - val_loss: 0.5243\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5281 - val_loss: 0.5239\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5276 - val_loss: 0.5235\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5271 - val_loss: 0.5233\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5266 - val_loss: 0.5231\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5262 - val_loss: 0.5225\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 3s 14ms/step - loss: 0.5258 - val_loss: 0.5222\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 3s 13ms/step - loss: 0.5254 - val_loss: 0.5220\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.5251 - val_loss: 0.5217\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5247 - val_loss: 0.5216\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 0.5244 - val_loss: 0.5213\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5240 - val_loss: 0.5211\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5238 - val_loss: 0.5210\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.5206\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5232 - val_loss: 0.5206\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.5203\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5227 - val_loss: 0.5197\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.5197\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5223 - val_loss: 0.5194\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5221 - val_loss: 0.5192\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5220 - val_loss: 0.5193\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5218 - val_loss: 0.5194\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5216 - val_loss: 0.5192\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5214 - val_loss: 0.5190\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5213 - val_loss: 0.5191\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5211 - val_loss: 0.5190\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5210 - val_loss: 0.5186\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5208 - val_loss: 0.5186\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5207 - val_loss: 0.5184\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5206 - val_loss: 0.5186\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5204 - val_loss: 0.5184\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5204 - val_loss: 0.5183\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5202 - val_loss: 0.5181\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5201 - val_loss: 0.5180\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5200 - val_loss: 0.5180\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5199 - val_loss: 0.5180\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5198 - val_loss: 0.5178\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5198 - val_loss: 0.5176\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5196 - val_loss: 0.5173\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5196 - val_loss: 0.5173\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5195 - val_loss: 0.5174\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5195 - val_loss: 0.5175\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5173\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5193 - val_loss: 0.5173\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5192 - val_loss: 0.5172\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5192 - val_loss: 0.5173\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5191 - val_loss: 0.5171\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5191 - val_loss: 0.5172\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5190 - val_loss: 0.5172\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5189 - val_loss: 0.5171\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5189 - val_loss: 0.5170\n",
      "121/121 [==============================] - 1s 3ms/step - loss: 0.5325\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0009 - val_loss: 0.6074\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5218 - val_loss: 0.4792\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4567 - val_loss: 0.4400\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4329 - val_loss: 0.4199\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4104\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4036 - val_loss: 0.3979\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3937 - val_loss: 0.3926\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3855 - val_loss: 0.3812\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3820 - val_loss: 0.3755\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3760 - val_loss: 0.3777\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3705 - val_loss: 0.3710\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3681 - val_loss: 0.3697\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3650 - val_loss: 0.3756\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3635 - val_loss: 0.3634\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3573 - val_loss: 0.3638\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3539 - val_loss: 0.3607\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3528 - val_loss: 0.3558\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3499 - val_loss: 0.3605\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3482 - val_loss: 0.3586\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3474 - val_loss: 0.3519\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3433 - val_loss: 0.3567\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 18s 74ms/step - loss: 0.3415 - val_loss: 0.3438\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 11s 46ms/step - loss: 0.3384 - val_loss: 0.3458\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.3375 - val_loss: 0.3540\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 3s 13ms/step - loss: 0.3339 - val_loss: 0.3517\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 0.3360 - val_loss: 0.3422\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 3s 13ms/step - loss: 0.3335 - val_loss: 0.3508\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.3309 - val_loss: 0.3355\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 4s 15ms/step - loss: 0.3298 - val_loss: 0.3448\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 0.3294 - val_loss: 0.3494\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3267 - val_loss: 0.3323\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3272 - val_loss: 0.3377\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3258 - val_loss: 0.3367\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3236 - val_loss: 0.3294\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3271 - val_loss: 0.3309\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3243 - val_loss: 0.3279\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3231 - val_loss: 0.3267\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.3211 - val_loss: 0.3386\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3213 - val_loss: 0.3258\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.3262\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3177 - val_loss: 0.3228\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3170 - val_loss: 0.3303\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3169 - val_loss: 0.3237\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3158 - val_loss: 0.3294\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3135 - val_loss: 0.3507\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3145 - val_loss: 0.3225\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3136 - val_loss: 0.3190\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3124 - val_loss: 0.3203\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3115 - val_loss: 0.3180\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3111 - val_loss: 0.3206\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3117 - val_loss: 0.3199\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3103 - val_loss: 0.3193\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3093 - val_loss: 0.3155\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3083 - val_loss: 0.3216\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3083 - val_loss: 0.3142\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3072 - val_loss: 0.3558\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3122 - val_loss: 0.3148\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3069 - val_loss: 0.3303\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3062 - val_loss: 0.3182\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3055 - val_loss: 0.3122\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3042 - val_loss: 0.3173\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3049 - val_loss: 0.3168\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3040 - val_loss: 0.3178\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3037 - val_loss: 0.3168\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3025 - val_loss: 0.3141\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3018 - val_loss: 0.3218\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3020 - val_loss: 0.3109\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3014 - val_loss: 0.3178\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3018 - val_loss: 0.3136\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3015 - val_loss: 0.3714\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3110 - val_loss: 0.3107\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3011 - val_loss: 0.3102\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3021 - val_loss: 0.3121\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3002 - val_loss: 0.3147\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3006 - val_loss: 0.3107\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2993 - val_loss: 0.3104\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2987 - val_loss: 0.3358\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2981 - val_loss: 0.3187\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2987 - val_loss: 0.3176\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2983 - val_loss: 0.3113\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2986 - val_loss: 0.3060\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2972 - val_loss: 0.3130\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2961 - val_loss: 0.3037\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2948 - val_loss: 0.3150\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2960 - val_loss: 0.3075\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2959 - val_loss: 0.3061\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2957 - val_loss: 0.3084\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2967 - val_loss: 0.3089\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2955 - val_loss: 0.3061\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2955 - val_loss: 0.3029\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2940 - val_loss: 0.3099\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2955 - val_loss: 0.3023\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2937 - val_loss: 0.3027\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2931 - val_loss: 0.3124\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2925 - val_loss: 0.3014\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2933 - val_loss: 0.3027\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2917 - val_loss: 0.3015\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2926 - val_loss: 0.3119\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2921 - val_loss: 0.3038\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2920 - val_loss: 0.3114\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3209\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0514 - val_loss: 1.2186\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5763 - val_loss: 0.6382\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5042 - val_loss: 0.4882\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4641 - val_loss: 0.4547\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4417 - val_loss: 0.4484\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4267 - val_loss: 0.4346\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4124 - val_loss: 0.4320\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4297\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3965 - val_loss: 0.4303\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.4245\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.4142\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3785 - val_loss: 0.4012\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3737 - val_loss: 0.3880\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3692 - val_loss: 0.3803\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3765\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3795\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3658\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3569 - val_loss: 0.3607\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3545 - val_loss: 0.3553\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3508 - val_loss: 0.3553\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3490 - val_loss: 0.3496\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3463 - val_loss: 0.3528\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3444 - val_loss: 0.3459\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3427 - val_loss: 0.3451\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3396 - val_loss: 0.3500\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3382 - val_loss: 0.3630\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3358 - val_loss: 0.3566\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3361 - val_loss: 0.3645\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3335 - val_loss: 0.3550\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3315 - val_loss: 0.3657\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3304 - val_loss: 0.3818\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3292 - val_loss: 0.3820\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3280 - val_loss: 0.3794\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3264 - val_loss: 0.3864\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3488\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.2869 - val_loss: 0.8080\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6078 - val_loss: 0.5470\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5122 - val_loss: 0.4857\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 0.4541\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.4277\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4128 - val_loss: 0.4180\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.4099\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3919 - val_loss: 0.3978\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3856 - val_loss: 0.3931\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.3896\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3850\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3724 - val_loss: 0.3783\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3676 - val_loss: 0.3750\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3639 - val_loss: 0.3873\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3609 - val_loss: 0.3722\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3582 - val_loss: 0.3805\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3576 - val_loss: 0.3670\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3535 - val_loss: 0.3677\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3521 - val_loss: 0.3794\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3505 - val_loss: 0.3571\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3469 - val_loss: 0.3647\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3463 - val_loss: 0.3559\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3447 - val_loss: 0.3541\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3409 - val_loss: 0.3654\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3405 - val_loss: 0.3530\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3386 - val_loss: 0.3516\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3385 - val_loss: 0.3460\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3359 - val_loss: 0.3462\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3344 - val_loss: 0.3415\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3332 - val_loss: 0.3449\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3319 - val_loss: 0.3371\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3298 - val_loss: 0.3437\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3396\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3271 - val_loss: 0.3404\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3384\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3253 - val_loss: 0.3412\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3244 - val_loss: 0.3372\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3229 - val_loss: 0.3327\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3230 - val_loss: 0.3313\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3197 - val_loss: 0.3298\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3193 - val_loss: 0.3314\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3191 - val_loss: 0.3293\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3184 - val_loss: 0.3262\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3161 - val_loss: 0.3262\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3152 - val_loss: 0.3284\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3143 - val_loss: 0.3223\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3140 - val_loss: 0.3225\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3127 - val_loss: 0.3195\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3121 - val_loss: 0.3195\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3108 - val_loss: 0.3176\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3094 - val_loss: 0.3172\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.3190\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3091 - val_loss: 0.3122\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3069 - val_loss: 0.3138\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3077 - val_loss: 0.3115\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3057 - val_loss: 0.3171\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3052 - val_loss: 0.3122\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3046 - val_loss: 0.3146\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3026 - val_loss: 0.3192\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3039 - val_loss: 0.3134\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3027 - val_loss: 0.3095\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3002 - val_loss: 0.3295\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3008 - val_loss: 0.3143\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3005 - val_loss: 0.3055\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2994 - val_loss: 0.3026\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2984 - val_loss: 0.3042\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.3021\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2974 - val_loss: 0.3030\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2966 - val_loss: 0.3022\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2962 - val_loss: 0.3034\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2953 - val_loss: 0.3010\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2929 - val_loss: 0.2997\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2939 - val_loss: 0.3039\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2930 - val_loss: 0.3004\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2923 - val_loss: 0.3034\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2916 - val_loss: 0.2999\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2923 - val_loss: 0.3033\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2904 - val_loss: 0.2977\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.3019\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2904 - val_loss: 0.2974\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2883 - val_loss: 0.3001\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2894 - val_loss: 0.2965\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2888 - val_loss: 0.2943\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2876 - val_loss: 0.2985\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2879 - val_loss: 0.2953\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2862 - val_loss: 0.3033\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2875 - val_loss: 0.2991\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2860 - val_loss: 0.3010\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2861 - val_loss: 0.2990\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2854 - val_loss: 0.2939\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2834 - val_loss: 0.2952\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2850 - val_loss: 0.2914\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2837 - val_loss: 0.2948\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2828 - val_loss: 0.2938\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2821 - val_loss: 0.2961\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2811 - val_loss: 0.2942\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2823 - val_loss: 0.2986\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2824 - val_loss: 0.2898\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2813 - val_loss: 0.2953\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2800 - val_loss: 0.2984\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3157\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.1295 - val_loss: 1.9183\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4114 - val_loss: 1.1921\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0168 - val_loss: 0.9603\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8891 - val_loss: 0.8498\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8258 - val_loss: 0.7895\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7856 - val_loss: 0.7490\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7561 - val_loss: 0.7220\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7329 - val_loss: 0.7008\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7131 - val_loss: 0.6829\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6961 - val_loss: 0.6673\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6807 - val_loss: 0.6534\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6666 - val_loss: 0.6408\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6536 - val_loss: 0.6293\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6416 - val_loss: 0.6184\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6305 - val_loss: 0.6082\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6200 - val_loss: 0.5987\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6101 - val_loss: 0.5897\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6007 - val_loss: 0.5812\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5919 - val_loss: 0.5731\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5835 - val_loss: 0.5654\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5752 - val_loss: 0.5579\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5676 - val_loss: 0.5508\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5602 - val_loss: 0.5440\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5532 - val_loss: 0.5377\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5467 - val_loss: 0.5317\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5406 - val_loss: 0.5261\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5349 - val_loss: 0.5208\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.5159\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5246 - val_loss: 0.5111\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5198 - val_loss: 0.5067\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5152 - val_loss: 0.5025\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5109 - val_loss: 0.4985\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5070 - val_loss: 0.4949\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5033 - val_loss: 0.4914\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.4881\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4965 - val_loss: 0.4850\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4934 - val_loss: 0.4821\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4905 - val_loss: 0.4794\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4876 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4850 - val_loss: 0.4743\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4825 - val_loss: 0.4721\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4801 - val_loss: 0.4698\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4778 - val_loss: 0.4677\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4757 - val_loss: 0.4657\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4736 - val_loss: 0.4639\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4717 - val_loss: 0.4620\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4698 - val_loss: 0.4601\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4679 - val_loss: 0.4586\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4662 - val_loss: 0.4571\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4646 - val_loss: 0.4555\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4630 - val_loss: 0.4541\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4615 - val_loss: 0.4527\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4600 - val_loss: 0.4514\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4586 - val_loss: 0.4502\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4572 - val_loss: 0.4489\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4559 - val_loss: 0.4478\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4468\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4533 - val_loss: 0.4456\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4521 - val_loss: 0.4445\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4509 - val_loss: 0.4436\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4498 - val_loss: 0.4425\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4486 - val_loss: 0.4416\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4475 - val_loss: 0.4405\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4464 - val_loss: 0.4395\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4453 - val_loss: 0.4386\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4443 - val_loss: 0.4378\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4433 - val_loss: 0.4369\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 0.4423 - val_loss: 0.4361\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4413 - val_loss: 0.4355\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4404 - val_loss: 0.4344\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4395 - val_loss: 0.4337\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4384 - val_loss: 0.4329\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4378 - val_loss: 0.4321\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4368 - val_loss: 0.4316\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4360 - val_loss: 0.4308\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.4302\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4344 - val_loss: 0.4294\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4335 - val_loss: 0.4288\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.4283\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4320 - val_loss: 0.4275\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4312 - val_loss: 0.4268\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4306 - val_loss: 0.4262\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4298 - val_loss: 0.4256\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4290 - val_loss: 0.4250\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4283 - val_loss: 0.4245\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4277 - val_loss: 0.4239\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4270 - val_loss: 0.4234\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4263 - val_loss: 0.4228\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4257 - val_loss: 0.4223\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4251 - val_loss: 0.4216\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4245 - val_loss: 0.4212\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4238 - val_loss: 0.4207\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4232 - val_loss: 0.4201\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4225 - val_loss: 0.4195\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4220 - val_loss: 0.4189\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4213 - val_loss: 0.4184\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4208 - val_loss: 0.4179\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4175\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4197 - val_loss: 0.4171\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4191 - val_loss: 0.4166\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4214\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.8583 - val_loss: 2.7886\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7240 - val_loss: 2.1977\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0914 - val_loss: 2.0163\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8763 - val_loss: 1.8779\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7884 - val_loss: 1.7468\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7436 - val_loss: 1.6246\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7157 - val_loss: 1.5145\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6951 - val_loss: 1.4051\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6782 - val_loss: 1.3162\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 1.2351\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6499 - val_loss: 1.1549\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6377 - val_loss: 1.0836\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6263 - val_loss: 1.0187\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6157 - val_loss: 0.9583\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6058 - val_loss: 0.9024\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5965 - val_loss: 0.8503\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5877 - val_loss: 0.8021\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5794 - val_loss: 0.7591\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5717 - val_loss: 0.7199\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5645 - val_loss: 0.6842\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5578 - val_loss: 0.6512\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.5514 - val_loss: 0.6217\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5455 - val_loss: 0.5972\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5399 - val_loss: 0.5746\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5347 - val_loss: 0.5559\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5296 - val_loss: 0.5397\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5249 - val_loss: 0.5261\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.5205 - val_loss: 0.5152\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.5163 - val_loss: 0.5067\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.5123 - val_loss: 0.5007\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.5086 - val_loss: 0.4969\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 3s 14ms/step - loss: 0.5050 - val_loss: 0.4953\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 18s 73ms/step - loss: 0.5017 - val_loss: 0.4957\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 3s 10ms/step - loss: 0.4985 - val_loss: 0.4978\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 0.4955 - val_loss: 0.5011\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.4928 - val_loss: 0.5066\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 6s 23ms/step - loss: 0.4901 - val_loss: 0.5138\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4876 - val_loss: 0.5225\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4853 - val_loss: 0.5273\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4830 - val_loss: 0.5332\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4809 - val_loss: 0.5386\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4788 - val_loss: 0.5441\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5130\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.0538 - val_loss: 2.2787\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8918 - val_loss: 1.5632\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3516 - val_loss: 1.1587\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0476 - val_loss: 0.9279\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8768 - val_loss: 0.8032\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7832 - val_loss: 0.7362\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7313 - val_loss: 0.6984\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7003 - val_loss: 0.6748\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6795 - val_loss: 0.6582\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6640 - val_loss: 0.6453\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6512 - val_loss: 0.6340\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6401 - val_loss: 0.6242\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6300 - val_loss: 0.6153\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6207 - val_loss: 0.6065\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6122 - val_loss: 0.5986\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6040 - val_loss: 0.5916\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5964 - val_loss: 0.5842\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5891 - val_loss: 0.5774\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5823 - val_loss: 0.5714\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5758 - val_loss: 0.5651\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5695 - val_loss: 0.5597\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5634 - val_loss: 0.5533\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5575 - val_loss: 0.5484\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5520 - val_loss: 0.5425\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5466 - val_loss: 0.5377\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5413 - val_loss: 0.5333\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5361 - val_loss: 0.5285\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5312 - val_loss: 0.5239\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.5198\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5220 - val_loss: 0.5159\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5177 - val_loss: 0.5123\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5134 - val_loss: 0.5085\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5092 - val_loss: 0.5058\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5054 - val_loss: 0.5019\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5016 - val_loss: 0.4984\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.4951\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4947 - val_loss: 0.4925\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.4896\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4886 - val_loss: 0.4871\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4857 - val_loss: 0.4840\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4830 - val_loss: 0.4816\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4805 - val_loss: 0.4793\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4781 - val_loss: 0.4773\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4757 - val_loss: 0.4760\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4737 - val_loss: 0.4740\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4716 - val_loss: 0.4721\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4696 - val_loss: 0.4699\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4678 - val_loss: 0.4678\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4660 - val_loss: 0.4661\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4644 - val_loss: 0.4646\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4628 - val_loss: 0.4638\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4612 - val_loss: 0.4625\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4597 - val_loss: 0.4605\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4584 - val_loss: 0.4594\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4570 - val_loss: 0.4580\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4558 - val_loss: 0.4569\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4545 - val_loss: 0.4557\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4533 - val_loss: 0.4545\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4520 - val_loss: 0.4538\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4509 - val_loss: 0.4522\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4498 - val_loss: 0.4513\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4488 - val_loss: 0.4503\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.4477 - val_loss: 0.4497\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4467 - val_loss: 0.4484\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4457 - val_loss: 0.4478\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4447 - val_loss: 0.4468\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4438 - val_loss: 0.4459\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4428 - val_loss: 0.4447\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.4419 - val_loss: 0.4444\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.4411 - val_loss: 0.4438\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.4402 - val_loss: 0.4431\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4393 - val_loss: 0.4418\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4385 - val_loss: 0.4406\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4376 - val_loss: 0.4395\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.4369 - val_loss: 0.4389\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4361 - val_loss: 0.4380\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4353 - val_loss: 0.4373\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4345 - val_loss: 0.4370\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4337 - val_loss: 0.4358\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4330 - val_loss: 0.4357\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4323 - val_loss: 0.4350\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4316 - val_loss: 0.4340\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4310 - val_loss: 0.4331\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4303 - val_loss: 0.4323\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4296 - val_loss: 0.4316\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.4289 - val_loss: 0.4309\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4283 - val_loss: 0.4302\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4277 - val_loss: 0.4295\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4270 - val_loss: 0.4295\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4264 - val_loss: 0.4288\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4258 - val_loss: 0.4278\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4252 - val_loss: 0.4272\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4246 - val_loss: 0.4267\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4240 - val_loss: 0.4261\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4234 - val_loss: 0.4254\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4229 - val_loss: 0.4248\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4223 - val_loss: 0.4245\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4217 - val_loss: 0.4238\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.4236\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4205 - val_loss: 0.4226\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.4415\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.5733 - val_loss: 0.9354\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8068 - val_loss: 0.7892\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7155 - val_loss: 0.6659\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6673 - val_loss: 0.6270\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6293 - val_loss: 0.5958\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6006 - val_loss: 0.5757\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5742 - val_loss: 0.5551\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5574 - val_loss: 0.5401\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5390 - val_loss: 0.5249\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.5070\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5136 - val_loss: 0.5077\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5010 - val_loss: 0.4881\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.4981 - val_loss: 0.4846\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4878 - val_loss: 0.4830\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.4801 - val_loss: 0.4703\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4741 - val_loss: 0.4627\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4665 - val_loss: 0.4635\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4618 - val_loss: 0.4533\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4587 - val_loss: 0.4584\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4527 - val_loss: 0.4549\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4530 - val_loss: 0.4470\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4465 - val_loss: 0.4500\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4429 - val_loss: 0.4531\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4391 - val_loss: 0.4325\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4359 - val_loss: 0.4390\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4340 - val_loss: 0.4493\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4328 - val_loss: 0.4455\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4276 - val_loss: 0.4227\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4280 - val_loss: 0.4268\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4252 - val_loss: 0.4188\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4209 - val_loss: 0.4304\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4210 - val_loss: 0.4471\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4170 - val_loss: 0.4128\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4206\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4133 - val_loss: 0.4100\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4136 - val_loss: 0.4090\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4107 - val_loss: 0.4081\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4085 - val_loss: 0.4056\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4074 - val_loss: 0.4046\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4028\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4055 - val_loss: 0.4009\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4046 - val_loss: 0.4034\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4045 - val_loss: 0.4062\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.4028\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4012 - val_loss: 0.3970\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3980 - val_loss: 0.4006\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3964 - val_loss: 0.3934\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3958 - val_loss: 0.3964\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3950 - val_loss: 0.4003\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3950 - val_loss: 0.4071\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3951 - val_loss: 0.4098\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3917 - val_loss: 0.3892\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3904 - val_loss: 0.3947\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3895 - val_loss: 0.3871\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3886 - val_loss: 0.3922\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3878 - val_loss: 0.3848\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3870 - val_loss: 0.3866\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3860 - val_loss: 0.3958\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3851 - val_loss: 0.3831\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3842 - val_loss: 0.3931\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3837 - val_loss: 0.3806\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3828 - val_loss: 0.3852\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3827 - val_loss: 0.3953\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3813 - val_loss: 0.3796\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3803 - val_loss: 0.3849\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3808 - val_loss: 0.3941\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3803 - val_loss: 0.3930\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3804 - val_loss: 0.3932\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3776 - val_loss: 0.3752\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3771 - val_loss: 0.3743\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3763 - val_loss: 0.3807\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3759 - val_loss: 0.3881\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3762 - val_loss: 0.3872\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3741 - val_loss: 0.3727\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3733 - val_loss: 0.3786\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3721 - val_loss: 0.3709\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3715 - val_loss: 0.3771\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3719 - val_loss: 0.3820\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3719 - val_loss: 0.3833\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3708 - val_loss: 0.3815\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3704 - val_loss: 0.3844\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3704 - val_loss: 0.3787\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3690 - val_loss: 0.3783\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3670 - val_loss: 0.3677\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3666 - val_loss: 0.3645\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3660 - val_loss: 0.3644\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3656 - val_loss: 0.3660\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3643 - val_loss: 0.3645\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3637 - val_loss: 0.3631\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3631 - val_loss: 0.3638\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3625 - val_loss: 0.3609\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3622 - val_loss: 0.3663\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3615 - val_loss: 0.3609\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3616 - val_loss: 0.3605\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3619 - val_loss: 0.3612\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3604 - val_loss: 0.3594\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3609 - val_loss: 0.3627\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3611 - val_loss: 0.3577\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3608 - val_loss: 0.3614\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3612 - val_loss: 0.3570\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3648\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 1.7995 - val_loss: 1.4886\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7632 - val_loss: 1.0837\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6919 - val_loss: 0.8309\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6408 - val_loss: 0.6696\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6007 - val_loss: 0.5813\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5686 - val_loss: 0.5418\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5424 - val_loss: 0.5416\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5213 - val_loss: 0.5587\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5050 - val_loss: 0.6060\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4911 - val_loss: 0.6563\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4801 - val_loss: 0.7320\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4703 - val_loss: 0.8033\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4622 - val_loss: 0.8777\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4552 - val_loss: 0.9421\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4495 - val_loss: 1.0324\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4443 - val_loss: 1.1117\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4398 - val_loss: 1.1891\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.7694\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 2.1489 - val_loss: 0.8158\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6860 - val_loss: 0.6380\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6199 - val_loss: 0.6012\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5824 - val_loss: 0.5654\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5495 - val_loss: 0.5359\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5246 - val_loss: 0.5112\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5062 - val_loss: 0.5022\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4928 - val_loss: 0.4875\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4812 - val_loss: 0.4733\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4727 - val_loss: 0.4660\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4663 - val_loss: 0.4596\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4604 - val_loss: 0.4551\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4556 - val_loss: 0.4507\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4518 - val_loss: 0.4470\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4465 - val_loss: 0.4497\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4441 - val_loss: 0.4398\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4414 - val_loss: 0.4368\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4379 - val_loss: 0.4381\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4350 - val_loss: 0.4312\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4327 - val_loss: 0.4286\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4298 - val_loss: 0.4293\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4273 - val_loss: 0.4265\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4250 - val_loss: 0.4247\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4228 - val_loss: 0.4216\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4205 - val_loss: 0.4194\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4180 - val_loss: 0.4182\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4127\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4135 - val_loss: 0.4127\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4122 - val_loss: 0.4127\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4101 - val_loss: 0.4073\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4077 - val_loss: 0.4089\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4065 - val_loss: 0.4074\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4051 - val_loss: 0.4040\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4031 - val_loss: 0.4041\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4009\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4003 - val_loss: 0.3983\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3982 - val_loss: 0.3995\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3972 - val_loss: 0.3977\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.3973\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3944 - val_loss: 0.3965\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.3940\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.3904\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3903 - val_loss: 0.3921\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3917\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3875 - val_loss: 0.3870\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3873 - val_loss: 0.3857\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.3854\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3848 - val_loss: 0.3863\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3838 - val_loss: 0.3854\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3830 - val_loss: 0.3843\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3821 - val_loss: 0.3849\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3805 - val_loss: 0.3815\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3805 - val_loss: 0.3805\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3791 - val_loss: 0.3811\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3782 - val_loss: 0.3782\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3774 - val_loss: 0.3807\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3775 - val_loss: 0.3792\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3766 - val_loss: 0.3790\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3759 - val_loss: 0.3779\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3751 - val_loss: 0.3773\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3741 - val_loss: 0.3746\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3732 - val_loss: 0.3757\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3732 - val_loss: 0.3747\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3725 - val_loss: 0.3755\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3740\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3715 - val_loss: 0.3731\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3710\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3696 - val_loss: 0.3720\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3697 - val_loss: 0.3721\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3694 - val_loss: 0.3717\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3681 - val_loss: 0.3694\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3678 - val_loss: 0.3692\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3671 - val_loss: 0.3707\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3668 - val_loss: 0.3686\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3661 - val_loss: 0.3697\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3657 - val_loss: 0.3683\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3683\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3653 - val_loss: 0.3672\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.3669\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3641 - val_loss: 0.3667\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3637 - val_loss: 0.3673\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3630 - val_loss: 0.3631\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3625 - val_loss: 0.3646\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3625 - val_loss: 0.3638\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3615 - val_loss: 0.3634\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3609 - val_loss: 0.3634\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3610 - val_loss: 0.3648\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3602 - val_loss: 0.3624\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3596 - val_loss: 0.3628\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3597 - val_loss: 0.3623\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3586 - val_loss: 0.3603\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3584 - val_loss: 0.3609\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3583 - val_loss: 0.3620\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3572 - val_loss: 0.3600\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.3597\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3573 - val_loss: 0.3626\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3574 - val_loss: 0.3615\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3562 - val_loss: 0.3589\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3596\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3551 - val_loss: 0.3594\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3678\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.3714 - val_loss: 1.8969\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.4004 - val_loss: 1.0237\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9111 - val_loss: 0.7961\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7723 - val_loss: 0.7243\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7213 - val_loss: 0.6927\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6943 - val_loss: 0.6723\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6751 - val_loss: 0.6559\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6594 - val_loss: 0.6408\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6454 - val_loss: 0.6282\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6333 - val_loss: 0.6166\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6224 - val_loss: 0.6065\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.5978\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6047 - val_loss: 0.5893\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 0.5833\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5901 - val_loss: 0.5764\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5710\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5784 - val_loss: 0.5659\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5733 - val_loss: 0.5605\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5693 - val_loss: 0.5569\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5651 - val_loss: 0.5534\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5616 - val_loss: 0.5495\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5584 - val_loss: 0.5472\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 0.5439\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5526 - val_loss: 0.5425\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5502 - val_loss: 0.5396\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5480 - val_loss: 0.5372\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5460 - val_loss: 0.5352\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5440 - val_loss: 0.5349\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5423 - val_loss: 0.5319\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5409 - val_loss: 0.5304\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5396 - val_loss: 0.5294\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5382 - val_loss: 0.5282\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5370 - val_loss: 0.5270\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5359 - val_loss: 0.5264\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5347 - val_loss: 0.5252\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5341 - val_loss: 0.5246\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5332 - val_loss: 0.5244\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5324 - val_loss: 0.5236\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 3s 10ms/step - loss: 0.5316 - val_loss: 0.5230\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 24s 101ms/step - loss: 0.5309 - val_loss: 0.5222\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 19s 76ms/step - loss: 0.5303 - val_loss: 0.5220\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.5295 - val_loss: 0.5205\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5292 - val_loss: 0.5209\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5286 - val_loss: 0.5203\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5281 - val_loss: 0.5203\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5276 - val_loss: 0.5188\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5273 - val_loss: 0.5184\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5269 - val_loss: 0.5182\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5267 - val_loss: 0.5185\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5263 - val_loss: 0.5189\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5191\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5257 - val_loss: 0.5183\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5170\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5250 - val_loss: 0.5176\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5251 - val_loss: 0.5173\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5249 - val_loss: 0.5177\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5245 - val_loss: 0.5171\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5242 - val_loss: 0.5165\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5244 - val_loss: 0.5167\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5242 - val_loss: 0.5164\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5237 - val_loss: 0.5156\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.5160\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5237 - val_loss: 0.5172\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5235 - val_loss: 0.5157\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5235 - val_loss: 0.5169\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5234 - val_loss: 0.5166\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5233 - val_loss: 0.5162\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5231 - val_loss: 0.5162\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.5158\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5161\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.5155\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5156\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.5157\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.5155\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.5152\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.5150\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.5156\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5225 - val_loss: 0.5153\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.5168\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.5151\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5223 - val_loss: 0.5146\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.5147\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.5154\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.5150\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.5149\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5223 - val_loss: 0.5146\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.5155\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5148\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5221 - val_loss: 0.5144\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.5155\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5220 - val_loss: 0.5153\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5222 - val_loss: 0.5156\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5150\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5221 - val_loss: 0.5154\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5154\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5219 - val_loss: 0.5147\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5221 - val_loss: 0.5151\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5220 - val_loss: 0.5153\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5219 - val_loss: 0.5146\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5226\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 3.8244 - val_loss: 2.2202\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.6237 - val_loss: 1.2751\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.0540 - val_loss: 1.0443\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8780 - val_loss: 0.9905\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8047 - val_loss: 0.9881\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7610 - val_loss: 1.0030\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7278 - val_loss: 1.0286\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6999 - val_loss: 1.0621\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6758 - val_loss: 1.1022\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6550 - val_loss: 1.1482\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6367 - val_loss: 1.1983\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6207 - val_loss: 1.2534\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6067 - val_loss: 1.3125\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5944 - val_loss: 1.3741\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5835 - val_loss: 1.4394\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.9384\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 3.9477 - val_loss: 2.1465\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3486 - val_loss: 0.9469\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7864 - val_loss: 0.6765\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6375 - val_loss: 0.6008\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5912 - val_loss: 0.5754\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5745 - val_loss: 0.5647\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5660 - val_loss: 0.5591\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5607 - val_loss: 0.5551\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5566 - val_loss: 0.5517\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5531 - val_loss: 0.5481\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5502 - val_loss: 0.5470\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5476 - val_loss: 0.5451\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5452 - val_loss: 0.5418\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5431 - val_loss: 0.5396\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5411 - val_loss: 0.5381\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5392 - val_loss: 0.5368\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5377 - val_loss: 0.5344\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5361 - val_loss: 0.5334\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5348 - val_loss: 0.5320\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5334 - val_loss: 0.5305\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5325 - val_loss: 0.5296\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5314 - val_loss: 0.5290\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5303 - val_loss: 0.5290\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5293 - val_loss: 0.5274\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5285 - val_loss: 0.5262\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5278 - val_loss: 0.5255\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5272 - val_loss: 0.5253\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.5245\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5258 - val_loss: 0.5248\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5251 - val_loss: 0.5230\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5248 - val_loss: 0.5225\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5243 - val_loss: 0.5220\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5239 - val_loss: 0.5216\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5237 - val_loss: 0.5215\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5230 - val_loss: 0.5208\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5207\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.5219\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5220 - val_loss: 0.5206\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5218 - val_loss: 0.5197\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5215 - val_loss: 0.5194\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5213 - val_loss: 0.5191\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5212 - val_loss: 0.5189\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5211 - val_loss: 0.5191\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5206 - val_loss: 0.5186\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5206 - val_loss: 0.5189\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5203 - val_loss: 0.5184\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5202 - val_loss: 0.5187\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5200 - val_loss: 0.5188\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5198 - val_loss: 0.5179\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5197 - val_loss: 0.5183\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5198 - val_loss: 0.5178\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5196 - val_loss: 0.5181\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5193 - val_loss: 0.5171\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5193 - val_loss: 0.5170\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5194 - val_loss: 0.5172\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5191 - val_loss: 0.5187\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5191 - val_loss: 0.5188\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5188 - val_loss: 0.5166\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5190 - val_loss: 0.5169\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5190 - val_loss: 0.5172\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5189 - val_loss: 0.5180\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5189 - val_loss: 0.5176\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5188 - val_loss: 0.5178\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5188 - val_loss: 0.5177\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5185 - val_loss: 0.5163\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5187 - val_loss: 0.5167\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5185 - val_loss: 0.5161\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5185 - val_loss: 0.5177\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5186 - val_loss: 0.5170\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5185 - val_loss: 0.5170\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5184 - val_loss: 0.5166\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5184 - val_loss: 0.5167\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5184 - val_loss: 0.5163\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5184 - val_loss: 0.5173\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5183 - val_loss: 0.5174\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5183 - val_loss: 0.5165\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5183 - val_loss: 0.5162\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5311\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.0487 - val_loss: 3.8624\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8714 - val_loss: 0.5010\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4899 - val_loss: 0.4536\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4557 - val_loss: 0.4398\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4384 - val_loss: 0.4439\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4272 - val_loss: 0.4107\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4165 - val_loss: 0.4059\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.3993\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4026 - val_loss: 0.3934\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3984 - val_loss: 0.3894\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3938 - val_loss: 0.3857\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3906 - val_loss: 0.3827\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3855 - val_loss: 0.3815\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3827 - val_loss: 0.3954\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3814 - val_loss: 0.3769\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3788 - val_loss: 0.3742\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3765 - val_loss: 0.3781\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3739 - val_loss: 0.3717\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3712 - val_loss: 0.3703\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3729 - val_loss: 0.3689\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3685 - val_loss: 0.3700\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3665 - val_loss: 0.3660\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3648 - val_loss: 0.3634\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3631 - val_loss: 0.3632\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3619 - val_loss: 0.3628\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3593 - val_loss: 0.3609\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3582 - val_loss: 0.3599\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3577 - val_loss: 0.3572\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3556 - val_loss: 0.3615\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3546 - val_loss: 0.3579\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3529 - val_loss: 0.3604\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3531 - val_loss: 0.3533\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3532 - val_loss: 0.3545\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3492 - val_loss: 0.3523\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3487 - val_loss: 0.3534\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3476 - val_loss: 0.3518\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3495 - val_loss: 0.3527\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3463 - val_loss: 0.3485\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3452 - val_loss: 0.3465\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3468 - val_loss: 0.3477\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3445 - val_loss: 0.3497\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3423 - val_loss: 0.3476\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3418 - val_loss: 0.3459\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3403 - val_loss: 0.3481\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3386 - val_loss: 0.3478\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3389 - val_loss: 0.3440\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3378 - val_loss: 0.3476\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3382 - val_loss: 0.3454\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3366 - val_loss: 0.3442\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3364 - val_loss: 0.3416\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3336 - val_loss: 0.3403\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3336 - val_loss: 0.3409\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3371 - val_loss: 0.3393\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3340 - val_loss: 0.3459\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3337 - val_loss: 0.3431\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3349 - val_loss: 0.3408\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3322 - val_loss: 0.3411\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3297 - val_loss: 0.3368\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3306 - val_loss: 0.3353\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3282 - val_loss: 0.3415\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3270 - val_loss: 0.3403\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3282 - val_loss: 0.3354\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3264 - val_loss: 0.3391\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3337 - val_loss: 0.3343\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3273 - val_loss: 0.3335\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3251 - val_loss: 0.3327\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3257 - val_loss: 0.3341\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3278 - val_loss: 0.3327\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3228 - val_loss: 0.3413\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3236 - val_loss: 0.3348\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3243 - val_loss: 0.3307\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3228 - val_loss: 0.3380\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3228 - val_loss: 0.3319\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3231 - val_loss: 0.3272\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.3304\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3240 - val_loss: 0.3298\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3204 - val_loss: 0.3322\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3252 - val_loss: 0.3248\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3189 - val_loss: 0.3331\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3177 - val_loss: 0.3310\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3195 - val_loss: 0.3322\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3190 - val_loss: 0.3282\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3215 - val_loss: 0.3256\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3179 - val_loss: 0.3275\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3150 - val_loss: 0.3244\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3166 - val_loss: 0.3613\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3166 - val_loss: 0.3258\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3150 - val_loss: 0.3265\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3155 - val_loss: 0.3247\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3150 - val_loss: 0.3200\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3135 - val_loss: 0.3235\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3131 - val_loss: 0.3263\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3158 - val_loss: 0.3287\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3146 - val_loss: 0.3367\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3127 - val_loss: 0.3561\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3137 - val_loss: 0.3246\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3129 - val_loss: 0.3205\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3119 - val_loss: 0.3405\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3120 - val_loss: 0.3244\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3100 - val_loss: 0.3242\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3361\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.9229 - val_loss: 0.9225\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5593 - val_loss: 0.5964\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.4846\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4647 - val_loss: 0.4451\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4439 - val_loss: 0.4466\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4274 - val_loss: 0.4709\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4167 - val_loss: 0.4837\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.5342\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4019 - val_loss: 0.5123\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3966 - val_loss: 0.5290\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3928 - val_loss: 0.5162\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3891 - val_loss: 0.5045\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3855 - val_loss: 0.4943\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3832 - val_loss: 0.4859\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4334\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 2.4962 - val_loss: 2.6571\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8515 - val_loss: 0.5195\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4905 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4499 - val_loss: 0.4405\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4283 - val_loss: 0.4235\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4144 - val_loss: 0.4108\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4029 - val_loss: 0.4003\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3935 - val_loss: 0.3939\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3878 - val_loss: 0.3919\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3792 - val_loss: 0.3838\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3729 - val_loss: 0.3760\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3696 - val_loss: 0.3720\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3680 - val_loss: 0.3684\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3620 - val_loss: 0.3628\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3627 - val_loss: 0.3598\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3566 - val_loss: 0.3596\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3535 - val_loss: 0.3555\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3516 - val_loss: 0.3549\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3509 - val_loss: 0.3550\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3538 - val_loss: 0.3509\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3524 - val_loss: 0.3503\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3471 - val_loss: 0.3470\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3432 - val_loss: 0.3453\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3439 - val_loss: 0.3460\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3410 - val_loss: 0.3413\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3394 - val_loss: 0.3428\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3386 - val_loss: 0.3393\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3371 - val_loss: 0.3388\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3371 - val_loss: 0.3409\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3544 - val_loss: 0.3466\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3400 - val_loss: 0.3378\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3330 - val_loss: 0.3364\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3386 - val_loss: 0.3378\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3308 - val_loss: 0.3346\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3310 - val_loss: 0.3365\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3280 - val_loss: 0.3331\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3287 - val_loss: 0.3290\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3263 - val_loss: 0.3281\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3292 - val_loss: 0.3286\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3272 - val_loss: 0.3309\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3237 - val_loss: 0.3259\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3265 - val_loss: 0.3284\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3214 - val_loss: 0.3322\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3217 - val_loss: 0.3252\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3188 - val_loss: 0.3261\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3191 - val_loss: 0.3277\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3252 - val_loss: 0.3255\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3215 - val_loss: 0.3364\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3182 - val_loss: 0.3330\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3558 - val_loss: 0.3277\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3203 - val_loss: 0.3229\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3173 - val_loss: 0.3338\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3159 - val_loss: 0.3257\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3162 - val_loss: 0.3221\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3134 - val_loss: 0.3244\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3129 - val_loss: 0.3182\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3107 - val_loss: 0.3167\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3111 - val_loss: 0.3158\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3150 - val_loss: 0.3185\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3114 - val_loss: 0.3174\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3091 - val_loss: 0.3172\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3119 - val_loss: 0.3135\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3096 - val_loss: 0.3150\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3096 - val_loss: 0.3146\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3085 - val_loss: 0.3308\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3076 - val_loss: 0.3129\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3089 - val_loss: 0.3189\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3070 - val_loss: 0.3128\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3097 - val_loss: 0.3150\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3065 - val_loss: 0.3140\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3089 - val_loss: 0.3168\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3062 - val_loss: 0.3118\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3049 - val_loss: 0.3182\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3046 - val_loss: 0.3118\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3046 - val_loss: 0.3130\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3049 - val_loss: 0.3102\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3032 - val_loss: 0.3127\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3059 - val_loss: 0.3078\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3031 - val_loss: 0.3096\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3017 - val_loss: 0.3133\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3007 - val_loss: 0.3079\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3037 - val_loss: 0.3093\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.3086\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3002 - val_loss: 0.3070\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3001 - val_loss: 0.3085\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3265 - val_loss: 0.3192\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3032 - val_loss: 0.3066\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2994 - val_loss: 0.3074\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3002 - val_loss: 0.3073\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2994 - val_loss: 0.3082\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3010 - val_loss: 0.3047\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3007 - val_loss: 0.3059\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2979 - val_loss: 0.3043\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2990 - val_loss: 0.3877\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3085 - val_loss: 0.3074\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2984 - val_loss: 0.3254\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3013 - val_loss: 0.3066\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2983 - val_loss: 0.3053\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2971 - val_loss: 0.3034\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2967 - val_loss: 0.3052\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3158\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 4.6203 - val_loss: 4.2770\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2.7842 - val_loss: 2.4770\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.8317 - val_loss: 1.6126\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3262 - val_loss: 1.1798\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0528 - val_loss: 0.9602\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9012 - val_loss: 0.8396\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8147 - val_loss: 0.7711\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7634 - val_loss: 0.7313\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7315 - val_loss: 0.7064\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7102 - val_loss: 0.6895\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6950 - val_loss: 0.6768\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6833 - val_loss: 0.6666\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6737 - val_loss: 0.6582\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6654 - val_loss: 0.6504\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6580 - val_loss: 0.6436\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6513 - val_loss: 0.6373\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6451 - val_loss: 0.6313\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6392 - val_loss: 0.6257\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6338 - val_loss: 0.6204\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6285 - val_loss: 0.6155\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6237 - val_loss: 0.6107\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6191 - val_loss: 0.6063\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6147 - val_loss: 0.6020\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6106 - val_loss: 0.5980\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6066 - val_loss: 0.5942\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6029 - val_loss: 0.5906\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5994 - val_loss: 0.5872\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5960 - val_loss: 0.5839\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5928 - val_loss: 0.5808\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5897 - val_loss: 0.5780\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5869 - val_loss: 0.5751\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5841 - val_loss: 0.5726\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5815 - val_loss: 0.5701\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5791 - val_loss: 0.5678\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5767 - val_loss: 0.5654\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5744 - val_loss: 0.5632\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5723 - val_loss: 0.5611\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5703 - val_loss: 0.5591\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5683 - val_loss: 0.5573\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5664 - val_loss: 0.5555\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5647 - val_loss: 0.5538\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5630 - val_loss: 0.5523\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5613 - val_loss: 0.5506\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5598 - val_loss: 0.5492\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5583 - val_loss: 0.5478\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5570 - val_loss: 0.5465\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5556 - val_loss: 0.5452\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5543 - val_loss: 0.5441\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5531 - val_loss: 0.5429\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5519 - val_loss: 0.5418\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5507 - val_loss: 0.5406\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5497 - val_loss: 0.5397\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5487 - val_loss: 0.5387\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5476 - val_loss: 0.5377\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5467 - val_loss: 0.5370\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5458 - val_loss: 0.5360\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5449 - val_loss: 0.5353\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5441 - val_loss: 0.5343\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5433 - val_loss: 0.5336\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5425 - val_loss: 0.5330\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5418 - val_loss: 0.5322\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5411 - val_loss: 0.5317\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5404 - val_loss: 0.5312\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5397 - val_loss: 0.5305\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5391 - val_loss: 0.5299\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5385 - val_loss: 0.5294\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5379 - val_loss: 0.5288\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5374 - val_loss: 0.5283\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5368 - val_loss: 0.5278\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5363 - val_loss: 0.5273\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5358 - val_loss: 0.5268\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5353 - val_loss: 0.5265\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5349 - val_loss: 0.5260\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5344 - val_loss: 0.5255\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5340 - val_loss: 0.5250\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5336 - val_loss: 0.5248\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5332 - val_loss: 0.5244\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5328 - val_loss: 0.5242\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5324 - val_loss: 0.5237\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5321 - val_loss: 0.5234\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5317 - val_loss: 0.5231\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5314 - val_loss: 0.5228\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5311 - val_loss: 0.5226\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5308 - val_loss: 0.5223\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5304 - val_loss: 0.5221\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5301 - val_loss: 0.5220\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5299 - val_loss: 0.5216\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5296 - val_loss: 0.5212\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5294 - val_loss: 0.5210\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5291 - val_loss: 0.5209\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5289 - val_loss: 0.5207\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5286 - val_loss: 0.5206\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5284 - val_loss: 0.5203\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5282 - val_loss: 0.5202\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5280 - val_loss: 0.5200\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5277 - val_loss: 0.5197\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5276 - val_loss: 0.5195\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5274 - val_loss: 0.5193\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5272 - val_loss: 0.5191\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5270 - val_loss: 0.5190\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5246\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 4.6267 - val_loss: 4.6766\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.7930 - val_loss: 3.2102\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.8386 - val_loss: 2.4246\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3331 - val_loss: 1.9839\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0604 - val_loss: 1.7212\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9102 - val_loss: 1.5536\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.8250 - val_loss: 1.4378\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7749 - val_loss: 1.3508\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7437 - val_loss: 1.2804\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7229 - val_loss: 1.2201\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7078 - val_loss: 1.1661\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6961 - val_loss: 1.1167\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6864 - val_loss: 1.0709\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6779 - val_loss: 1.0281\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6702 - val_loss: 0.9877\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6631 - val_loss: 0.9499\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6565 - val_loss: 0.9143\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6502 - val_loss: 0.8808\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6443 - val_loss: 0.8493\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6387 - val_loss: 0.8197\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6333 - val_loss: 0.7919\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6283 - val_loss: 0.7660\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6234 - val_loss: 0.7418\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6188 - val_loss: 0.7194\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6144 - val_loss: 0.6985\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6102 - val_loss: 0.6793\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6062 - val_loss: 0.6617\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6024 - val_loss: 0.6455\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5987 - val_loss: 0.6309\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5952 - val_loss: 0.6176\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5918 - val_loss: 0.6058\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5886 - val_loss: 0.5953\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5855 - val_loss: 0.5861\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5826 - val_loss: 0.5782\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5798 - val_loss: 0.5716\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5772 - val_loss: 0.5661\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5746 - val_loss: 0.5619\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5721 - val_loss: 0.5588\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5698 - val_loss: 0.5568\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5675 - val_loss: 0.5558\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5653 - val_loss: 0.5560\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5632 - val_loss: 0.5572\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5612 - val_loss: 0.5594\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5593 - val_loss: 0.5626\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5574 - val_loss: 0.5667\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5557 - val_loss: 0.5717\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5539 - val_loss: 0.5777\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5523 - val_loss: 0.5845\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5507 - val_loss: 0.5922\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5492 - val_loss: 0.6007\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.5751\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 5.6945 - val_loss: 4.4987\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.4847 - val_loss: 2.8489\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.2967 - val_loss: 1.9516\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6464 - val_loss: 1.4529\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2835 - val_loss: 1.1713\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0759 - val_loss: 1.0063\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9532 - val_loss: 0.9064\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8779 - val_loss: 0.8440\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8289 - val_loss: 0.8023\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7949 - val_loss: 0.7719\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7696 - val_loss: 0.7489\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7496 - val_loss: 0.7303\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.7330 - val_loss: 0.7145\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7185 - val_loss: 0.7008\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7056 - val_loss: 0.6883\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6939 - val_loss: 0.6770\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6831 - val_loss: 0.6665\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6732 - val_loss: 0.6568\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6639 - val_loss: 0.6479\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.6552 - val_loss: 0.6396\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6470 - val_loss: 0.6319\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6394 - val_loss: 0.6247\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6323 - val_loss: 0.6176\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6256 - val_loss: 0.6113\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6193 - val_loss: 0.6053\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6133 - val_loss: 0.5998\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6078 - val_loss: 0.5945\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6026 - val_loss: 0.5896\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5977 - val_loss: 0.5850\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5930 - val_loss: 0.5806\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5887 - val_loss: 0.5766\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5846 - val_loss: 0.5728\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5808 - val_loss: 0.5693\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5772 - val_loss: 0.5659\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5737 - val_loss: 0.5628\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5705 - val_loss: 0.5598\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5676 - val_loss: 0.5571\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5647 - val_loss: 0.5545\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5620 - val_loss: 0.5521\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5595 - val_loss: 0.5498\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5572 - val_loss: 0.5478\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5550 - val_loss: 0.5458\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5529 - val_loss: 0.5439\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5509 - val_loss: 0.5421\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5490 - val_loss: 0.5405\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5473 - val_loss: 0.5390\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5456 - val_loss: 0.5375\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5441 - val_loss: 0.5361\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5427 - val_loss: 0.5349\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5413 - val_loss: 0.5338\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5326\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5387 - val_loss: 0.5315\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5376 - val_loss: 0.5305\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5365 - val_loss: 0.5296\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5355 - val_loss: 0.5288\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5345 - val_loss: 0.5280\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5336 - val_loss: 0.5273\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5328 - val_loss: 0.5266\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5320 - val_loss: 0.5260\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5312 - val_loss: 0.5253\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5305 - val_loss: 0.5248\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5298 - val_loss: 0.5242\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5292 - val_loss: 0.5237\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5286 - val_loss: 0.5232\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5280 - val_loss: 0.5229\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5274 - val_loss: 0.5226\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5270 - val_loss: 0.5221\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.5216\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5260 - val_loss: 0.5212\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5256 - val_loss: 0.5210\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5252 - val_loss: 0.5206\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5248 - val_loss: 0.5204\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5244 - val_loss: 0.5201\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.5199\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5238 - val_loss: 0.5196\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.5193\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5232 - val_loss: 0.5192\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5190\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.5188\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.5186\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5222 - val_loss: 0.5185\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5220 - val_loss: 0.5185\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5217 - val_loss: 0.5182\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5215 - val_loss: 0.5179\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5214 - val_loss: 0.5182\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5212 - val_loss: 0.5180\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5210 - val_loss: 0.5179\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5209 - val_loss: 0.5178\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5207 - val_loss: 0.5175\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5206 - val_loss: 0.5173\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5204 - val_loss: 0.5172\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5203 - val_loss: 0.5172\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5202 - val_loss: 0.5171\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5201 - val_loss: 0.5170\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5200 - val_loss: 0.5171\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5199 - val_loss: 0.5171\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5198 - val_loss: 0.5170\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5197 - val_loss: 0.5170\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5196 - val_loss: 0.5169\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5195 - val_loss: 0.5170\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 0.5328\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.7423 - val_loss: 0.8020\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6970 - val_loss: 0.6259\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6228 - val_loss: 0.5780\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5837 - val_loss: 0.5455\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5523 - val_loss: 0.5192\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5256 - val_loss: 0.4982\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5030 - val_loss: 0.4800\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4836 - val_loss: 0.4632\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4675 - val_loss: 0.4500\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.4541 - val_loss: 0.4420\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4435 - val_loss: 0.4318\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4341 - val_loss: 0.4236\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4265 - val_loss: 0.4176\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4195 - val_loss: 0.4137\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4134 - val_loss: 0.4099\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4082 - val_loss: 0.4058\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4039 - val_loss: 0.4005\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3998 - val_loss: 0.3973\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3959 - val_loss: 0.3941\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3925 - val_loss: 0.3913\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3892 - val_loss: 0.3907\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3865 - val_loss: 0.3863\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3835 - val_loss: 0.3838\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3816 - val_loss: 0.3798\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3789 - val_loss: 0.3771\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3763 - val_loss: 0.3753\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3746 - val_loss: 0.3743\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3726 - val_loss: 0.3713\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3705 - val_loss: 0.3698\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3686 - val_loss: 0.3681\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3673 - val_loss: 0.3664\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3652 - val_loss: 0.3665\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3633 - val_loss: 0.3691\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3623 - val_loss: 0.3631\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3604 - val_loss: 0.3629\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3589 - val_loss: 0.3625\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3578 - val_loss: 0.3633\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3564 - val_loss: 0.3576\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3552 - val_loss: 0.3582\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3538 - val_loss: 0.3572\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3524 - val_loss: 0.3575\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3517 - val_loss: 0.3551\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3491 - val_loss: 0.3563\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3485 - val_loss: 0.3538\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3471 - val_loss: 0.3532\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3462 - val_loss: 0.3524\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3458 - val_loss: 0.3520\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3443 - val_loss: 0.3456\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3432 - val_loss: 0.3480\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3421 - val_loss: 0.3441\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3410 - val_loss: 0.3449\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3406 - val_loss: 0.3424\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3393 - val_loss: 0.3444\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3383 - val_loss: 0.3472\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3375 - val_loss: 0.3442\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3371 - val_loss: 0.3426\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3359 - val_loss: 0.3427\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3354 - val_loss: 0.3409\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3339 - val_loss: 0.3428\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3328 - val_loss: 0.3416\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3324 - val_loss: 0.3413\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3313 - val_loss: 0.3377\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3303 - val_loss: 0.3366\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3302 - val_loss: 0.3337\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3282 - val_loss: 0.3382\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3284 - val_loss: 0.3329\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3274 - val_loss: 0.3320\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3269 - val_loss: 0.3307\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3257 - val_loss: 0.3341\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3249 - val_loss: 0.3348\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3244 - val_loss: 0.3287\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3235 - val_loss: 0.3304\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3229 - val_loss: 0.3361\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3228 - val_loss: 0.3302\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3218 - val_loss: 0.3323\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3213 - val_loss: 0.3303\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3202 - val_loss: 0.3295\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3191 - val_loss: 0.3251\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3186 - val_loss: 0.3259\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3180 - val_loss: 0.3240\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3177 - val_loss: 0.3229\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3169 - val_loss: 0.3275\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3168 - val_loss: 0.3231\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3163 - val_loss: 0.3221\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3138 - val_loss: 0.3274\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3141 - val_loss: 0.3213\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3142 - val_loss: 0.3204\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3130 - val_loss: 0.3279\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3129 - val_loss: 0.3254\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3116 - val_loss: 0.3247\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3112 - val_loss: 0.3253\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3113 - val_loss: 0.3208\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3099 - val_loss: 0.3186\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3096 - val_loss: 0.3189\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3093 - val_loss: 0.3170\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3078 - val_loss: 0.3229\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3074 - val_loss: 0.3197\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3073 - val_loss: 0.3156\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3074 - val_loss: 0.3156\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3065 - val_loss: 0.3146\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3240\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.8469 - val_loss: 1.0650\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.7138 - val_loss: 0.7414\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6267 - val_loss: 0.6261\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5793 - val_loss: 0.5625\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5433 - val_loss: 0.5182\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5141 - val_loss: 0.4951\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4913 - val_loss: 0.4835\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4736 - val_loss: 0.4832\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4602 - val_loss: 0.4834\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4492 - val_loss: 0.4866\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4402 - val_loss: 0.5036\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4328 - val_loss: 0.5049\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4263 - val_loss: 0.5055\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4206 - val_loss: 0.5177\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4155 - val_loss: 0.5220\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4101 - val_loss: 0.5236\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4062 - val_loss: 0.5306\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4021 - val_loss: 0.5416\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4691\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 2.0922 - val_loss: 1.5287\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.8249 - val_loss: 0.7023\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.6601 - val_loss: 0.6271\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.6172 - val_loss: 0.5913\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5851 - val_loss: 0.5628\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5572 - val_loss: 0.5386\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5333 - val_loss: 0.5165\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5123 - val_loss: 0.4982\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4942 - val_loss: 0.4817\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4783 - val_loss: 0.4677\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4644 - val_loss: 0.4564\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4522 - val_loss: 0.4460\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4422 - val_loss: 0.4368\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4332 - val_loss: 0.4296\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4256 - val_loss: 0.4237\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4188 - val_loss: 0.4177\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4131 - val_loss: 0.4122\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4079 - val_loss: 0.4081\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4033 - val_loss: 0.4054\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3990 - val_loss: 0.4005\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3952 - val_loss: 0.3985\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3919 - val_loss: 0.3944\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3889 - val_loss: 0.3932\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3857 - val_loss: 0.3904\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3832 - val_loss: 0.3865\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3807 - val_loss: 0.3876\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3786 - val_loss: 0.3834\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3761 - val_loss: 0.3822\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3744 - val_loss: 0.3819\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3723 - val_loss: 0.3793\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3704 - val_loss: 0.3776\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3690 - val_loss: 0.3755\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3672 - val_loss: 0.3751\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3655 - val_loss: 0.3702\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3637 - val_loss: 0.3684\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3621 - val_loss: 0.3695\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3610 - val_loss: 0.3650\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3594 - val_loss: 0.3644\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3576 - val_loss: 0.3626\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3569 - val_loss: 0.3618\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3551 - val_loss: 0.3622\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3537 - val_loss: 0.3637\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3525 - val_loss: 0.3607\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3515 - val_loss: 0.3600\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3499 - val_loss: 0.3561\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3492 - val_loss: 0.3568\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3482 - val_loss: 0.3562\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3468 - val_loss: 0.3584\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3457 - val_loss: 0.3519\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3446 - val_loss: 0.3533\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3435 - val_loss: 0.3494\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3426 - val_loss: 0.3525\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3420 - val_loss: 0.3502\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3404 - val_loss: 0.3468\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3393 - val_loss: 0.3503\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3385 - val_loss: 0.3467\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3377 - val_loss: 0.3471\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3371 - val_loss: 0.3476\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3361 - val_loss: 0.3423\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3352 - val_loss: 0.3458\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3342 - val_loss: 0.3496\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3337 - val_loss: 0.3408\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3331 - val_loss: 0.3440\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3320 - val_loss: 0.3390\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3318 - val_loss: 0.3410\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3305 - val_loss: 0.3411\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3301 - val_loss: 0.3373\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3295 - val_loss: 0.3365\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3290 - val_loss: 0.3349\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3276 - val_loss: 0.3389\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3271 - val_loss: 0.3355\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3267 - val_loss: 0.3342\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3256 - val_loss: 0.3364\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3251 - val_loss: 0.3324\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3237 - val_loss: 0.3356\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3237 - val_loss: 0.3360\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3226 - val_loss: 0.3314\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3219 - val_loss: 0.3298\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3215 - val_loss: 0.3350\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3208 - val_loss: 0.3326\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3206 - val_loss: 0.3351\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3199 - val_loss: 0.3307\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3186 - val_loss: 0.3298\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3187 - val_loss: 0.3266\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3183 - val_loss: 0.3261\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3177 - val_loss: 0.3251\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3162 - val_loss: 0.3255\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3156 - val_loss: 0.3278\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3153 - val_loss: 0.3278\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3147 - val_loss: 0.3261\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3144 - val_loss: 0.3263\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3142 - val_loss: 0.3248\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3130 - val_loss: 0.3216\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3120 - val_loss: 0.3249\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3119 - val_loss: 0.3204\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3114 - val_loss: 0.3210\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3110 - val_loss: 0.3203\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3102 - val_loss: 0.3223\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3103 - val_loss: 0.3222\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.3254\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3346\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.6748 - val_loss: 2.5674\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9281 - val_loss: 1.6216\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.2777 - val_loss: 1.1982\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0142 - val_loss: 0.9850\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8959 - val_loss: 0.8759\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8369 - val_loss: 0.8172\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8031 - val_loss: 0.7814\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7806 - val_loss: 0.7569\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7636 - val_loss: 0.7392\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7498 - val_loss: 0.7246\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7378 - val_loss: 0.7130\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7267 - val_loss: 0.7022\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7165 - val_loss: 0.6925\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7069 - val_loss: 0.6833\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6977 - val_loss: 0.6747\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6890 - val_loss: 0.6665\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6806 - val_loss: 0.6587\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6725 - val_loss: 0.6512\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6647 - val_loss: 0.6439\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6572 - val_loss: 0.6367\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6499 - val_loss: 0.6298\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6429 - val_loss: 0.6233\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6361 - val_loss: 0.6170\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6295 - val_loss: 0.6108\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6232 - val_loss: 0.6051\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6172 - val_loss: 0.5994\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6114 - val_loss: 0.5938\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6057 - val_loss: 0.5882\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6002 - val_loss: 0.5830\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5950 - val_loss: 0.5781\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5898 - val_loss: 0.5730\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5848 - val_loss: 0.5680\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5801 - val_loss: 0.5636\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5754 - val_loss: 0.5594\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5709 - val_loss: 0.5549\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5666 - val_loss: 0.5509\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5624 - val_loss: 0.5471\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5583 - val_loss: 0.5431\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5545 - val_loss: 0.5397\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5507 - val_loss: 0.5363\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5471 - val_loss: 0.5329\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5436 - val_loss: 0.5295\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5402 - val_loss: 0.5262\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5369 - val_loss: 0.5232\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5337 - val_loss: 0.5201\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5307 - val_loss: 0.5177\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5277 - val_loss: 0.5146\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5249 - val_loss: 0.5119\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5222 - val_loss: 0.5096\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5196 - val_loss: 0.5073\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5171 - val_loss: 0.5047\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5148 - val_loss: 0.5024\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5125 - val_loss: 0.5004\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5102 - val_loss: 0.4982\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5081 - val_loss: 0.4961\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5060 - val_loss: 0.4943\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5040 - val_loss: 0.4923\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5020 - val_loss: 0.4905\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5001 - val_loss: 0.4889\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4983 - val_loss: 0.4871\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4965 - val_loss: 0.4858\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4948 - val_loss: 0.4841\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4931 - val_loss: 0.4824\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4915 - val_loss: 0.4809\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4898 - val_loss: 0.4792\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4883 - val_loss: 0.4776\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4868 - val_loss: 0.4762\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4854 - val_loss: 0.4750\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4839 - val_loss: 0.4740\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4825 - val_loss: 0.4728\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4812 - val_loss: 0.4713\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4798 - val_loss: 0.4706\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4786 - val_loss: 0.4694\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4773 - val_loss: 0.4684\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4761 - val_loss: 0.4669\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4749 - val_loss: 0.4658\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4737 - val_loss: 0.4647\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4726 - val_loss: 0.4638\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4715 - val_loss: 0.4628\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4704 - val_loss: 0.4617\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4693 - val_loss: 0.4609\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4683 - val_loss: 0.4600\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4672 - val_loss: 0.4588\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4663 - val_loss: 0.4580\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4653 - val_loss: 0.4572\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4644 - val_loss: 0.4564\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4634 - val_loss: 0.4557\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4625 - val_loss: 0.4550\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4616 - val_loss: 0.4542\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4607 - val_loss: 0.4536\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4598 - val_loss: 0.4526\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4589 - val_loss: 0.4518\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4581 - val_loss: 0.4511\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4572 - val_loss: 0.4503\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4564 - val_loss: 0.4495\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4556 - val_loss: 0.4489\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4548 - val_loss: 0.4481\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4540 - val_loss: 0.4474\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4533 - val_loss: 0.4469\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4526 - val_loss: 0.4463\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4516\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.7124 - val_loss: 2.8959\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.9720 - val_loss: 2.3658\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3647 - val_loss: 2.1911\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1043 - val_loss: 2.0563\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9689 - val_loss: 1.9360\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8898 - val_loss: 1.8249\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8409 - val_loss: 1.7274\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8089 - val_loss: 1.6442\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7866 - val_loss: 1.5672\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7702 - val_loss: 1.5036\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7571 - val_loss: 1.4479\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7463 - val_loss: 1.3918\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7368 - val_loss: 1.3434\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7280 - val_loss: 1.2991\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7201 - val_loss: 1.2547\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7125 - val_loss: 1.2130\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7053 - val_loss: 1.1736\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6984 - val_loss: 1.1363\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6918 - val_loss: 1.1018\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6854 - val_loss: 1.0697\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6792 - val_loss: 1.0376\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6731 - val_loss: 1.0095\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6673 - val_loss: 0.9822\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6616 - val_loss: 0.9549\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6561 - val_loss: 0.9294\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6506 - val_loss: 0.9021\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6455 - val_loss: 0.8805\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6404 - val_loss: 0.8582\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6355 - val_loss: 0.8375\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6307 - val_loss: 0.8170\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6260 - val_loss: 0.7973\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6214 - val_loss: 0.7778\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6169 - val_loss: 0.7598\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6125 - val_loss: 0.7428\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6083 - val_loss: 0.7274\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6042 - val_loss: 0.7127\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6002 - val_loss: 0.6975\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5963 - val_loss: 0.6838\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5925 - val_loss: 0.6715\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5888 - val_loss: 0.6597\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5852 - val_loss: 0.6477\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5817 - val_loss: 0.6367\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5783 - val_loss: 0.6262\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5750 - val_loss: 0.6161\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5717 - val_loss: 0.6066\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5685 - val_loss: 0.5975\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5654 - val_loss: 0.5891\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5624 - val_loss: 0.5816\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5594 - val_loss: 0.5744\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5565 - val_loss: 0.5678\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5537 - val_loss: 0.5614\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5509 - val_loss: 0.5555\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5482 - val_loss: 0.5499\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5456 - val_loss: 0.5445\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5430 - val_loss: 0.5399\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5405 - val_loss: 0.5357\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5381 - val_loss: 0.5316\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5358 - val_loss: 0.5277\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5335 - val_loss: 0.5245\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5312 - val_loss: 0.5211\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5290 - val_loss: 0.5185\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5269 - val_loss: 0.5162\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5248 - val_loss: 0.5140\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5228 - val_loss: 0.5121\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5208 - val_loss: 0.5105\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5188 - val_loss: 0.5092\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5170 - val_loss: 0.5082\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5152 - val_loss: 0.5073\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5134 - val_loss: 0.5068\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5116 - val_loss: 0.5065\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5100 - val_loss: 0.5063\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5083 - val_loss: 0.5064\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5067 - val_loss: 0.5067\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5052 - val_loss: 0.5072\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5036 - val_loss: 0.5078\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5022 - val_loss: 0.5087\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5007 - val_loss: 0.5094\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.5107\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.5118\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4966 - val_loss: 0.5130\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4952 - val_loss: 0.5144\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5133\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 4.3074 - val_loss: 2.8056\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 2.1130 - val_loss: 1.6001\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3472 - val_loss: 1.1572\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0337 - val_loss: 0.9606\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.8887 - val_loss: 0.8622\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8157 - val_loss: 0.8082\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7754 - val_loss: 0.7757\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7507 - val_loss: 0.7541\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7335 - val_loss: 0.7383\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7204 - val_loss: 0.7256\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7094 - val_loss: 0.7145\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6996 - val_loss: 0.7045\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6905 - val_loss: 0.6953\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6820 - val_loss: 0.6866\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6739 - val_loss: 0.6784\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6662 - val_loss: 0.6706\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6587 - val_loss: 0.6631\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6515 - val_loss: 0.6558\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6446 - val_loss: 0.6488\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6380 - val_loss: 0.6422\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6316 - val_loss: 0.6358\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6254 - val_loss: 0.6296\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6196 - val_loss: 0.6237\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6139 - val_loss: 0.6180\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6083 - val_loss: 0.6125\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6031 - val_loss: 0.6071\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5979 - val_loss: 0.6020\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5929 - val_loss: 0.5970\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5880 - val_loss: 0.5923\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5834 - val_loss: 0.5877\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5789 - val_loss: 0.5832\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5745 - val_loss: 0.5790\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5703 - val_loss: 0.5748\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5662 - val_loss: 0.5708\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5623 - val_loss: 0.5669\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5585 - val_loss: 0.5631\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5548 - val_loss: 0.5595\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5513 - val_loss: 0.5560\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5478 - val_loss: 0.5526\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5445 - val_loss: 0.5493\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5413 - val_loss: 0.5461\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.5383 - val_loss: 0.5431\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5352 - val_loss: 0.5401\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5324 - val_loss: 0.5372\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.5295 - val_loss: 0.5344\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5268 - val_loss: 0.5317\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5242 - val_loss: 0.5290\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5216 - val_loss: 0.5266\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5192 - val_loss: 0.5241\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5168 - val_loss: 0.5218\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5145 - val_loss: 0.5195\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5122 - val_loss: 0.5172\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5101 - val_loss: 0.5151\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.5080 - val_loss: 0.5130\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5059 - val_loss: 0.5110\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5039 - val_loss: 0.5091\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5020 - val_loss: 0.5072\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5002 - val_loss: 0.5054\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4984 - val_loss: 0.5037\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4967 - val_loss: 0.5020\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4950 - val_loss: 0.5003\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4934 - val_loss: 0.4987\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4919 - val_loss: 0.4971\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4904 - val_loss: 0.4956\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4889 - val_loss: 0.4942\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4875 - val_loss: 0.4928\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4861 - val_loss: 0.4914\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4848 - val_loss: 0.4901\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4834 - val_loss: 0.4888\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4822 - val_loss: 0.4875\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4809 - val_loss: 0.4863\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4797 - val_loss: 0.4850\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4786 - val_loss: 0.4840\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4774 - val_loss: 0.4828\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4763 - val_loss: 0.4817\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4752 - val_loss: 0.4806\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4742 - val_loss: 0.4796\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4731 - val_loss: 0.4786\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4721 - val_loss: 0.4776\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4711 - val_loss: 0.4766\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4701 - val_loss: 0.4757\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4692 - val_loss: 0.4747\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4683 - val_loss: 0.4738\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4673 - val_loss: 0.4729\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4665 - val_loss: 0.4721\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4656 - val_loss: 0.4712\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4648 - val_loss: 0.4703\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4639 - val_loss: 0.4695\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4631 - val_loss: 0.4686\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4623 - val_loss: 0.4678\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4615 - val_loss: 0.4671\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4608 - val_loss: 0.4662\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4600 - val_loss: 0.4654\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4593 - val_loss: 0.4647\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4586 - val_loss: 0.4641\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4579 - val_loss: 0.4634\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4572 - val_loss: 0.4628\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4565 - val_loss: 0.4621\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4559 - val_loss: 0.4615\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4552 - val_loss: 0.4609\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4787\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.4072 - val_loss: 0.6986\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6663 - val_loss: 0.6130\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5969 - val_loss: 0.5604\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5490 - val_loss: 0.5217\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5139 - val_loss: 0.4956\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4866 - val_loss: 0.4747\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4656 - val_loss: 0.4589\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4495 - val_loss: 0.4478\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4367 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4268 - val_loss: 0.4288\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4192 - val_loss: 0.4227\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4128 - val_loss: 0.4159\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4070 - val_loss: 0.4080\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4020 - val_loss: 0.4062\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3984 - val_loss: 0.4044\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3941 - val_loss: 0.3992\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3903 - val_loss: 0.3951\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3870 - val_loss: 0.3926\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3843 - val_loss: 0.3897\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3813 - val_loss: 0.3851\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3791 - val_loss: 0.3835\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3761 - val_loss: 0.3841\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3745 - val_loss: 0.3816\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3722 - val_loss: 0.3772\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3702 - val_loss: 0.3793\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3678 - val_loss: 0.3774\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3670 - val_loss: 0.3725\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3647 - val_loss: 0.3751\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3632 - val_loss: 0.3690\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3614 - val_loss: 0.3695\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3602 - val_loss: 0.3682\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3585 - val_loss: 0.3682\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3564 - val_loss: 0.3686\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3557 - val_loss: 0.3637\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3538 - val_loss: 0.3635\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3525 - val_loss: 0.3600\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3515 - val_loss: 0.3573\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3502 - val_loss: 0.3574\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3486 - val_loss: 0.3597\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3482 - val_loss: 0.3562\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3461 - val_loss: 0.3627\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3450 - val_loss: 0.3529\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3452 - val_loss: 0.3508\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3433 - val_loss: 0.3502\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3426 - val_loss: 0.3519\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3422 - val_loss: 0.3485\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3408 - val_loss: 0.3535\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3408 - val_loss: 0.3475\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3399 - val_loss: 0.3492\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3397 - val_loss: 0.3501\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3381 - val_loss: 0.3452\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3374 - val_loss: 0.3490\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3367 - val_loss: 0.3464\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3363 - val_loss: 0.3451\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3356 - val_loss: 0.3458\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3350 - val_loss: 0.3456\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3346 - val_loss: 0.3460\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3339 - val_loss: 0.3430\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3335 - val_loss: 0.3429\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3321 - val_loss: 0.3422\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3330 - val_loss: 0.3426\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3320 - val_loss: 0.3414\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3305 - val_loss: 0.3394\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3310 - val_loss: 0.3372\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3306 - val_loss: 0.3416\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3295 - val_loss: 0.3419\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3294 - val_loss: 0.3384\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3284 - val_loss: 0.3353\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3277 - val_loss: 0.3391\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3281 - val_loss: 0.3397\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3272 - val_loss: 0.3344\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3269 - val_loss: 0.3370\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3269 - val_loss: 0.3389\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3264 - val_loss: 0.3363\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3257 - val_loss: 0.3324\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3253 - val_loss: 0.3317\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3255 - val_loss: 0.3317\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3243 - val_loss: 0.3354\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3242 - val_loss: 0.3370\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3233 - val_loss: 0.3331\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3235 - val_loss: 0.3324\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3224 - val_loss: 0.3366\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3236 - val_loss: 0.3356\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3229 - val_loss: 0.3384\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3219 - val_loss: 0.3295\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3218 - val_loss: 0.3300\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3204 - val_loss: 0.3372\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3209 - val_loss: 0.3346\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3204 - val_loss: 0.3297\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3193 - val_loss: 0.3351\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3208 - val_loss: 0.3340\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3198 - val_loss: 0.3345\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3191 - val_loss: 0.3278\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3185 - val_loss: 0.3330\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3183 - val_loss: 0.3306\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3194 - val_loss: 0.3286\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3176 - val_loss: 0.3338\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3175 - val_loss: 0.3334\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3183 - val_loss: 0.3316\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3167 - val_loss: 0.3294\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3427\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.4436 - val_loss: 1.8136\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7237 - val_loss: 1.2023\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6397 - val_loss: 0.8614\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5856 - val_loss: 0.6734\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5472 - val_loss: 0.5713\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5180 - val_loss: 0.5225\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4948 - val_loss: 0.4883\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4761 - val_loss: 0.4663\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4614 - val_loss: 0.4532\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4486 - val_loss: 0.4468\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4381 - val_loss: 0.4466\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4289 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4213 - val_loss: 0.4396\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4151 - val_loss: 0.4375\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4099 - val_loss: 0.4347\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4040 - val_loss: 0.4359\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4001 - val_loss: 0.4339\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3960 - val_loss: 0.4343\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3930 - val_loss: 0.4369\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3904 - val_loss: 0.4289\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3872 - val_loss: 0.4334\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3849 - val_loss: 0.4330\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3819 - val_loss: 0.4311\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3796 - val_loss: 0.4233\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3781 - val_loss: 0.4211\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3759 - val_loss: 0.4239\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3745 - val_loss: 0.4195\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3735 - val_loss: 0.4157\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3714 - val_loss: 0.4108\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3700 - val_loss: 0.4015\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3686 - val_loss: 0.4039\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3669 - val_loss: 0.4029\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3655 - val_loss: 0.3935\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3640 - val_loss: 0.3865\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3628 - val_loss: 0.3844\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3618 - val_loss: 0.3779\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3604 - val_loss: 0.3750\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3592 - val_loss: 0.3760\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3591 - val_loss: 0.3686\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3570 - val_loss: 0.3639\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3560 - val_loss: 0.3662\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3556 - val_loss: 0.3596\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3543 - val_loss: 0.3575\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3531 - val_loss: 0.3533\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3518 - val_loss: 0.3547\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3510 - val_loss: 0.3522\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3493 - val_loss: 0.3515\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3490 - val_loss: 0.3476\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3483 - val_loss: 0.3449\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3473 - val_loss: 0.3439\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3457 - val_loss: 0.3432\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3447 - val_loss: 0.3478\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3442 - val_loss: 0.3419\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3430 - val_loss: 0.3403\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3411 - val_loss: 0.3473\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3414 - val_loss: 0.3395\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3400 - val_loss: 0.3407\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3391 - val_loss: 0.3420\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3382 - val_loss: 0.3500\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3377 - val_loss: 0.3443\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3366 - val_loss: 0.3446\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3355 - val_loss: 0.3477\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3349 - val_loss: 0.3490\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3345 - val_loss: 0.3612\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3338 - val_loss: 0.3652\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3328 - val_loss: 0.3590\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.3456\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.3672 - val_loss: 0.6829\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6174 - val_loss: 0.5819\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5496 - val_loss: 0.5234\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5080 - val_loss: 0.5025\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4823 - val_loss: 0.4848\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4649 - val_loss: 0.4617\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4494 - val_loss: 0.4475\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4434 - val_loss: 0.4378\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4357 - val_loss: 0.4334\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4308 - val_loss: 0.4264\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4256 - val_loss: 0.4222\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4193 - val_loss: 0.4270\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4169 - val_loss: 0.4256\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4111 - val_loss: 0.4090\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4074 - val_loss: 0.4193\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4075 - val_loss: 0.4252\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4016 - val_loss: 0.4046\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4013 - val_loss: 0.3974\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3981 - val_loss: 0.3976\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3935 - val_loss: 0.4019\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3919 - val_loss: 0.3909\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3922 - val_loss: 0.3949\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3917 - val_loss: 0.3913\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3858 - val_loss: 0.3918\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3841 - val_loss: 0.4015\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3846 - val_loss: 0.4030\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3827 - val_loss: 0.3983\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3780 - val_loss: 0.3808\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3769 - val_loss: 0.3848\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3746 - val_loss: 0.3775\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3764 - val_loss: 0.3813\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3723 - val_loss: 0.3792\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3710 - val_loss: 0.3742\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3674 - val_loss: 0.3763\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3686 - val_loss: 0.3877\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3659 - val_loss: 0.3672\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3667 - val_loss: 0.3736\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3637 - val_loss: 0.3746\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3644 - val_loss: 0.3826\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3612 - val_loss: 0.3761\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3601 - val_loss: 0.3709\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3598 - val_loss: 0.3655\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3582 - val_loss: 0.3700\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3589 - val_loss: 0.3770\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3561 - val_loss: 0.3628\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3574 - val_loss: 0.3650\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3579 - val_loss: 0.3684\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3576 - val_loss: 0.3673\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3578 - val_loss: 0.3662\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3522 - val_loss: 0.3629\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3511 - val_loss: 0.3585\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3491 - val_loss: 0.3615\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3488 - val_loss: 0.3554\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3476 - val_loss: 0.3619\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3496 - val_loss: 0.3671\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3468 - val_loss: 0.3527\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3481 - val_loss: 0.3648\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3448 - val_loss: 0.3591\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3445 - val_loss: 0.3543\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3431 - val_loss: 0.3559\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3452 - val_loss: 0.3653\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3448 - val_loss: 0.3682\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3421 - val_loss: 0.3493\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3411 - val_loss: 0.3560\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3408 - val_loss: 0.3478\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3396 - val_loss: 0.3536\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3414 - val_loss: 0.3621\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3385 - val_loss: 0.3444\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3401 - val_loss: 0.3485\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3413 - val_loss: 0.3564\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.3372 - val_loss: 0.3497\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3386 - val_loss: 0.3601\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3393 - val_loss: 0.3583\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3351 - val_loss: 0.3384\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3364 - val_loss: 0.3461\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3337 - val_loss: 0.3467\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3330 - val_loss: 0.3396\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3352 - val_loss: 0.3458\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3325 - val_loss: 0.3416\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3317 - val_loss: 0.3395\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3301 - val_loss: 0.3395\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3301 - val_loss: 0.3376\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3293 - val_loss: 0.3466\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3297 - val_loss: 0.3398\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3287 - val_loss: 0.3429\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3282 - val_loss: 0.3395\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3315 - val_loss: 0.3416\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3289 - val_loss: 0.3452\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3266 - val_loss: 0.3311\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3271 - val_loss: 0.3388\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3278 - val_loss: 0.3279\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3259 - val_loss: 0.3284\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3245 - val_loss: 0.3313\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3245 - val_loss: 0.3280\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3241 - val_loss: 0.3290\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3231 - val_loss: 0.3272\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3227 - val_loss: 0.3283\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3221 - val_loss: 0.3266\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3215 - val_loss: 0.3289\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3216 - val_loss: 0.3261\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.3382\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.9096 - val_loss: 0.5321\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4974 - val_loss: 0.4444\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4375 - val_loss: 0.4143\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4103 - val_loss: 0.3989\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3945 - val_loss: 0.3837\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3835 - val_loss: 0.3794\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3747 - val_loss: 0.3722\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3696 - val_loss: 0.3643\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3638 - val_loss: 0.3630\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3578 - val_loss: 0.3525\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3537 - val_loss: 0.3548\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3500 - val_loss: 0.3456\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3472 - val_loss: 0.3457\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3447 - val_loss: 0.3418\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3410 - val_loss: 0.3313\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3382 - val_loss: 0.3400\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3361 - val_loss: 0.3332\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3337 - val_loss: 0.3296\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3306 - val_loss: 0.3226\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3294 - val_loss: 0.3221\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3269 - val_loss: 0.3241\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3246 - val_loss: 0.3238\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3228 - val_loss: 0.3210\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3201 - val_loss: 0.3192\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3183 - val_loss: 0.3140\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3161 - val_loss: 0.3145\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3152 - val_loss: 0.3064\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3114 - val_loss: 0.3131\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3113 - val_loss: 0.3086\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3102 - val_loss: 0.3018\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3086 - val_loss: 0.3065\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3070 - val_loss: 0.2990\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3059 - val_loss: 0.3043\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3058 - val_loss: 0.3184\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3066 - val_loss: 0.2959\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3018 - val_loss: 0.3059\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3016 - val_loss: 0.3009\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3014 - val_loss: 0.3003\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2994 - val_loss: 0.2922\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2980 - val_loss: 0.3002\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2974 - val_loss: 0.2978\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2963 - val_loss: 0.2892\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2957 - val_loss: 0.3022\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2943 - val_loss: 0.2890\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2940 - val_loss: 0.3069\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2924 - val_loss: 0.3000\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2938 - val_loss: 0.2896\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2911 - val_loss: 0.2929\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2920 - val_loss: 0.3047\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2918 - val_loss: 0.2850\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2894 - val_loss: 0.3142\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2942 - val_loss: 0.2901\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2900 - val_loss: 0.2940\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2884 - val_loss: 0.2871\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2894 - val_loss: 0.2844\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2864 - val_loss: 0.2860\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2869 - val_loss: 0.2877\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2886 - val_loss: 0.2951\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2868 - val_loss: 0.2821\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2854 - val_loss: 0.2879\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2852 - val_loss: 0.2955\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2841 - val_loss: 0.2890\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2847 - val_loss: 0.2813\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2838 - val_loss: 0.2876\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2828 - val_loss: 0.3044\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2815 - val_loss: 0.2820\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2834 - val_loss: 0.2805\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2836 - val_loss: 0.2847\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2820 - val_loss: 0.2849\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2823 - val_loss: 0.2805\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2810 - val_loss: 0.2830\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2813 - val_loss: 0.2858\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2806 - val_loss: 0.2790\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2802 - val_loss: 0.3046\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2796 - val_loss: 0.2811\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2791 - val_loss: 0.2866\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2797 - val_loss: 0.2803\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2776 - val_loss: 0.2850\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2780 - val_loss: 0.2763\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2766 - val_loss: 0.2822\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2758 - val_loss: 0.2886\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2770 - val_loss: 0.3021\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2795 - val_loss: 0.2863\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2778 - val_loss: 0.2777\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2775 - val_loss: 0.2824\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2758 - val_loss: 0.2801\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2764 - val_loss: 0.2865\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2757 - val_loss: 0.2771\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2749 - val_loss: 0.2849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae62eb7690&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fae5bd62a50&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae62eb7690&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fae5bd62a50&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae62eb7690&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae62eb7690&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae62eb7690>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fae5bd62a50>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_hidden' : [0,1,2,3],\n",
    "    'n_neurons' : np.arange(1,100),\n",
    "    'learning_rate' : reciprocal(3e-4,3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg,param_dist,n_iter=10,cv=3)\n",
    "rnd_search_cv.fit(X_train,y_train,epochs=100,validation_data=(X_valid,y_valid),callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.007536623143000498, 'n_hidden': 2, 'n_neurons': 31}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.32845398783683777"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
