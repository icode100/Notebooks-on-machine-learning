{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body { font-family: Cambria Math; font-size: 16px; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#markdown customization\n",
    "from IPython.display import display, HTML # type:ignore\n",
    "style = '<style> body { font-family: Cambria Math; font-size: 16px; } </style>'\n",
    "display(HTML(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Training Deep Neural Networks</div>\n",
    "For a model with so many parameters will risk overfitting. There are several problems with backpropagation. That are discussed below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Vanishing / Exploding Gradients</div>\n",
    "The gradients in backpropagation gets smaller and smaller as the algorithm progresses down to the lower layers. As a result Gradient Descent update leaves lower layers connection weights virtually unchanged and training never converges to a good solution. The opposite this state is: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the _exploding gradients_ problem, which is mostly encountered in recurrent neural networks.\\\n",
    "Few suspects in this field are :\n",
    "* the sigmoid logistic function: this creates the anomally as it saturates for a large value of *x* in both posittive and negative direction. Thus making the gradient $0$\n",
    "* weight initialization technique: using normal distribution with mean = 0 and standard deviation=1. Thus each layers output variance is much greater than the previous layer's output. The variance keeps on increasing till the activation function saturates.\n",
    "Thus backpropagation fails out here and  the little gradinet present keeps diluting as the backpropagation proeceds from top layer to down.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining sigmoid\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Glorot and He intilization</div>\n",
    "* The variance of outputs of each layer should be equal to that of the previous layer in forward flow\n",
    "* also the gradients should have equal variance before and after flowing through a layer in the reverse direction. \n",
    "It is impossible to guarantee both of them at the same time unless the layer has an equal number of inputs and neurons (these numbers are called the *fan-in* and *fan-out* of the layer). Thus we tweak the weights of the connections. The weights are now randomly initialized with,\\\n",
    "normal distribution with $\\mu=0$ and $\\sigma = \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$\n",
    "$\\mathbf{OR}$, uniform distribution btwn $-r$ and $r$ with $r=\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$\\\n",
    "Note that the above if the acivation function used is logistic function.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mvN9LaSohnWht80kht_q5__cgD1YRCIAotHDmTEasOmNqo5ljYpNPW9CUz-EY9eV-hG3M1xwqSKJR1gZwFlTYPwTcr6D63vvOt_SjMgN5MRDspQ8IURLEwSX_KmTMxlCOHsu-4QAEgo-ud8JuyT6DURQBIR2HFGx98kGVQRZrJQ36HOfv_tBnrOCXVllTCDLX?width=413&height=127&cropmode=none\" width=\"413\" height=\"127\" />\n",
    "\n",
    "here $fan_{avg} = \\frac{fan_{in}+fan_{out}}{2}$\n",
    "We can use this to speed up training. We can set the layer to the He initialization by setting the layer as\\\n",
    "\n",
    "    keras.layers.Dense(10,activation='relu',kernel_initializer='he_normal')\n",
    "    \n",
    "though we can also use `he_uniform`.\\\n",
    "Also for using fan-avg,\n",
    "\n",
    "\n",
    "    he_avg_init = keras.initializers.VarianceScaling(scale=2.0,mode='fan_avg',distribution='uniform')\n",
    "    keras.layers.Dense(10,activation='sogmoid',kernel_initializer='he_avg_init')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Non Saturating Activation function</div>\n",
    "now we faced the problem of unstable gradients as the sigmoid function saturated to a constant value of both positive and negative thresholds. Thus creating unstable gradients.\\\n",
    "Thus it is important to choose a good activation function.\n",
    "1. ReLU:\n",
    "\n",
    "    * This seems to be a good activation function. But even this suffers from neuron killing, i.e. as the training proceeds some neurons continously output 0\n",
    "    * even with large learning rate a neuron dies when its weights get tweaked such that weighted sum of its inputs are negative for all instance in the training set. Also gradient descent cannot affect it because it's gradient is 0 when input is negative.\n",
    "\n",
    "2. Leaky ReLU:\n",
    "\n",
    "    $LeakyReLU_{\\alpha}(z) = max(\\alpha z,z)$\n",
    "    $\\alpha$ defines how much function leaks. It is actually the slope of the function for $z<0$ and is typically set to 0.01. The small slope ensures that the function do not die. Though they can go on a long break but they will not die. Setting $\\alpha=0.2$(huge leak) results in much better performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining leaky ReLU\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 5.0, -0.5, 4.2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Randomized Leaky ReLU (RReLU):\n",
    "\n",
    "    * the $\\alpha$ is picked up randomly in a given range during training and is fixed to an average value during testing.\n",
    "    * RReLU seems to act as a regularizer. \n",
    "4. parametric leaky ReLU (PReLU):\n",
    "\n",
    "    * here $\\alpha$ is authorized to be learned during training. PReLU outperforms well on large image datasets but on smaller datasets it runs the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu, input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=keras.losses.mse, optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Exponential Linear Unit:\n",
    "\n",
    "    This reduced the training time and performed better on the test set as well. \n",
    "    $ELU_{\\alpha}(z) = \n",
    "    \\begin{cases}\n",
    "        \\alpha(\\text{exp}(z)-2), & \\text{if } z<0\\\\\n",
    "        z, & \\text{if } z \\geq 0\\\\\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining ELU\n",
    "def elu(z, alpha):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 5.0, -4.0, 4.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting with alpha = 0.5\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(z, elu(z, 0.5), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-0.5, -0.5], 'k:')\n",
    "plt.plot([0, 0], [-4,4], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(r\"ELU activation function with $\\alpha=0.5$\", fontsize=14)\n",
    "plt.axis([-5, 5, -4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 5.0, -4.0, 4.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting with alpha = 1\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(z, elu(z, 1), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-4, 4], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k:')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(r\"ELU activation function with $\\alpha=1$\", fontsize=14)\n",
    "plt.axis([-5, 5, -4, 4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "differences btween ReLU and ELU:\n",
    "* It takes on negative values for z < 0thus having aveage output close to 0. Thus helps reducing the vanishing gradients problem.\n",
    "* $\\alpha$ defines the value that ELU approaches when z<0 and is large \n",
    "* It has non zero gradient for z<0 thus helps reduce the dead nerons problem.\n",
    "* If $\\alpha$ = 1 the function is smooth everywhere which helps speed up the gradient descent as it does not bounce as much to the left and right of z=0.\n",
    "\n",
    "6. Scaled ELU (SELU):\n",
    "\n",
    "    * if all the layers use SELU then the network will self normalize. The output of each layer tend to preserve a mean of 0 and variance of 1 during training. Thus totally eliminating the vanishing/exploding gradient anomally.\n",
    "    conditions for self-normalization are:\n",
    "\n",
    "        * input features must be standardized \n",
    "        * every layer's weight must be initialized with LeCun normal initialization.\\\n",
    "            `kernel_initializer='lecun_normal'`\n",
    "        * the network architechture must be sequential (like it should not be like wide and deep network)\n",
    "        * This is effective for CNNs as well\n",
    "    * By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family:C059\">creating a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(learning_rate=1e-3),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full),(X_test,y_test) = fashion_mnist.load_data()\n",
    "X_valid,X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "X_test = X_test/255.0\n",
    "y_valid,y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we scale the inputs to mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 84s 38ms/step - loss: 1.1379 - accuracy: 0.5547 - val_loss: 0.9962 - val_accuracy: 0.6356\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 65s 38ms/step - loss: 0.7011 - accuracy: 0.7407 - val_loss: 0.6063 - val_accuracy: 0.7912\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 65s 38ms/step - loss: 0.5942 - accuracy: 0.7859 - val_loss: 0.5877 - val_accuracy: 0.7980\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 65s 38ms/step - loss: 0.5389 - accuracy: 0.8072 - val_loss: 0.5198 - val_accuracy: 0.8244\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 65s 38ms/step - loss: 0.5338 - accuracy: 0.8112 - val_loss: 0.6266 - val_accuracy: 0.7704\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Batch Normalization</div>\n",
    "working:\\\n",
    "Adding an operation in the model just before or after activation function of each hidden layer. This operation simply zero centers and normalizes each input then scales ans shifts the result using two new paramter vectors per layer. One for scaling and other for shifting. The operation lets model learn the optimal scale and mean of each layer's input.If you add a BN layer as the very first layer of your neural network, you do not need to standardize your train\n",
    "ing set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\\\n",
    "The algoritm estimates each input's mean and standard deviation. It does so by evaluating the mean and standard deviation of input over current mini batch.\n",
    "\n",
    "Algorithm:\n",
    "1. $\\mathbf{\\mu_B} = \\frac{1}{m_B} \\sum_{i=1}^{m_B}\\mathbf{x}^{(i)}$\n",
    "2. $\\mathbf{\\sigma_B}^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B}(\\mathbf{x}^{(i)}-\\mathbf{\\mu_B})^2$\n",
    "3. $\\mathbf{\\hat{x}^{(i)}} = \\frac{\\mathbf{x}^{(i)}-\\mathbf{\\mu_B}}{\\mathbf{\\sigma_B}^2 - \\varepsilon}$\n",
    "4. $\\mathbf{z}^{(i)} = \\gamma \\otimes \\mathbf{\\hat{x}^{(i)}} + \\mathbf{\\beta}$\n",
    "\n",
    "    * $\\mathbf{\\mu_B}: \\text{mean of the mini batch}$\n",
    "    * $\\mathbf{\\sigma_B}: \\text{std. deviation of the mini batch}$\n",
    "    * $m_B: \\text{number of instances in the mini the mini batch}$\n",
    "    * $\\mathbf{\\hat{x}^{(i)}}: \\text{vector of zero centered normalized inputs for the i}^{th} \\text{ instance}$\n",
    "    * $\\otimes: \\text{element wise multiplication}$\n",
    "    * $\\varepsilon: \\text{tiny number that helps avoiding division by zero}$\n",
    "    * $\\mathbf{z}^{(i)}: \\text{output of the batch normalization operation. It is rescaled and shifted version of the inputs}$\n",
    "\n",
    "advantages and limitations:\\\n",
    "So during training, BN standardizes its inputs, then rescales and offsets them. But is faces problems during the testing. we may need to make predictions for individual instances rather than for batches of instances: in this case, we\n",
    "will  have  no  way  to  compute  each  inputs  mean  and  standard  deviation.  Moreover,\n",
    "even if we do have a batch of instances, it may be too small, or the instances may not\n",
    "be  independent  and  identically  distributed,  so  computing  statistics  over  the  batch\n",
    "instances would be unreliable. One solution could be to wait until the end of training,\n",
    "then  run  the  whole  training  set  through  the  neural  network  and  compute  the  mean\n",
    "and standard deviation of each input of the BN layer. These final input means and\n",
    "standard  deviations  could  then  be  used  instead  of  the  batch  input  means  and  stan\n",
    "dard  deviations  when  making  predictions. Most  implementations  of  Batch\n",
    "Normalization estimate these final statistics during training by using a moving aver\n",
    "age of the layers input means and standard deviations.    and    are  estimated  during\n",
    "training,  but  they  are  used  only  after  training  (to  replace  the  batch  input  means  and\n",
    "standard deviations in Equation). Thus batch normalization helps us to use any activation function of our wish and any weight initializer as it normalizes the data by performing it in a totally separate layer the `BatchNormalization` layer in keras. BN acts like regularizer as well reducing the need of other regularization  techniques  (such  as  dropout).\n",
    "\n",
    "though BN add complexity to the model thus NN makes slower predictions due to the extra computations required at each layer. Though it is often possible to fuse BN layer with the previous layer after training thus avoiding the runtime penalty. This is done by updating the previous layers weights and biases so that it directly produces  outputs  of  the  appropriate  scale  and offset. \n",
    "\n",
    "example:\n",
    "if the previous layer computes $out = \\mathbf{X}\\mathbf{W}+\\mathbf{b}$ then the BN layer will compute $out = \\mathbf{\\gamma}\\otimes\\frac{(\\mathbf{X}\\mathbf{W}+\\mathbf{b}-\\mathbf{\\mu})}{\\mathbf{\\sigma}}+\\mathbf{\\beta}$\\\n",
    "Now, if we define $\\mathbf{W'} = \\mathbf{\\gamma}\\otimes\\frac{\\mathbf{W}}{\\mathbf{\\sigma}}$ and $\\mathbf{b'} = \\mathbf{\\gamma}\\otimes\\frac{\\mathbf{b}-\\mathbf{\\mu}}{\\mathbf{\\sigma}}+\\mathbf{\\beta}$,\\\n",
    "Now the equation becomes $out = \\mathbf{X}\\mathbf{W'}+\\mathbf{b'}$. Or we replaced the previous layers weight and biases with updated weight and biases so that we can get rid of the BN layer.\n",
    "\n",
    "##### <div style=\"font-family:fantasy;\">Implementing Batch Normalization with keras</div>\n",
    "Just  add  a  `BatchNormalization`  layer  before  or  after  each  hidden  layers activation  function,  and  optionally  add  a  BN  layer  as  well  as  the  first  layer  in  your\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,activation=keras.activations.elu,kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100,activation=keras.activations.elu,kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation=keras.activations.softmax)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the parameters $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$ are moving averages i.e. they are not affected by backpropagation so they are listed as non-trainale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name,var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it also creates two operations that will be called by the keras at each iteration during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipsit\\AppData\\Local\\Temp\\ipykernel_6596\\3873162892.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  model.layers[1].updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add BN layer before the activation function we add the activation function as separate layers after the BN layer. Since batch normalization  layer includes one offset parameter per input, we can remove the bias term from the previous layer(by using `use_bias=False`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>`momentum` hyperparameter</u>: this is used while updating the exponential moving averages; given a new value $\\mathbf{v}$  (i.e.,  a  new  vector  of  input  means or  standard  deviations  computed  over  the  current  batch),  the  layer  updates  the  running average $\\hat{\\mathbf{v}}$ using the following equation:\n",
    "$$\n",
    "\\hat{\\mathbf{v}} \\leftarrow \\hat{\\mathbf{v}} \\times momentum + \\mathbf{v} \\times (1-momentum)\n",
    "$$\n",
    "A good momentum value is typically close to one.\n",
    "\n",
    "<u>`axis` hyperparameter</u>: it determines which axis is to be normalized. By default it normalizes the last axis.\\\n",
    "ex:<div style=\"border:2px solid;width:50vw;border-radius:4px;\"> When the input\n",
    "    batch  is  2D  (i.e.,  the  batch  shape  is  [batch  size,  features]),  this  means  that  each  input\n",
    "    feature  will  be  normalized  based  on  the  mean  and  standard  deviation  computed\n",
    "    across  all  the  instances  in  the  batch.  For  example,  the  first  BN  layer  in  the  previous\n",
    "    code  example  will  independently  normalize  (and  rescale  and  shift)  each  of  the  784\n",
    "    input features. If we move the first BN layer before the Flatten layer, then the input\n",
    "    batches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will\n",
    "    compute  28  means  and  28  standard  deviations  (1  per  column  of  pixels,  computed\n",
    "    across  all  instances  in  the  batch  and  across  all  rows  in  the  column),  and  it  will  nor\n",
    "    malize  all  pixels  in  a  given  column  using  the  same  mean  and  standard  deviation.\n",
    "    There will also be just 28 scale parameters and 28 shift parameters. If instead you still\n",
    "    want to treat each of the 784 pixels independently, then you should set axis=[1, 2]</div>\n",
    "\n",
    "BN layer uses batch statistics during the training and final statistics after the training(i.e the final values of moving averages).\n",
    "here's how the BN actually works \n",
    "```\n",
    "class BatchNormalization(keras.layers.Layer):\n",
    "    [...]\n",
    "    def call(self,inputs,training=None):\n",
    "        [...]\n",
    "```\n",
    "if we want to make a custom layer that must behave differently during the training and testing then we must add an extra layer in the `call()` namely `training`. Here it is set to `None` by default but during the training it is set to `1`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family:Ani;font-size:22px\">by using a novel fixed-update (fixup) weight initialization technique, the\n",
    "authors  managed  to  train  a  very  deep  neural  network  (10,000  layers!)  without  BN, achieving state-of-the-art performance on complex image classification tasks</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Gradient Clipping</div>\n",
    "* used especially in RNN as using batch normalization is difficult there. \n",
    "* works of the principle of clipping the gradient to a fixed value when it increases a particular threshold. \n",
    "* use to aleviate exploding gradients problem.\n",
    "```\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\",optimizer=optimizer)\n",
    "```\n",
    "This clips all the values of the gradient vector between -1 and 1. This means that all the partial derivatives of the loss wrt all the trainable parameter will be shifted between -1 and 1. because of this the direction of the gradinet vector gets altered. To resolve this we clip the gradients by norm. This is done by using `clipnorm` instead of `clipvalue`. For  example,  if  you  set  clipnorm=1.0,  then  the  vector  [0.9,  100.0]\n",
    "will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. Though we can use both and check which works best for our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Reusing Pretrained Layers</div>\n",
    "we can user the lower layers of a pretrained model that is trained to do a similar task as we are trying to do. This techinque is called *transfer learning*. It will not only speed up the training but also require less training data.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mSq9V0M8LeQHcqA637PQYbjNQ8SfFBEtStzQU1AzTPU9TyQWTavcAwtaecsUHVIt2V6mpm8xA33Z_yygsruHUVvqNTGFESu-1jH-JrO-2Dls_0bVp_BY7KrujtYYVwOnzOyFQ2nhbgh77Wnf3zfv2Dxz6pHGaT6OORZBVSSChJxp4S5Ixu2b5Ep7Z4gWRmJ-y?width=544&height=375&cropmode=none\" width=\"544\" height=\"375\" />\n",
    "\n",
    "the upper hidden layers of the original model are less likely to be as useful as the lower layer, since high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. But the output layer needs to be different (obviously!!!).\n",
    "\n",
    "Now to find the number of layers to use, \n",
    "1. we freeze all the resued layers at first(i.e. making their weights non-trainable or unaffected by the gradient descent). Then we train the model and note the performance.\n",
    "2. we unfreeze one or two of the top hidden layers to let the backpropgation tweak them and note if the performance improves(it is better to reduce the learning rate after unfreezing as this will avoid wrecking their fine-tuned weights.){more the training data more layers we can unfreeze}.\n",
    "\n",
    "If you still cannot get good performance, and you have little training data, try drop\n",
    "ping  the  top  hidden  layer(s)  and  freezing  all  the  remaining  hidden  layers  again.  You\n",
    "can  iterate  until  you  find  the  right  number  of  layers  to  reuse.  If  you  have  plenty  of\n",
    "training data, you may try replacing the top hidden layers instead of dropping them,\n",
    "and even adding more hidden layers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Transfer learning with keras</div>\n",
    "we here use fashion mnist dataset and divide it into two datasets one without the images of sandals and shirt and other small datasetn with only 200 images of sandals or shirt. we here are intended to build a binary classifier with shirt as positive class and sandals as negative class.\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: if it is a shirt then set 1 (positive class)\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28],name=\"inp_layer\"))\n",
    "i=1\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\",name=f\"hidden_{i}\"))\n",
    "    i+=1\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\",name=\"out_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inp_layer (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " hidden_3 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " hidden_4 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " hidden_5 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(learning_rate=1e-3),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 19s 13ms/step - loss: 0.6228 - accuracy: 0.7961 - val_loss: 0.4023 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.3631 - accuracy: 0.8764 - val_loss: 0.3322 - val_accuracy: 0.8842\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.3188 - accuracy: 0.8893 - val_loss: 0.3018 - val_accuracy: 0.8999\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.2979 - accuracy: 0.8973 - val_loss: 0.2883 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 10s 7ms/step - loss: 0.2838 - accuracy: 0.9021 - val_loss: 0.2760 - val_accuracy: 0.9068\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 10s 7ms/step - loss: 0.2740 - accuracy: 0.9057 - val_loss: 0.2709 - val_accuracy: 0.9063\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2655 - accuracy: 0.9096 - val_loss: 0.2670 - val_accuracy: 0.9068\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 16s 12ms/step - loss: 0.2587 - accuracy: 0.9116 - val_loss: 0.2578 - val_accuracy: 0.9123\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 14s 10ms/step - loss: 0.2531 - accuracy: 0.9138 - val_loss: 0.2564 - val_accuracy: 0.9128\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.2485 - accuracy: 0.9152 - val_loss: 0.2532 - val_accuracy: 0.9128\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 11s 8ms/step - loss: 0.2440 - accuracy: 0.9167 - val_loss: 0.2466 - val_accuracy: 0.9123\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 11s 8ms/step - loss: 0.2399 - accuracy: 0.9184 - val_loss: 0.2475 - val_accuracy: 0.9141\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 10s 7ms/step - loss: 0.2366 - accuracy: 0.9190 - val_loss: 0.2430 - val_accuracy: 0.9143\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 12s 8ms/step - loss: 0.2333 - accuracy: 0.9204 - val_loss: 0.2396 - val_accuracy: 0.9153\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.2304 - accuracy: 0.9214 - val_loss: 0.2407 - val_accuracy: 0.9168\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 11s 8ms/step - loss: 0.2272 - accuracy: 0.9220 - val_loss: 0.2375 - val_accuracy: 0.9173\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 10s 8ms/step - loss: 0.2247 - accuracy: 0.9234 - val_loss: 0.2401 - val_accuracy: 0.9143\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 11s 8ms/step - loss: 0.2219 - accuracy: 0.9242 - val_loss: 0.2450 - val_accuracy: 0.9121\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 10s 8ms/step - loss: 0.2194 - accuracy: 0.9248 - val_loss: 0.2328 - val_accuracy: 0.9178\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 11s 8ms/step - loss: 0.2174 - accuracy: 0.9257 - val_loss: 0.2342 - val_accuracy: 0.9150\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"./models/model_A_MNIST.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"./models/model_A_MNIST.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inp_layer (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " hidden_3 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " hidden_4 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " hidden_5 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A.summary()#type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])#type:ignore\n",
    "model_B_on_A.add(keras.layers.Dense(1,activation=keras.activations.sigmoid))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem is training `model_B_on_A` will affect `model_A` as `model_A`'s layers are being directly used by `model_B_on_A`. Thus to avoid this we clone `model_A`. But even then the weights will not be cloned thus we need to clone the weights separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())#type:ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now during training `model_B_on_A` since the output layer is randomly added, it will make large errors atleast during first few epochs. So there will be large error gradients that may wreck the reused weights. To avoid this one approach is to freeze the reused layers during the first few epochs giving the new layer some time to learn reasonable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we compile the model\n",
    "model_B_on_A.compile(loss='binary_crossentropy',optimizer='sgd',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 68ms/step - loss: 0.4077 - accuracy: 0.7700 - val_loss: 0.3174 - val_accuracy: 0.8641\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2783 - accuracy: 0.8900 - val_loss: 0.2384 - val_accuracy: 0.9229\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.2105 - accuracy: 0.9250 - val_loss: 0.1944 - val_accuracy: 0.9493\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1715 - accuracy: 0.9550 - val_loss: 0.1644 - val_accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B,y_train_B,epochs=4,validation_data=(X_valid_B,y_valid_B))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we unfreeze the reused layers and use recompile the `model_B_on_A` and then fit the data. Also we should lower the learning rate which is by default `1e-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=True\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 2s 90ms/step - loss: 0.1517 - accuracy: 0.9700 - val_loss: 0.1618 - val_accuracy: 0.9635\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.1490 - accuracy: 0.9700 - val_loss: 0.1595 - val_accuracy: 0.9645\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.1464 - accuracy: 0.9700 - val_loss: 0.1572 - val_accuracy: 0.9645\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1439 - accuracy: 0.9700 - val_loss: 0.1550 - val_accuracy: 0.9665\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1414 - accuracy: 0.9700 - val_loss: 0.1530 - val_accuracy: 0.9665\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.1392 - accuracy: 0.9700 - val_loss: 0.1511 - val_accuracy: 0.9675\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1371 - accuracy: 0.9700 - val_loss: 0.1492 - val_accuracy: 0.9675\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.1351 - accuracy: 0.9700 - val_loss: 0.1473 - val_accuracy: 0.9675\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1330 - accuracy: 0.9750 - val_loss: 0.1454 - val_accuracy: 0.9686\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.1310 - accuracy: 0.9750 - val_loss: 0.1438 - val_accuracy: 0.9696\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1292 - accuracy: 0.9850 - val_loss: 0.1421 - val_accuracy: 0.9716\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1274 - accuracy: 0.9850 - val_loss: 0.1405 - val_accuracy: 0.9736\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1255 - accuracy: 0.9850 - val_loss: 0.1389 - val_accuracy: 0.9736\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1239 - accuracy: 0.9850 - val_loss: 0.1373 - val_accuracy: 0.9736\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1222 - accuracy: 0.9850 - val_loss: 0.1357 - val_accuracy: 0.9736\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1204 - accuracy: 0.9850 - val_loss: 0.1342 - val_accuracy: 0.9746\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B,y_train_B,epochs=16,validation_data=(X_valid_B,y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13329514861106873, 0.9779999852180481]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B,y_test_B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But actually this is just framed if we change the random seed by a little bit then the improvements may vanish. Lesson: You should be always suspicious about perfection because nothing is perfect it might be a pretending model in that particular data. This is called \"torturing till confession\".\n",
    "\n",
    "It  turns  out  that  transfer  learning  does  not  work  very  well  with\n",
    "small  dense  networks,  presumably  because  small  networks  learn  few  patterns,  and\n",
    "dense  networks  learn  very  specific  patterns,  which  are  unlikely  to  be  useful  in  other\n",
    "tasks.  Transfer  learning  works  best  with  deep  convolutional  neural  networks,  which\n",
    "tend  to  learn  feature  detectors  that  are  much  more  general  (especially  in  the  lower\n",
    "layers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Unsupervised Pretraining</div>\n",
    "If we have lots of unlabelled training data but very few labelled data and we cannot find a similar model trained on that then *unsupervised pretrained* can help.\n",
    "\n",
    "we can gather plenty of unlabelled data and train as unsupervised model on it such as autoencoders or generative adversarial network. Then we can use the lower layers of autoencoder or GAN discriminator, add the output layer for our task on the top and fine tune the final network using supervised learning(i.e with some labelled training examples).\n",
    "\n",
    "In earlier days people used to train the deep learning models using a technique called *greedy layer-wise pretraining*. Here they would first train an unsupervised model with a single layer, typically an Restricted Boltzmann Machine(RBM), then would freeze that layer and add another one on top of it, then train the model again (effectively just training the new layer), then freeze this new layer and another on top of it train the model again and so on. Now a days these models are being trained in one shot using autoencoders and GAN descriminators rather than RBMs.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mwB6oc5ckhN7BmpMSpT9noPoRuIE3CDbhKApugtC5w-2ByKyHW7i1u0IZYBVBKZcyHTA0lJHx3SuTtJ08whirId8sN1S6gy8OoexQsnc3ZiGgvmxypVdN552MnlhwTeVEdwgn781Y7H4tawqJsTNm_M2wBFjBe3kclK148-NLROpq-ncfIcFoM1W16y4ft5lf?width=710&height=473&cropmode=none\" width=\"510\" height=\"273\" />\n",
    "\n",
    "### <div style=\"font-family:fantasy;\">Pretraining on an Auxiliary Task</div>\n",
    "If you do not have much labeled training data, one last option is to train a first neural\n",
    "network  on  an  auxiliary  task  for  which  you  can  easily  obtain  or  generate  labeled\n",
    "training  data,  then  reuse  the  lower  layers  of  that  network  for  your  actual  task.  The\n",
    "first neural networks lower layers will learn feature detectors that will likely be reusa\n",
    "ble by the second neural network.\n",
    "\n",
    "For  example,  if  you  want  to  build  a  system  to  recognize  faces,  you  may  only  have  a\n",
    "few pictures of each individualclearly not enough to train a good classifier. Gather\n",
    "ing hundreds of pictures of each person would not be practical. You could, however,\n",
    "gather a lot of pictures of random people on the web and train a first neural network\n",
    "to  detect  whether  or  not  two  different  pictures  feature  the  same  person. Such a network  would  learn  good  feature  detectors  for  faces,  so  reusing  its  lower  layers\n",
    "would allow you to train a good face classifier that uses little training data. \n",
    "\n",
    "For  natural  language  processing  (NLP)  applications,  you  can  download  a  corpus  of\n",
    "millions of text documents and automatically generate labeled data from it. For exam\n",
    "ple, you could randomly mask out some words and train a model to predict what the\n",
    "missing words are (e.g., it should predict that the missing word in the sentence What\n",
    "___ you saying? is probably are or were). If you can train a model to reach good\n",
    "performance  on  this  task,  then  it  will  already  know  quite  a  lot  about  language,  and\n",
    "you  can  certainly  reuse  it  for  your  actual  task  and  fine-tune  it  on  your  labeled  data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Fast Optimizers</div>\n",
    "Another huge speed boost in training of DNN come from using faster optimizers rather than Gradient Descent optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Momentum optimiation</div>\n",
    "Imagine  a  bowling  ball  rolling  down  a  gentle  slope  on  a  smooth  surface:  it  will  start\n",
    "out slowly, but it will quickly pick up momentum until it eventually reaches terminal\n",
    "velocity (if there is some friction or air resistance). This is the very simple idea behind\n",
    "*momentum  optimization*. while gradient descent will take small regular steps down the slope, so the algorithm will take muvh more time to reach the bottom.\\\n",
    "In gradient Descent the weights are updated by directly subtracting the gradient of the cost function $J(\\mathbf{\\theta})$ with regrards to the weights $\\mathbf{\\theta}$ i.e. ($\\nabla_{\\theta}J({\\mathbf{\\theta}})$). Or by equation,\n",
    ">$$\\mathbf{\\theta}\\leftarrow\\mathbf{\\theta}-\\eta\\nabla_{\\theta}J({\\mathbf{\\theta}})$$\n",
    "It does not care about the earlier gradients.\n",
    "\n",
    "Momentum  optimization  cares  a  great  deal  about  what  previous  gradients  were:  at\n",
    "each  iteration,  it  subtracts  the  local  gradient  from  the  _momentum_  vector  **m**  (multiplied  by  the  learning  rate  ),  and  it  updates  the  weights  by  adding  this  momentum\n",
    "vector . In other words, the gradient is used for acceleration, not\n",
    "for speed. To simulate some sort of friction mechanism and prevent the momentum\n",
    "from growing too large, the algorithm introduces a new hyperparameter , called the\n",
    "momentum, which must be set between 0 (high friction) and 1 (no friction). A typical\n",
    "momentum value is 0.9."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*momentum algorithm*\n",
    "\n",
    ">$\\mathbf{m}\\leftarrow\\beta\\mathbf{m}-\\eta\\nabla_{\\theta}J({\\mathbf{\\theta}})$\n",
    "\n",
    ">$\\mathbf{\\theta}\\leftarrow\\mathbf{\\theta}+\\mathbf{m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the speed increases to a certain value and stops increasing. OR, we can say that the rate of change of momentum is constant and the terminal velocity(i.e. the maximum size of weight updates) is equal to $gradient\\text{(the acceletation or rate of change of momentum)}\\times\\frac{1}{1-\\beta}$.\\\n",
    "Thus, similar to general physics considering $\\beta$ as coefficient of friction. The terminal velocity will be inversely proportional to $\\beta$. Thus for a $\\beta$ this algorithm goes $\\frac{1}{1-\\beta}$ tmes faster than gradient descent. This will go down the valley faster and faster until it reaches the minimum. In DNN without batch normalization upper layers will often end up having inputs with very different scales so using momentum optimization helps.\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=0.001,momentum=0.9)\n",
    "\n",
    "where $\\beta$ is defined by the `momentum` hyperparameter.\n",
    "Only one drawback is it adds another hyperparameter to tune. Though 0.9 momentum value works well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Nesterov Accelerated Gradient</div>\n",
    "It measures the gradient of the cost function not at the local position function not at the local position but slightly ahead in the direction of momentum at $\\boldsymbol{\\theta}+\\beta \\mathbf{m}$.\n",
    ">$\\boldsymbol{m}\\leftarrow\\beta\\boldsymbol{m}-\\eta\\nabla_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}+\\beta\\boldsymbol{m})$\n",
    "\n",
    ">$\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\boldsymbol{m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum points towards the optimum. So it will be slightly more accurate to use gradient measured a bit farther in that direction rather than the gradient at the same position. Also this ends slightly closer to the optimum. Now finally these small improvements add up and NAG ends up being significantly faster than the regular momentum optimization. Also, it is always a bit closer to the optimum. Thus pushing more towards the valley. This helps it to converge faster.\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=0.001,momentum=0.9,nesterov=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Ada Grad</div>\n",
    "This algortihm corrects the direction of progress to point more where the gradient is more inclined towards the goal at an earlier phase. Quite like the nesterov's accelerated gradient.\n",
    "\n",
    "*AdaGrad algorithm*\n",
    ">$\\mathbf{s}\\leftarrow\\mathbf{s}+\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "\n",
    ">$\\mathbf{\\theta}\\leftarrow\\mathbf{\\theta}-\\eta\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\varepsilon}$\n",
    "\n",
    "Now the first equation accumulates the sqare of the gradients to the vector $\\mathbf{s}$. $\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$ can also be written as $(\\frac{\\partial{J(\\boldsymbol{\\theta})}}{\\partial\\theta_i})^2 \\forall i=0\\rightarrow n$.\\\n",
    "Thus equation $s_i\\leftarrow s_i+(\\frac{\\partial{J(\\boldsymbol{\\theta})}}{\\partial\\theta_i})^2$ for each element $s_i$ of vector $\\vec{s}$. OR, each $s_i$ accumulates the square of partial derivatives of cost function with regards to $\\theta_i$ if cost function is steeper along $\\theta_i$ then $s_i$ will get larger and larger over each iteration.\\\n",
    "In the second equation gradient is scaled down by a factor $\\sqrt{\\mathbf{s}+\\varepsilon}$, so as to reach to a point of steeper gradient but at an early stage.\\\n",
    "Thus, it decays the learning rate by a factor $\\sqrt{\\mathbf{s}+\\varepsilon}$ but does it faster for the steep dimension rather than gentler sloped dimensions. This is called adaptive learning rate.\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mszbW29OdilLYFz5UCsnUzMUposUnLP3DJ5fXjVdrSuFg9pyaqP-wcYi65O7FJdUoKvQvS_2PDdNQyijDtimvJJmrIgE5SHVwWN0y2pYhSs6yL0oGu1oYAgkcDwex8RQZhAjzHVQ7_8CCH2RuEeIpy221YRtQaO1HmJ0AX-3wIrdJ1PhFTM9qf_3sARU5A3yy?width=2659&height=2034&cropmode=none\" width=\"459\" height=\"334\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">RMS Prop</div>\n",
    "Ada grad risks slowing down to fast and never converging to global optimum.\n",
    "\n",
    "RMS prop fixes this by accumulating onlygradients from most recent iterations(as opposed to all the gradients in the gradients). It does so by using exponential decays in the first step.\n",
    "\n",
    ">$\\mathbf{s}\\leftarrow\\beta\\mathbf{s}+(1-\\beta)\\nabla_{\\boldsymbol{\\theta}}J({\\boldsymbol{\\theta}})\\otimes\\nabla_{\\boldsymbol{\\theta}}J({\\boldsymbol{\\theta}})$\n",
    "\n",
    ">$\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}J({\\boldsymbol{\\theta}})\\oslash\\sqrt{\\mathbf{s}+\\varepsilon}$\n",
    "\n",
    "The decay rate $\\beta=0.9$ generally works.\n",
    "\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001,rho=0.9)\n",
    "\n",
    "`rho` defines $\\beta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Adam and Nadam Optimizaation</div>\n",
    "$Adaptive \\ momentum \\ estimation = momentum \\ optimization + RMSprop$\n",
    "\n",
    "Like momentum optimization it keeps track of an exponentially decaying average of past gradients.\n",
    "\n",
    "Like RMSprop it keeps tracks of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "1. $\\mathbf{m}\\leftarrow\\beta_1\\mathbf{m} - (1-\\beta_1)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "2. $\\mathbf{s}\\leftarrow\\beta_2\\mathbf{s} - (1-\\beta_2)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "3. $\\widehat{\\mathbf{m}}\\leftarrow\\frac{\\mathbf{m}}{1-\\beta_1^t}$\n",
    "4. $\\widehat{\\mathbf{s}}\\leftarrow\\frac{\\mathbf{s}}{1-\\beta_2^t}$\n",
    "5. $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\eta\\widehat{\\boldsymbol{m}}\\oslash\\sqrt{\\widehat{\\mathbf{s}}+\\varepsilon}$\n",
    "\n",
    "Here $t$ represents the iteration count. The difference between RMSprop and momentum optimization is in step 1 it computes the exponentially decaying average, rather that just the sum. Though these are equivalent they differ only by a constant factor: decaying average is $(1-\\beta_1)$ times the decaying sum. In step 2 and 4 **m** and **s** are initialized at 0, they will be biased toward 0 at the beginning of training,so these two steps will help boost **m** and **s** at the beginning of training. The momentum decay hyperparameter $_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $_2$  is  often  initialized  to  0.999.  As  earlier,  the  smoothing term $$ is usually initialized to a tiny number such as 107.\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\n",
    "\n",
    "as it is an adaptive algorithm it requires less tuning of learning rate $\\eta$\n",
    "\n",
    "variants of adam's optimization:\n",
    "\n",
    "*AdaMax*\\\n",
    "In step 5 Adam scales down the parameter updates by $\\ell_2$ norm of the time-decayed gradients. Adamax does the same thing but by using $\\ell_\\infty$, which is practically finding the maximum of the cumulation vector $\\mathbf{s}$ . OR, specifically it replaces step 2 with $\\mathbf{s}\\leftarrow max(\\beta_2\\mathbf{s},\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}))$ and drops down step 4 and in 5 it scales down the gradients by a factor $\\mathbf{s}$ which is just the max of the time decayed gradients.\n",
    "In step 5 Adam scales down the parameter updates by $\\ell_2$ norm of the time-decayed gradients. Adamax does the same thing but by using $\\ell_\\infty$, which is practically finding the maximum of the cumulation vector $\\mathbf{s}$ . OR, specifically it replaces step 2 with $\\mathbf{s}\\leftarrow max(\\beta_2\\mathbf{s},\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}))$ and drops down step 4 and in 5 it scales down the gradients by a factor $\\mathbf{s}$ which is just the max of the time decayed gradients.\\\n",
    "Thus, algortihm is:\n",
    "\n",
    "1. $\\mathbf{m}\\leftarrow\\beta_1\\mathbf{m} - (1-\\beta_1)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "2. $\\mathbf{s}\\leftarrow max(\\beta_2\\mathbf{s},\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}))$\n",
    "3. $\\widehat{\\mathbf{m}}\\leftarrow\\frac{\\mathbf{m}}{1-\\beta_1^t}$\n",
    "4. $\\widehat{\\mathbf{s}}\\leftarrow\\frac{\\mathbf{s}}{1-\\beta_2^t}$\n",
    "5. $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\eta\\widehat{\\boldsymbol{m}}\\oslash(\\widehat{\\mathbf{s}}+\\varepsilon)$\n",
    "\n",
    "*Nadam*\\\n",
    "$Nadam = Adam+Nesterov$\\\n",
    "Thus is converges slightly faster than adam. It is sometimes outperformed by RMSprop.\n",
    "\n",
    "<div style=\"padding: 5px;border: 2px solid;border-radius: 4px;justify-content: center;align-items:center;width:50vw\">\n",
    "    All  the  optimization  techniques  discussed  so  far  only  rely  on  the  first-order  partial\n",
    "    derivatives  (Jacobians).  The  optimization  literature  also  contains  amazing  algorithms\n",
    "    based  on  the  second-order  partial  derivatives  (the  Hessians,  which  are  the  partial\n",
    "    derivatives  of  the  Jacobians).  Unfortunately,  these  algorithms  are  very  hard  to  apply\n",
    "    to  deep  neural  networks  because  there  are  n2  Hessians  per  output  (where  n  is  the\n",
    "    number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often  dont  even  fit  in  memory,  and  even  when  they  do,  computing  the  Hessians  is\n",
    "    just too slow.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Learning Rate Scheduling</div>\n",
    "If  you  set  it  much  too  high,  training\n",
    "may  diverge  If  you  set  it  too\n",
    "low,  training  will  eventually  converge  to  the  optimum,  but  it  will  take  a  very  long\n",
    "time.  If  you  set  it  slightly  too  high,  it  will  make  progress  very  quickly  at  first,  but  it\n",
    "will  end  up  dancing  around  the  optimum,  never  really  settling  down. If  you  have  a\n",
    "limited computing budget, you may have to interrupt training before it has converged\n",
    "properly, yielding a suboptimal solution.  you  can  find  a  good  learning  rate  by  training  the\n",
    "model for a few hundred iterations, exponentially increasing the learning rate from a\n",
    "very  small  value  to  a  very  large  value,  and  then  looking  at  the  learning  curve  and\n",
    "picking  a  learning  rate  slightly  lower  than  the  one  at  which  the  learning  curve  starts\n",
    "shooting back up. You can then reinitialize your model and train it with that learning\n",
    "rate.\n",
    "\n",
    "We can do better with non-constant learning rate. That is we start with a very high leanring rate and lower it once the algorithm stops making fast rogress, in this way we can make fast progress to reach a good solution faster than with constant learning rate.\n",
    "\n",
    "*Power Scheduling*\\\n",
    "Here we set the learning rate as a function of the iteration count $t$,\n",
    "$$\\eta(t) = \\frac{\\eta_0}{(1+\\frac{t}{s})^c}$$\n",
    "The lr drops to half after $s$ steps then to one-third after another $s$ steps or net $2s$ steps and so on(if the power $c$ is set to 1, which is the general case). Thus the learning rate reduced at a faster rate initially and then the pace reduces. $\\eta_0$ is the initial learning rate.\n",
    "\n",
    "*exponential scheduling*\\\n",
    "Here the learning rate reduces by a factor of 10 after every *s* steps. Here the count does not fix to sequence of 1 but a power of 10 after every *s* steps.\n",
    "$$\\eta(t) = \\frac{\\eta_0}{10^{\\frac{t}{s}}}$$\n",
    "\n",
    "*piecewise constant scheduling*\\\n",
    "Use a constant learning rate for a number of epochs (e.g., $_0$ = 0.1 for 5 epochs),\n",
    "then a smaller learning rate for another number of epochs (e.g., $_1$ = 0.001 for 50\n",
    "epochs),  and  so  on.  Although  this  solution  can  work  very  well,  it  requires  fid\n",
    "dling  around  to  figure  out  the  right  sequence  of  learning  rates  and  how  long  to\n",
    "use each of them.\n",
    "\n",
    "*performance scheduling*\\\n",
    "Measure  the  validation  error  every  N  steps  (just  like  for  early  stopping),  and\n",
    "reduce the learning rate by a factor of  when the error stops dropping.\n",
    "\n",
    "*1cycle scheduling*\\\n",
    "starts  by  increasing  the  initial  learning  rate  0,  growing  linearly  up  to  1\n",
    "halfway through training. Then it decreases the learning rate linearly down to 0\n",
    "again  during  the  second  half  of  training,  finishing  the  last  few  epochs  by  drop\n",
    "ping the rate down by several orders of magnitude (still linearly). The maximum\n",
    "learning  rate  1  is  chosen  using  the  same  approach  we  used  to  find  the  optimal\n",
    "learning  rate,  and  the  initial  learning  rate  0  is  chosen  to  be  roughly  10  times\n",
    "lower.  When  using  a  momentum,  we  start  with  a  high  momentum  first  (e.g.,\n",
    "0.95),  then  drop  it  down  to  a  lower  momentum  during  the  first  half  of  training\n",
    "(e.g.,  down  to  0.85,  linearly),  and  then  bring  it  back  up  to  the  maximum  value\n",
    "(e.g.,  0.95)  during  the  second  half  of  training,  finishing  the  last  few  epochs  with\n",
    "that  maximum  value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing power scheduling\n",
    "optimizer_2 = keras.optimizers.SGD(learning_rate=0.01,decay=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`decay` is $\\frac{1}{s}$ and $c$ is assumed to be 1 in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing exponential scheduling \n",
    "def exponential_decay(eta0,s):\n",
    "    return lambda epoch:eta0//(10**(epoch/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(eta0=0.01,s=20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this as a callback argument\n",
    "```python\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating  the  learning  rate  once  per  epoch  is  usually\n",
    "enough,  but  if  you  want  it  to  be  updated  more  often,  for  example  at  every  step,  you\n",
    "can always write your own callback\n",
    "\n",
    "```python\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr * 0.1**(1 / self.s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating  the  learning  rate  at  every  step  makes  sense  if\n",
    "there  are  many  steps  per  epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "else we can also use:\n",
    "```python\n",
    "keras.optimizers.schedules\n",
    "```\n",
    "\n",
    "The schedule function can optionally take the current learning rate as a second argu\n",
    "ment.  For  example,  the  following  schedule  function  multiplies  the  previous  learning\n",
    "rate  by  0.1<sup>1/20</sup>,  which  results  in  the  same  exponential  decay  (except  the  decay  now\n",
    "starts at the beginning of epoch 0 instead of 1):\n",
    "\n",
    "```python\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When  you  save  a  model,  the  optimizer  and  its  learning  rate  get  saved  along  with  it.\n",
    "This means that with this new schedule function, you could just load a trained model\n",
    "and continue training where it left off, no problem. Things are not so simple if your\n",
    "schedule  function  uses  the  epoch  argument,  however:  the  epoch  does  not  get  saved,\n",
    "and  it  gets  reset  to  0  every  time  you  call  the  `fit()`  method.  If  you  were  to  continue\n",
    "training a model where it left off, this could lead to a very large learning rate, which\n",
    "would likely damage your models weights. One solution is to manually set the `fit()`\n",
    "methods `initial_epoch` argument so the epoch starts at the right value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if\n",
    "you pass the following callback to the `fit()` method, it will multiply the learning rate\n",
    "by 0.5 whenever the best validation loss does not improve for five consecutive epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can implement the same exponential scheduling using ```keras.optimizers.schedules```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)#type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing 1cycle scheduling\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))#type:ignore\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)#type:ignore\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs#type:ignore\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Avoiding Overfitting Through Regularization</div>\n",
    "### <div style=\"font-family:fantasy;\"><sub>1</sub> and <sub>2</sub> Normalization</div>\n",
    "here is how we can use $\\ell_2$ optimization with keras,\n",
    "```python\n",
    "layer = keras.layers.Dense(100,activation='elu',kernel_initializer = 'he_normal',kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "```\n",
    "The `l2()` function returns a regularizer that will be called at each step during training\n",
    "to compute the regularization loss. This is then added to the final loss. As you might\n",
    "expect,  you  can  just  use  keras.regularizers.`l1() ` if  you  want  $_1$  regularization;  if\n",
    "you want both $_1$ and $_2$ regularization, use keras.regularizers.`l1_l2()` (specifying\n",
    "both regularization factors)\n",
    "\n",
    "To make the code a bit organized we can use the following,\n",
    "\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Dropout</div>\n",
    " at  every  training  step,  every  neuron  (including  the\n",
    "input neurons, but always excluding the output neurons) has a probability *p* of being\n",
    "temporarily  dropped  out,  meaning  it  will  be  entirely  ignored  during  this  training\n",
    "step, but it may be active during the next step. The hyperparameter\n",
    "*p* is called the dropout rate, and it is typically set between 10% and 50%: closer to 20\n",
    "30% in recurrent neural nets, and closer to 4050% in convolutional\n",
    "neural  networks.  After  training,  neurons  dont  get  dropped  anymore. \n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mHYO9cV2PJOvljKMY-mrhrNLd0Ilvh9OobL_-zfDzCngJT3DLIDyecNws2QFepkbqoD1RDypqQPZYxPOpHWUgkjFcr6iET-Io_R6jLmpvq3EljcYHfDgxWvo-qaKpZGHiQWi0-3j7J0UQdpS9yYbHC0NRrkXjEudYs88weEgDikQA3lpbRWUrF6xH8lxvc4MO?width=750&height=313&cropmode=none\" width=\"750\" height=\"313\" />\n",
    "\n",
    "Neurons  trained  with  dropout  cannot  co-adapt  with  their\n",
    "neighboring  neurons;  they  have  to  be  as  useful  as  possible  on  their  own.  They  also\n",
    "cannot rely excessively on just a few input neurons; they must pay attention to each of\n",
    "their input neurons. They end up being less sensitive to slight changes in the inputs.\n",
    "In the end, you get a more robust network that generalizes better.\n",
    "\n",
    "There  is  one  small  but  important  technical  detail.  Suppose  *p*  =  50%,  in  which  case\n",
    "during  testing  a  neuron  would  be  connected  to  twice  as  many  input  neurons  as  it\n",
    "would be (on average) during training. To compensate for this fact, we need to multi\n",
    "ply  each  neurons  input  connection  weights  by  0.5  after  training.  If  we  dont,  each\n",
    "neuron  will  get  a  total  input  signal  roughly  twice  as  large  as  what  the  network  was\n",
    "trained on and will be unlikely to perform well. More generally, we need to multiply\n",
    "each  input  connection  weight  by  the  *keep  probability*  (1    *p*)  after  training.  Alterna\n",
    "tively,  we  can  divide  each  neurons  output  by  the  keep  probability  during  training\n",
    "(these alternatives are not perfectly equivalent, but they work equally well).\n",
    "\n",
    "`keras` does this division by default and we don't need to do it while using keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01,nesterov=True,momentum=0.9)\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 26s 13ms/step - loss: 0.5605 - accuracy: 0.8009 - val_loss: 0.4137 - val_accuracy: 0.8524\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.4535 - accuracy: 0.8350 - val_loss: 0.3747 - val_accuracy: 0.8696\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.4194 - accuracy: 0.8474 - val_loss: 0.3490 - val_accuracy: 0.8726\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3988 - accuracy: 0.8544 - val_loss: 0.3361 - val_accuracy: 0.8754\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3938 - accuracy: 0.8554 - val_loss: 0.3263 - val_accuracy: 0.8768\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3814 - accuracy: 0.8601 - val_loss: 0.3129 - val_accuracy: 0.8896\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.3689 - accuracy: 0.8652 - val_loss: 0.3337 - val_accuracy: 0.8800\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3612 - accuracy: 0.8673 - val_loss: 0.3159 - val_accuracy: 0.8800\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.3556 - accuracy: 0.8686 - val_loss: 0.3137 - val_accuracy: 0.8858\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.3477 - accuracy: 0.8721 - val_loss: 0.3141 - val_accuracy: 0.8858\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 18s 11ms/step - loss: 0.3493 - accuracy: 0.8718 - val_loss: 0.3146 - val_accuracy: 0.8860\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 18s 11ms/step - loss: 0.3407 - accuracy: 0.8752 - val_loss: 0.3410 - val_accuracy: 0.8790\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3366 - accuracy: 0.8750 - val_loss: 0.3068 - val_accuracy: 0.8868\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3345 - accuracy: 0.8771 - val_loss: 0.3111 - val_accuracy: 0.8934\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3295 - accuracy: 0.8786 - val_loss: 0.3202 - val_accuracy: 0.8888\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.3260 - accuracy: 0.8805 - val_loss: 0.3127 - val_accuracy: 0.8882\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3218 - accuracy: 0.8824 - val_loss: 0.3241 - val_accuracy: 0.8858\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.3234 - accuracy: 0.8814 - val_loss: 0.3161 - val_accuracy: 0.8862\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3188 - accuracy: 0.8826 - val_loss: 0.3198 - val_accuracy: 0.8890\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3180 - accuracy: 0.8834 - val_loss: 0.3033 - val_accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "#fit \n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.3149 - accuracy: 0.8845\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since  dropout  is  only  active  during  training,  comparing  the  train\n",
    "ing  loss  and  the  validation  loss  can  be  misleading.  In  particular,  a\n",
    "model  may  be  overfitting  the  training  set  and  yet  have  similar\n",
    "training and validation losses. So make sure to evaluate the training\n",
    "loss without dropout (e.g., after training).\n",
    "\n",
    "we should increase the rate if the model is overfitting and reduce it if it is underfitting. It  can  also  help  to  increase  the  dropout  rate  for  large  layers,  and  reduce  it  for small  ones.\n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
    "much better model when tuned properly.\n",
    "\n",
    "<div style=\"border:2px solid;border-radius:4px;width:50vw;\">\n",
    "If  you  want  to  regularize  a  self-normalizing  network  based  on  the\n",
    "SELU  activation  function  (as  discussed  earlier),  you  should  use\n",
    "alpha dropout: this is a variant of dropout that preserves the mean\n",
    "and standard deviation of its inputs</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Monte Carlo (MC) Dropout</div>\n",
    "This can  boost  the  performance  of  any  trained  dropout  model  without  having  to\n",
    "retrain it or even modify it at all, provides a much better measure of the models\n",
    "uncertainty.\n",
    "\n",
    " full  implementation  of  MC  Dropout,  boosting  the  dropout  model\n",
    "we trained earlier without retraining it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas=np.stack([model(X_test_scaled,training=True) for sample in range(100)]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probas.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just make 100 predictions over the test set, setting training=True to ensure that\n",
    "the  Dropout  layer  is  active,  and  stack  the  predictions.  Since  dropout  is  active,  all  the\n",
    "predictions will be different. Recall that `predict()` returns a matrix with one row per\n",
    "instance and one column per class. Because there are 10,000 instances in the test set\n",
    "and  10  classes,  this  is  a  matrix  of  shape  [10000,10].  We  stack  100  such  matrices,  so\n",
    "`y_probas`  is  an  array  of  shape  [100,10000,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we average over the first dimension we get `y_proba` an array of shape [10000,10], like we would get with a single prediction. Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single  prediction  with  dropout  off. For  example,  lets  look  at  the  models  prediction for the first instance in the Fashion MNIST test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea71c44190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0],cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 379ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.75, 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.2 , 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.62, 0.  , 0.38]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.67, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.67, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.12, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.88, 0.  , 0.12]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.64, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.63, 0.  , 0.37]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.72, 0.  , 0.28]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:,:1],2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  tells  a  very  different  story:  apparently,  when  we  activate  dropout,  the  model  is\n",
    "not  sure  anymore.  It  still  seems  to  prefer  class  9,  but  sometimes  it  hesitates  with\n",
    "classes 5 (sandal) and 7 (sneaker), which makes sense given theyre all footwear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.round(y_proba[:1],2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take a look at the standard deviation of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.21]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1],2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (such\n",
    "as BatchNormalization layers), then you should not force training mode like we just\n",
    "did.  Instead,  you  should  replace  the  Dropout  layers  with  the  following  MCDropout class:\n",
    "```python\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">max-norm regularization</div>\n",
    "For each neuron this regularization technique constraints the weights $\\mathbf{w}$ of the incoming connections such that the $\\ell_2$ norm of weights i.e $||\\mathbf{w}||_2\\leq r$. Where $r$ is the max-norm hyperparameter.\n",
    "\n",
    "Max-norm  regularization  does  not  add  a  regularization  loss  term  to  the  overall  loss\n",
    "function.  Instead,  it  is  typically  implemented  by  computing  $\\mathbf{w}_2$  after  each  training\n",
    "step and rescaling w if needed $(\\mathbf{w}  \\mathbf{w} \\ r/ ||\\mathbf{w}||_2)$.\n",
    "\n",
    "Reducing r increases the amount of regularization and helps reduce overfitting. Max-\n",
    "norm  regularization  can  also  help  alleviate  the  unstable  gradients  problems  (if  you\n",
    "are not using Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1ea6fa96690>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",kernel_constraint=keras.constraints.max_norm(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After  each  training  iteration,  the  models  `fit()`  method  will  call  the  object  returned\n",
    "by  `max_norm()`,  passing  it  the  layers  weights  and  getting  rescaled  weights  in  return,\n",
    "which  then  replace  the  layers  weights.\n",
    "\n",
    "The max_norm() function has an axis argument that defaults to 0. A Dense layer usu\n",
    "ally  has  weights  of  shape  [number  of  inputs,  number  of  neurons],  so  using  axis=0\n",
    "means  that  the  max-norm  constraint  will  apply  independently  to  each  neurons\n",
    "weight  vector.  If  you  want  to  use  max-norm  with  convolutional  layers,  make  sure  to  set  the  max_norm()  constraints  axis  argument  appropriately\n",
    "(usually axis=[0, 1, 2])."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Practical usage </div>\n",
    "\n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4maW_t-Ncj_wmwVdBL0Ei665eYx_3x35pJTojuWUOv7GoFHKCIvWuo0M6BQnPzmD5ZFnTr378TGy3q0xAlp7F_2-Ifvmg8yIGOwM6mjU0Kn2WYjnXBN9XsdfiqUrOMmZYa5CgbiQqhBEG7Un0xMdAMxoPKIpaogjW-TKAG8zbkSNzlMcQrV6KV4GrfECRrMAn_?width=388&height=201&cropmode=none\" width=\"388\" height=\"201\" />\n",
    "\n",
    "the following image suggests the default values of the hyperparameters of the `model.compile` which are good to go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "though if  the  network  is  a  simple  stack  of  dense  layers,  then  it  can  self-normalize,  and  you\n",
    "should use the configuration in the below table \n",
    "\n",
    "<img src=\"https://dsm04pap003files.storage.live.com/y4mfx157r4PqtD435SQ-6Fl5KrdoZ096PZxIDMlrgtjzPcLTnGqNp3z-_6-VBFJswBPeYlJwVNTij_sn4d1eQ4D1WrxYi5JK4natpK8zGRHIiaj8DeiuB3T8OEn5knb12Eli5DAAgxYzW5ntJdirLgVmjRhrkKXqHbgo1c-YjAkMpz9lM63WBr55Nj0UZ-gwaPf?width=389&height=198&cropmode=none\" width=\"389\" height=\"198\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
