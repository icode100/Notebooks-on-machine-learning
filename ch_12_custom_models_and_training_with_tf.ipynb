{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body { font-family: Times New Roman; font-size: 16px; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# markdown font settings\n",
    "from IPython.display import display, HTML # type:ignore\n",
    "style = '<style> body { font-family: Times New Roman; font-size: 16px; } </style>' \n",
    "display(HTML(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other imports \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">Tensors and operations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]]) #a matrix can often be said as a vector \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) # a scalar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is similar to numpy's ndarray. the above declared one is also a tensor. It does have the properties similar to numpy's ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing as well works like ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [4., 5.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other operations are available as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is code for $|t|^2=t.t^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference with numpy is being depicted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14., 32.],\n",
       "       [32., 77.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.dot(arr.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">keras' low level API</div>\n",
    "we can use `keras.backend` to create functions available in tensorflow. If  you  want  to  write  code  that  will  be\n",
    "portable to other Keras implementations, you should use these Keras functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = keras.backend\n",
    "K.square(K.transpose(t))+10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Tensors and numpy</div>\n",
    "np arrays and tensors go hand in hand and can be used interchangably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2.,4.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4.],\n",
       "       [2., 5.],\n",
       "       [3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid;border-radius:4px;width:50vw;\">note that tensorflow uses 64 bit by default while numpy uses 32 bit</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Type conversions</div>\n",
    "Type conversions significantly hurt performance and can easily go unnoticed when they are done automatically. To avoid this TF does not perform them automatically and raises exceptions for oprations on tensors with incompatible types. For  example,  you  cannot  add  a  float tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(2.)+tf.constant(3.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(2.)+tf.constant(3)\n",
    "#this will give error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(2.)+tf.constant(3.,dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(2.)+tf.constant(3.,dtype=tf.float64)\n",
    "# this will give error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do manual type conversion using `tf.cast()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40.,dtype=tf.float64)\n",
    "tf.constant(2.)+tf.cast(t2,tf.float32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Tensor` is immutable hence cannot be used for storing weights in a DNN as they will be changed during backpropagation. Also other parameters as well are needed to change hence we here use `tf.Variables()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]]) # type: ignore\n",
    "v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can modify it using `assign()` method (or `assign_sub()` and `assign_add()` method which is increment or decrement of the variable by a given value ) method. we can also update the values of a particular slice (or cell ) using `assign()` or by using `scatter_update()` or `scatter_nd_update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2*v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0,1].assign(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:,2].assign([0.,1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0,0],[1,2]],updates=[100.,200.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid;border-radius:4px;width:50vw;\">In  practice  you  will  rarely  have  to  create  variables  manually,  since\n",
    "Keras provides an <code>add_weight()</code> method that will take care of it for\n",
    "you,  as  we  will  see.  Moreover,  model  parameters  will  generally  be\n",
    "updated  directly  by  the  optimizers,  so  you  will  rarely  need  to\n",
    "update variables manually</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Other data structures</div>\n",
    "*Sparse tensors (`tf.SparseTensor`)*\\\n",
    "Efficiently  represent  tensors  containing  mostly  zeros.  The  tf.sparse  package\n",
    "contains operations for sparse tensors.\n",
    "\n",
    "*Tensor arrays (`tf.TensorArray`)*\\\n",
    "Are lists of tensors. They have a fixed size by default but can optionally be made\n",
    "dynamic. All tensors they contain must have the same shape and data type.\n",
    "\n",
    "*Ragged tensors (`tf.RaggedTensor`)*\\\n",
    "Represent  static  lists  of  lists  of  tensors,  where  every  tensor  has  the  same  shape\n",
    "and data type. The tf.ragged package contains operations for ragged tensors.\n",
    "*String tensors*\\\n",
    "Are regular tensors of type tf.string. These represent byte strings, not Unicode\n",
    "strings,  so  if  you  create  a  string  tensor  using  a  Unicode  string  (e.g.,  a  regular\n",
    "Python  3  string  like  \"café\"),  then  it  will  get  encoded  to  UTF-8  automatically\n",
    "(e.g.,  b\"caf\\xc3\\xa9\").  Alternatively,  you  can  represent  Unicode  strings  using\n",
    "tensors of type tf.int32, where each item represents a Unicode code point (e.g.,\n",
    "[99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte\n",
    "strings and Unicode strings (and to convert one into the other). It’s important to\n",
    "note  that  a  tf.string  is  atomic,  meaning  that  its  length  does  not  appear  in  the\n",
    "tensor’s  shape.  Once  you  convert  it  to  a  Unicode  tensor  (i.e.,  a  tensor  of  type\n",
    "tf.int32 holding Unicode code points), the length appears in the shape.\n",
    "\n",
    "*Sets*\\\n",
    "Are  represented  as  regular  tensors  (or  sparse  tensors).  For  example,  tf.con\n",
    "stant([[1, 2], [3, 4]]) represents the two sets {1, 2} and {3, 4}. More gener‐\n",
    "ally, each set is represented by a vector in the tensor’s last axis. You can\n",
    "manipulate sets using operations from the tf.sets package.\n",
    "\n",
    "*Queues*\\\n",
    "Store  tensors  across  multiple  steps.  TensorFlow  offers  various  kinds  of  queues:\n",
    "simple First In, First Out (FIFO) queues (FIFOQueue), queues that can prioritize\n",
    "Using TensorFlow like NumPy | 383\n",
    "some  items  (PriorityQueue),  shuffle  their  items  (RandomShuffleQueue),  and\n",
    "batch items of different shapes by padding (PaddingFIFOQueue). These classes are\n",
    "all in the tf.queue package.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family:fantasy;\">customizing models and Training algorithms</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42) #type:ignore\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">custom loss function</div>\n",
    "huber loss is most commonly used for higher outliers. \n",
    "$$L(y, \\hat{y}) = \\begin{cases} \n",
    "\\frac{1}{2}(y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As we can see it uses $\\ell_2$ error if the difference btwn `y_pred` and `y_true` is less than $\\delta$, else uses $\\ell_1$ error.\n",
    "\n",
    "This is available in `keras.losses.Huber`. but we can implement it as a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true,y_pred):\n",
    "    error = y_true-y_pred\n",
    "    is_small_error = tf.abs(error)<1\n",
    "    squared_loss = tf.square(error) / 2 # type: ignore\n",
    "    linear_loss = tf.abs(error)-0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building layers\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling mode\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - ETA: 0s - loss: 0.7835 - mae: 1.1571"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 4ms/step - loss: 0.7835 - mae: 1.1571 - val_loss: 0.2603 - val_mae: 0.5637\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2147 - mae: 0.5083 - val_loss: 0.2125 - val_mae: 0.4931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b2d6c8c10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It  is  also  preferable  to  return  a  tensor  containing  one  loss  per  instance,  rather  than\n",
    "returning  the  mean  loss.  This  way,  Keras  can  apply  class  weights  or  sample  weights\n",
    "when requested "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Saving and Loading Models that contain custom components</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model.save(\"./models/model_chapter_12_custom_fun.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you load it, you’ll need to provide a dictionary that maps\n",
    "the  function  name  to  the  actual  function.  More  generally,  when  you  load  a  model\n",
    "containing custom objects, you need to map the names to the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./models/model_chapter_12_custom_fun.h5\",custom_objects={\"huber_fn\":huber_fn})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have assumed the error threshold to be 1. but if we want it small then we need to do the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true,y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)<threshold\n",
    "        squared_loss = tf.square(error) / 2 # type: ignore\n",
    "        linear_loss = tf.abs(error)-0.5\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building layers\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", \n",
    "                       kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling model\n",
    "model.compile(loss=create_huber(2.0),optimizer='nadam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 3ms/step - loss: 0.5995 - val_loss: 0.2659\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2168 - val_loss: 0.2293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b2e981750>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving thus model \n",
    "model.save('./models/model_chapter_12_custom_fun_2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that\n",
    "the name to use is `\"huber_fn\"`, which is the name of the function you gave Keras, not\n",
    "the name of the function that created it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriving model\n",
    "model = keras.models.load_model(\"./models/model_chapter_12_custom_fun_2.h5\",custom_objects={'huber_fn':create_huber(2.0)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You  can  solve  this  by  creating  a  subclass  of  the  `keras.losses.Loss`  class,  and  then\n",
    "implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self,threshold=1.0,**kwargs):\n",
    "        self.threshold=threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self,y_true,y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)<self.threshold\n",
    "        squared_loss = tf.square(error) / 2 # type: ignore\n",
    "        linear_loss = tf.abs(error)-0.5\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config=super().get_config()\n",
    "        return {**base_config,'threshold':self.threshold}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the constructor accpets `**kwargs` and passes it to the parent constructor that handles the hyperparameters.\n",
    "    * the name of the algorithm\n",
    "    * the reduction algorithm to be used:\n",
    "        * `\"sum_over_batch_size\"`, which means the net loss will be the sum of the instance losses weighted by the sample weights(if any) and divided by the batch size rather than the sum of weights thus it is not weighted mean.Other possible values being `sum` and `none`.\n",
    "* `call()` method takes the labels and predictions computes all the instances losses and returns them.\n",
    "* `get_config()` returns a dictionary mapping of each hyperparameter name to it's value. It uses the super class dictionary and adds the new hyperparamters(or configurations) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building layers\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", \n",
    "                       kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "model.compile(loss=HuberLoss(2.0),optimizer='nadam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 2ms/step - loss: 0.5422 - val_loss: 0.2634\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2154 - val_loss: 0.2288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b2e884d90>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting model\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/model_chapter_12_custom_fun_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "model = keras.models.load_model(\"./models/model_chapter_12_custom_fun_3.h5\",custom_objects={'HuberLoss':HuberLoss})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When  you  save  a  model,  Keras  calls  the  loss  instance’s  `get_config()`  method  and\n",
    "saves  the  config  as  JSON  in  the  HDF5  file.  When  you  load  the  model,  it  calls  the\n",
    "<code class='language-python'>\n",
    "from_config()\n",
    "</code> \n",
    "class method on the HuberLoss class: this method is implemented by\n",
    "the  base  class  (Loss)  and  creates  an  instance  of  the  class,  passing  `**config`  to  the\n",
    "constructor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">custom activations functions, initializer, regularizers, and constraints</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here  are  examples  of  a  custom  activation\n",
    "function  (equivalent  to  `keras.activations.softplus()`  or  `tf.nn.softplus()`),  a\n",
    "custom  Glorot  initializer  (equivalent  to  `keras.initializers.glorot_normal()`),  a\n",
    "custom  ℓ1  regularizer  (equivalent  to  `keras.regularizers.l1(0.01)`),  and  a  custom\n",
    "constraint that ensures weights are all positive (equivalent to `keras.con\n",
    "straints.nonneg()` or `tf.nn.relu()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z)+1.0)\n",
    "def my_glorot_initializer(shape,dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2.0/(shape[0]+shape[1]))\n",
    "    return tf.random.normal(shape,stddev=stddev,dtype=dtype)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01*weights))\n",
    "def my_positive_weights(weights): # return value is same as tf.nn.relu(weights)\n",
    "    return tf.where(weights<0.0,tf.zeros_like(weights),weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer), # type: ignore\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 3ms/step - loss: 1.9400 - mae: 0.9837 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7117 - mae: 0.5390 - val_loss: inf - val_mae: inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b30c33dd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "model.save(\"./models/chapter_12_custom_fun_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "model = keras.models.load_model(\n",
    "    \"./models/chapter_12_custom_fun_4.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function will be applied to the output of this Dense layer, and its result\n",
    "will  be  passed  on  to  the  next  layer.  The  layer’s  weights  will  be  initialized  using  the\n",
    "value returned by the initializer. At each training step the weights will be passed to the\n",
    "regularization function to compute the regularization loss, which will be added to the\n",
    "main loss to get the final loss used for training. Finally, the constraint function will be\n",
    "called  after  each  training  step,  and  the  layer’s  weights  will  be  replaced  by  the  con‐\n",
    "strained weights.\n",
    "\n",
    "If  a  function  has  hyperparameters  that  need  to  be  saved  along  with  the  model,  then\n",
    "you will want to subclass the appropriate class, such as keras.regularizers.Regular\n",
    "izer, keras.constraints.Constraint, keras.initializers.Initializer, or\n",
    "keras.layers.Layer (for any layer, including activation functions). Much like we did\n",
    "for  the  custom  loss,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom l1 initializer\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self,factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self,weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor*weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\":self.factor}\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Custom Metrics</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics (e.g., accuracy)\n",
    "are used to evaluate a model: they must be more easily interpretable, and they can be\n",
    "non-differentiable or have 0 gradients everywhere.\n",
    "\n",
    "For each batch keras will compute the metric and keep track of its mean since the begining of the epoch. \n",
    "\n",
    "But this mean method will not be working with the precision (reason below)\n",
    "\n",
    "<div style=\"border:2px solid;border-radius:4px;width:50vw;\">\n",
    "Suppose the model\n",
    "made five positive predictions in the first batch, four of which were correct: that’s 80%\n",
    "precision.  Then  suppose  the  model  made  three  positive  predictions  in  the  second\n",
    "batch, but they were all incorrect: that’s 0% precision for the second batch. If you just\n",
    "compute the mean of these two precisions, you get 40%. But wait a second—that’s not\n",
    "the  model’s  precision  over  these  two  batches!  Indeed,  there  were  a  total  of  four  true\n",
    "positives  (4  +  0)  out  of  eight  positive  predictions  (5  +  3),  so  the  overall  precision  is\n",
    "50%,  not  40%.\n",
    "</div>\n",
    "\n",
    "Thus we need a tracker (an object ofcourse ) that keeps track of the true positives and false positives and return the ratio when required rather than computing the mean at each step. The application via \n",
    "```python\n",
    "keras.metrics.Precision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) #([predicted],[actual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus here it takes the cumulative data and not the mean. This is called a *streaming metric* (or *stateful metric*), as it is gradually updated, batch after batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also at any point we can we can call the `result()` method to get the current value of the metric. We can also look at its variables by using the `variables` attribute and we can reset them using `reset_states()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define such a custom defined _streaming metric_ it will return the huber loss when the result is asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self,threshold=1.0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold=threshold\n",
    "        self.huber_fn=create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\",initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\",initializer=\"zeros\")\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        metric=self.huber_fn(y_true,y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true),tf.float32))\n",
    "    def result(self):\n",
    "        return self.total/self.count\n",
    "    def get_config(self):\n",
    "        base_config=super().get_config()\n",
    "        return {**base_config,\"threshold\":self.threshold}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the above code:\n",
    "* The constructor uses `tf.add_weight()` to create the variable to keep track of the true positives and the count (i.e. metric state) over multiple batches(here it the sum of the huber losses (`total`) and the number of instances seen so far(`count`)). Keras tracks any tf.Variable that is set as an attribute (and more generally, any “trackable” object, such as layers or models).\n",
    "\n",
    "* `update_state` method this updates the metric state. Note that it adds the new number of the true positives (i.e.  labels) to the total and the true positives (or the labels) to the count at each epoch(or step).\n",
    "\n",
    "* the `result` method is kept separate as we are intended to take the ratio as and when required. so at the point of results the `update_state()` finishes operation then the `result()` method is called and is returned.\n",
    "\n",
    "* Then we have `get_config()` this was done earlier ofcourse!!!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the only benefit of our HuberMetric class is that the threshold will be saved.\n",
    "But  of  course,  some  metrics,  like  precision,  cannot  simply  be  averaged  over  batches:\n",
    "in those cases, there’s no other option than to implement a streaming metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Custom Layers</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may  simply  want  to  build  a  very  repetitive\n",
    "architecture, containing identical blocks of layers repeated many times, and it would be convenient to treat each block of layers as a single layer. For example, if the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a custom layer D containing layers A, B, C, so your model would then simply be D, D, D.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. layers ithout weights like `keras.layers.Flatten` or `keras.layers.ReLU`. If  you  want  to  create  a  custom  layer  without  any  weights,  the  simplest option is to write a function and wrap it in a keras.layers.Lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this layer can be used like any other layer. Also it can be used as an activation function layer. The  exponential  layer  is  sometimes\n",
    "used  in  the  output  layer  of  a  regression  model  when  the  values  to  predict  have  very different scales (e.g., 0.001, 10., 1,000.)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a custom stateful layer inheriting from keras.layers.Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self,units,activation=None,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units=units\n",
    "        self.activation=keras.activations.get(activation)\n",
    "    def build(self,batch_input_shape):\n",
    "        self.kernel=self.add_weight(\n",
    "            name=\"kernel\",shape=[batch_input_shape[-1],self.units],\n",
    "            initializer=\"glorot_normal\"\n",
    "        )\n",
    "        self.bias=self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=[self.units],initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self,X):\n",
    "        return self.activation(X@self.kernel+self.bias) #type:ignore\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1]+[self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "                **base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)\n",
    "            }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "* constructor(`__init__()`): It calls  the  parent  constructor,  passing  it  the  `kwargs`:  this  takes  care  of  standard arguments  such  as  `input_shape`,`trainable`,  and  `name`.  Then  it  saves  the  hyperparameters as attributes, converting the activation argument to the appropriate activation  function  using  the  `keras.activations.get()`  function (it  accepts functions, standard strings like `\"relu\"` or `\"selu\"`, or simply None)\n",
    "\n",
    "* `def build(self,batch_input_shape)`:  to create the layer’s variables by calling the add_weight()  method  for  each  weight.  The  build()  method  is  called  the  first time  the  layer  is  used At  that  point,  Keras  will  know  the  shape  of  this  layer’s inputs, and it will pass it to the build() method this is necessary to create ome of the weights. For example, we need to know the number of neurons in the  previous  layer  in  order  to  create  the  connection  weights  matrix  (i.e.,  the \"kernel\"): this corresponds to the size of the last dimension of the inputs. At the end  of  the  build()  method  (and  only  at  the  end), we need to call the parent's build() method: this tells Keras that the layer is built (it just sets self.built=True).\n",
    "\n",
    "* `def call(self,X)`: This is same as we did in the backpropagation. matrix product of weight vector(kernel) and the the input plus the bias vector and pass it to the activation function for manipulating the weights. \n",
    "\n",
    "* `def compute_output_shape(self, batch_input_shape)`: returns the shape of this layer's outputs. In this case, it is the same shape as the inputs, except the last dimension is replaced with the number of neurons in the layer.\n",
    "\n",
    "* `get_config()` is self explanatory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid;border-radius:4px;width:50vw;\">we can  generally  omit  the  <code>compute_output_shape()</code>  method,  as\n",
    "<code>tf.keras</code>  automatically  infers  the  output  shape,  except  when  the\n",
    "layer is dynamic.  In other Keras implemen‐\n",
    "tations, this method is either required or its default implementation\n",
    "assumes the output shape is the same as the input shape.</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create layer with multiple inputs (e.g. `Concatenate`) the argument to `call()` method should be a tuple of all the inputs and similarly the argument to the `compute_output_shape()` method should be a tuple containing all the input batch shape. similarly return type must be a list of outputs.\n",
    "here is an exaple for the same\n",
    "```python\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules\n",
    "```\n",
    "\n",
    "Now note that we cannot use this in sequential api as that supports only one input and one output. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the layer to have different behaviour during training and testing like in `Dropout` and `BatchNormalization`, we must add a training argument to the <code style=\"font-size: medium;\">call()</code> method and use this argument to decide what to do. \n",
    "\n",
    "here we create a layer that adds gaussian noise during the training and does nothing during the testing.\n",
    "\n",
    "```python\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Custom Models</div>\n",
    "we can build a custom model by making a new class for the same that inherits `keras.Model` class.\n",
    "Let's build the model depicted here\n",
    "\n",
    "<img src=\"https://onedrive.live.com/embed?resid=3E88EC0719160DD3%21351737&authkey=%21ACut8oZofHZMnNw&width=454&height=316\" width=\"300\" height=\"200\" />\n",
    "\n",
    "The inputs go through a first dense layer, then through a residual block composed of\n",
    "two  dense  layers  and  an  addition  operation  (a  residual\n",
    "block adds its inputs to its outputs), then through this same residual block three more\n",
    "times, then through a second residual block, and the final result goes through a dense\n",
    "output layer.\n",
    "\n",
    "To implement this model, it is best to first create\n",
    "a `ResidualBlock` layer, since we are going to create a couple of identical blocks (and\n",
    "we might want to reuse it in another model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):# here we are perfroming the actual addition operation\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)# the Z is replaced with the changed one after \n",
    "                        # every iteration of passing through a \n",
    "                        # layer in the hidden layers combination\n",
    "        return inputs + Z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use subclassing API to define our model using the `ResidualBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self,output_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1=keras.layers.Dense(30,activation=keras.activations.elu,kernel_initializer='he_normal')\n",
    "        self.block1=ResidualBlock(2,30)\n",
    "        self.block2=ResidualBlock(2,30)\n",
    "        self.out=keras.layers.Dense(output_dim)\n",
    "    def call(self,inputs):\n",
    "        Z=self.hidden1(inputs)\n",
    "        for _ in range(1+3):\n",
    "            Z=self.block1(Z)\n",
    "        Z=self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 5s 2ms/step - loss: 21.8962\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9738\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7014\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6519\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0165\n"
     ]
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 1s 2ms/step - loss: 1.9440\n",
      "162/162 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Model  class  is  a  subclass  of  the  Layer  class,  so  models  can  be  defined  and  used\n",
    "exactly like layers. But a model has some extra functionalities, including of course its\n",
    "compile(), fit(), evaluate(), and predict() methods (and a few variants), plus the\n",
    "get_layers()  method  (which  can  return  any  of  the  model’s  layers  by  name  or  by\n",
    "index)  and  the  save()  method  (and  support  for  keras.models.load_model()  and\n",
    "keras.models.clone_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_2_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model_chapter_12_custom_model.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model_chapter_12_custom_model.ckpt\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/model_chapter_12_custom_model.ckpt') #we used ckpt because it is created using subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./models/model_chapter_12_custom_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 5s 3ms/step - loss: 0.8302\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7003\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5897\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7401\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3806\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use sequential API for the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Losses and Metric Based on training internals</div>\n",
    "This can be helpful in regularization and also in keeping track of the internals of the model. \n",
    "\n",
    "To define a custom loss based on model internals, compute it based on any part of the\n",
    "model  you  want,  then  pass  the  result  to  the  `add_loss()`  method.For  example,  let’s\n",
    "build a custom regression MLP model composed of a stack of five hidden layers plus\n",
    "an  output  layer.  This  custom  model  will  also  have  an  auxiliary  output  on  top  of  the\n",
    "upper  hidden  layer.  The  loss  associated  to  this  auxiliary  output  will  be  called  the\n",
    "reconstruction  loss:  it  is  the  mean  squared  difference  between  the\n",
    "reconstruction and the inputs. By adding this reconstruction loss to the main loss, we\n",
    "will  encourage  the  model  to  preserve  as  much  information  as  possible  through  the\n",
    "hidden  layers—even  information  that  is  not  directly  useful  for  the  regression  task\n",
    "itself.  In  practice,  this  loss  sometimes  improves  generalization  (it  is  a  regularization\n",
    "loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self,output_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "                        keras.layers.Dense(30,activation='selu',kernel_initializer='lecun_normal')\n",
    "                        for _ in range(5)\n",
    "                        ]\n",
    "        self.out=keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs=batch_input_shape[-1]\n",
    "        self.reconstruct=keras.layers.Dense(n_inputs)\n",
    "        # super().build(batch_input_shape)\n",
    "    def call(self,inputs,training=None):\n",
    "        Z=inputs\n",
    "        for layer in self.hidden:\n",
    "            Z=layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss=tf.reduce_mean(tf.square(reconstruction-inputs))\n",
    "        self.add_loss(0.05*recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "* The constructor creates the DNN with five layer (dense hidden layers) and one dense output layer.\n",
    "* The  `build()`  method  creates  an  extra  dense  layer  which  will  be  used  to  reconstruct the inputs of the model. It must be created here because its number of units must  be  equal  to  the  number  of  inputs,  and  this  number  is  unknown  before  the `build()` method is called\n",
    "* The  `call()`  method  processes  the  inputs  through  all  five  hidden  layers,  then passes  the  result  through  the  reconstruction  layer,  which  produces  the  reconstruction.\n",
    "* Now again the `call()` method computes the reconstruction loss (the <span style=\"font-family:Menlo Nerd Font;font-size:small;color:red\">Mean squared difference</span> between the reconstruction and inputs) and adds it to the model's list of losses using the `add_loss()` method. (here we have scaled down the reconstruction by multiplying it with 0.05 just to make sure that the `recon_loss` )\n",
    "* finally the Z is passed through the output layer i.e self.out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 5s 3ms/step - loss: 0.7552 - reconstruction_error: 0.9936\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4191 - reconstruction_error: 0.3952\n",
      "162/162 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7758895],\n",
       "       [1.9025637],\n",
       "       [4.0374365],\n",
       "       ...,\n",
       "       [1.6079435],\n",
       "       [2.892401 ],\n",
       "       [4.2255177]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Computing Gradients using autodiff</div>\n",
    "\n",
    "consider the equation,\\\n",
    "$f(w_1,w_1) = 3w_1^2+2w_1w_2$\n",
    "we can say that, $\\frac{\\partial{f(w_1,w_2)}}{\\partial{w_1}} = 6w_1+2w_2$ also, $\\frac{\\partial{f(w_1,w_2)}}{\\partial{w_2}} = 2w_1$.\\\n",
    "It was easy in case of just 2 parameters but will be harder in case of more parameters like in a DNN. Thus we use approximation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"font-family:fantasy;\">Custom Training Loops</div>\n",
    "In some rare cases like wide and deep models the fit method doesnot work as we use only one optimizer in fit method while our model requires two. Here we need to design custom training loops for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model \n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation=\"elu\",kernel_initializer=\"he_normal\",kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1,kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we randomly sample the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X,y,batch_size=32):\n",
    "    idx=np.random.randint(len(X),size=batch_size)\n",
    "    return X[idx],y[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function to display training status, number of steps, total number of steps the mean loss since start of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f} \" for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(f\"\\r{iteration}/{total} - {metrics}\",\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss: 0.0900  - mean_square: 858.5000 \n"
     ]
    }
   ],
   "source": [
    "#testing the status bar\n",
    "import time\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare our model for training data, but before that we define some hyperparams. and choose optimier, loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "batch_size=32\n",
    "n_steps=len(X_train)//batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics=[keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/5\n",
      "11610/11610 - mean: 0.6530  - mean_absolute_error: 0.5219 \n",
      "Epoch2/5\n",
      "11610/11610 - mean: 0.6304  - mean_absolute_error: 0.5088 \n",
      "Epoch3/5\n",
      "11610/11610 - mean: 0.6650  - mean_absolute_error: 0.5221 \n",
      "Epoch4/5\n",
      "11610/11610 - mean: 0.6376  - mean_absolute_error: 0.5129 \n",
      "Epoch5/5\n",
      "11610/11610 - mean: 0.6207  - mean_absolute_error: 0.5054 \n"
     ]
    }
   ],
   "source": [
    "#custom Loop\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    print(f\"Epoch{epoch}/{n_epochs}\")\n",
    "    for step in range(1,n_steps+1):\n",
    "        X_batch,y_batch=random_batch(X_train_scaled,y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch,training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch,y_pred))\n",
    "            loss = tf.add_n([main_loss]+model.losses) \n",
    "        gradients = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch,y_pred)\n",
    "        print_status_bar(step*batch_size,len(y_train),mean_loss,metrics)\n",
    "    print_status_bar(len(y_train),len(y_train),mean_loss,metrics)\n",
    "    for metric in [mean_loss]+metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we create two loops one for the epochs and other for the batches in each epoch\n",
    "* then we sample the random batch from the training set\n",
    "* inside the `with tf.GradientTape() as tape:` block we compute the loss. First we find the testing predicted values with respect to the batch input then calculate the main loss by using `reduced_mean` as it should cumulate over each instance throughout the batch as it proceeds. Then define the net loss or `loss` by adding our main loss to the model losses i.e the regularization losses over here. `tf.add_n()` sums multiple tensors of same shape and same data type.\n",
    "* next as ususal we compute the gradients of the loss over the trainable variables of the model using autodiff. After that we pass the gradients to the optimizer to perform gradient descent in the backpropagation.\n",
    "* Then we find the mean loss cumulatively over each instance and display the status bar.\n",
    "* finally after each epoch we reset the `mean_loss` and the `metrics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add weight constaints using `kernel_constraints` or `bias_constraints` we should update the loop to apply these constraints just after `apply_gradients()`:\n",
    "```python\n",
    "#custom Loop\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    print(f\"Epoch{epoch}/{n_epochs}\")\n",
    "    for step in range(1,n_steps+1):\n",
    "        X_batch,y_batch=random_batch(X_train_scaled,y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch,training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch,y_pred))\n",
    "            loss = tf.add_n([main_loss]+model.losses) \n",
    "        gradients = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        # here we insert the code\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch,y_pred)\n",
    "        print_status_bar(step*batch_size,len(y_train),mean_loss,metrics)\n",
    "    print_status_bar(len(y_train),len(y_train),mean_loss,metrics)\n",
    "    for metric in [mean_loss]+metrics:\n",
    "        metric.reset_states()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this custom loop does not handle training and testing differently. To handle these we need to call model with <code style=\"font-size: medium;color: skyblue;\">training</code><code style=\"font-size: medium;color:white\"> = </code><code style=\"font-size: medium;color:rgb(0, 161, 254)\">True</code> and make sure it propagates to every layer concerned with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
